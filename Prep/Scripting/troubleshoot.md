# Real-World DevOps Scenario Questions Asked in MNC & Startup Interviews

## Q1: Your production website is down at 2 AM. Walk me through your complete incident response process from detection to resolution

---

### ğŸ§  Overview

A production outage at 2 AM is a **Severity-1 (P1)** incident.

The goal is not â€œfix perfectlyâ€ â€” it is:

1. **Restore service ASAP (MTTR focus)**
2. **Minimize business impact**
3. **Preserve evidence**
4. **Prevent recurrence**

In real-world DevOps (AWS + EKS + CI/CD), this typically involves:

* Monitoring alerts (CloudWatch, Prometheus, Datadog)
* Infra layer (ALB, EC2, EKS, RDS)
* Deployment layer (Helm, Docker, CI/CD)
* Application layer (logs, configs, secrets)

---

# ğŸ”¥ Phase 1: Detection & Alerting

### âš™ï¸ How it works

Production is monitored via:

* **Prometheus + Alertmanager**
* **AWS CloudWatch Alarms**
* **Route53 Health Checks**
* Synthetic checks (Pingdom, UptimeRobot)

Example alert:

```yaml
- alert: WebsiteDown
  expr: up{job="frontend"} == 0
  for: 2m
  labels:
    severity: critical
```

PagerDuty / OpsGenie triggers on-call.

---

# ğŸš¨ Phase 2: Triage (First 5â€“10 Minutes)

### ğŸ¯ Goal: Confirm impact & identify blast radius

### Step 1: Validate outage

```bash
curl -I https://prod.example.com
```

Check:

* 502? 503? 504?
* Timeout?
* DNS issue?

---

### Step 2: Check Infra Layer (AWS)

#### Check ALB

```bash
aws elbv2 describe-target-health --target-group-arn <tg-arn>
```

Look for:

* `unhealthy`
* 5xx spike

#### Check EC2 / EKS Nodes

```bash
kubectl get nodes
kubectl get pods -n prod
```

---

### Step 3: Check Application Health

```bash
kubectl logs deployment/frontend -n prod
```

Look for:

* CrashLoopBackOff
* OOMKilled
* DB connection errors

---

# ğŸ›  Phase 3: Containment & Immediate Recovery

### ğŸ§© Common Scenarios & Actions

| Problem        | Action             |
| -------------- | ------------------ |
| Bad deployment | Rollback           |
| Pods crashed   | Restart / Scale    |
| Node failure   | Replace node       |
| DB down        | Failover           |
| CPU spike      | Scale horizontally |
| Memory leak    | Restart pods       |

---

## ğŸ” Example: Rollback via Kubernetes

```bash
kubectl rollout history deployment/frontend -n prod
kubectl rollout undo deployment/frontend -n prod
```

Helm rollback:

```bash
helm history frontend -n prod
helm rollback frontend 12 -n prod
```

---

## ğŸ“ˆ Example: Scale temporarily

```bash
kubectl scale deployment frontend --replicas=6 -n prod
```

---

## ğŸ”„ Example: Restart Pods

```bash
kubectl rollout restart deployment/frontend -n prod
```

---

# ğŸ“¢ Phase 4: Communication

### âœ… Always communicate

* Create Slack war room
* Update stakeholders every 15â€“30 min
* Log timeline

Example update:

```
02:08 AM â€“ Identified failed deployment
02:12 AM â€“ Rollback initiated
02:15 AM â€“ Service restored
```

Never debug silently.

---

# ğŸ§ª Phase 5: Root Cause Analysis (After Stabilization)

### ğŸ” Investigate:

* What changed before failure?
* CI/CD deployment?
* Infra change?
* Config/Secret update?

Check:

```bash
kubectl describe pod <pod>
kubectl get events -n prod
```

CI/CD:

```bash
git log --oneline -5
```

---

# ğŸ“Š Phase 6: Postmortem (Within 24 Hours)

### Document:

* Timeline
* Root cause
* Detection gap?
* Preventive actions

Example action items:

* Add readiness probes
* Add canary deployment
* Add memory limits
* Add alert for error rate > 5%

---

# ğŸ”’ Production-Grade Best Practices

### 1ï¸âƒ£ Use Readiness & Liveness Probes

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
```

---

### 2ï¸âƒ£ Enable Auto Scaling

```bash
kubectl autoscale deployment frontend \
  --cpu-percent=70 --min=3 --max=10
```

---

### 3ï¸âƒ£ Use Blue/Green or Canary Deployments

Reduces blast radius.

---

### 4ï¸âƒ£ Maintain Runbooks

Every service should have:

* Restart steps
* Rollback steps
* Health check commands
* Owner contacts

---

### 5ï¸âƒ£ Track SLO & Error Budget

Alert on:

* Error rate > 5%
* Latency > threshold
* Pod restarts spike

---

# â± Real Incident Timeline Example

| Time     | Action                    |
| -------- | ------------------------- |
| 02:00    | PagerDuty alert           |
| 02:02    | Confirm outage            |
| 02:05    | Found failing pods        |
| 02:08    | Identified bad deployment |
| 02:12    | Rolled back               |
| 02:15    | Traffic normal            |
| 02:40    | Root cause found          |
| Next day | Postmortem                |

---

# âš ï¸ What NOT to Do

* âŒ Restart everything blindly
* âŒ Make multiple changes at once
* âŒ Debug in production without rollback plan
* âŒ Stay silent without communication

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Confirm impact
* Identify blast radius
* Restore service fast (rollback > debug)
* Communicate continuously
* Do RCA & prevent recurrence

ğŸ‘‰ Incident response = **Stabilize â†’ Investigate â†’ Prevent**.

----
## Q2: A deployment went live and within 5 minutes, error rates spiked from 0.1% to 40%. What are your immediate actions?

---

### ğŸ§  Overview

This is a **classic bad deployment scenario**.

Error spike from **0.1% â†’ 40% in 5 minutes** means:

* High customer impact
* Likely code/config regression
* Immediate rollback > deep debugging

In production DevOps (AWS + EKS + CI/CD), priority is:

> ğŸš¨ **Stop the blast radius â†’ Restore last stable state â†’ Then investigate**

MTTR > ego.

---

# ğŸš¨ Phase 1: Confirm & Assess (1â€“3 Minutes)

### âš™ï¸ Validate metrics

Check:

* Prometheus
* CloudWatch
* ALB 5xx
* Application logs

Example:

```bash
kubectl get pods -n prod
kubectl logs deployment/frontend -n prod --tail=50
```

Check:

* 500 errors?
* DB connection failures?
* OOMKilled?
* CrashLoopBackOff?

---

# ğŸ”¥ Phase 2: Immediate Containment (Rollback First)

### ğŸ¯ Decision Rule

If spike started **right after deployment**, assume itâ€™s the deployment.

Do NOT debug live unless rollback is impossible.

---

## ğŸ” Option 1: Kubernetes Rollback

```bash
kubectl rollout history deployment/frontend -n prod
kubectl rollout undo deployment/frontend -n prod
```

---

## ğŸ” Option 2: Helm Rollback

```bash
helm history frontend -n prod
helm rollback frontend <previous-revision> -n prod
```

---

## ğŸ” Option 3: Blue/Green Switch

If using ALB weighted routing:

```bash
aws elbv2 modify-listener \
  --listener-arn <arn> \
  --default-actions Type=forward,TargetGroupArn=<old-tg>
```

Switch traffic back instantly.

---

# ğŸ“Š Phase 3: Verify Recovery

Monitor:

* Error rate drops?
* Latency normal?
* Pods stable?

```bash
kubectl get pods -n prod -w
```

Check metrics for 5â€“10 minutes.

Only declare resolved when:

* Error rate < 1%
* No restarts
* No 5xx spikes

---

# ğŸ” Phase 4: Root Cause Investigation (After Stabilization)

Now safely analyze.

Check:

```bash
git log --oneline -5
```

Was it:

* Config change?
* Secret change?
* DB migration?
* Feature flag?
* Dependency upgrade?

---

## ğŸ§© Common Root Causes

| Issue                | What to Check    |
| -------------------- | ---------------- |
| Missing env variable | ConfigMap/Secret |
| DB schema mismatch   | Migration logs   |
| Increased CPU usage  | Resource limits  |
| External API failure | Timeout config   |
| Feature flag issue   | Toggle state     |

---

# ğŸ›¡ Phase 5: Prevent Future Blast

### âœ… Implement Canary Deployments

Example (Argo Rollouts):

```yaml
strategy:
  canary:
    steps:
      - setWeight: 20
      - pause: {duration: 2m}
```

Deploy to 20% first.

---

### âœ… Add Automatic Rollback

Prometheus alert:

```yaml
- alert: HighErrorRate
  expr: rate(http_requests_total{status=~"5.."}[2m]) > 0.05
```

Trigger pipeline rollback.

---

### âœ… Add Readiness Probes

Prevents broken pods from receiving traffic.

```yaml
readinessProbe:
  httpGet:
    path: /health
    port: 8080
```

---

# ğŸ“¢ Communication During Incident

Post update:

```
02:10 AM â€“ Error spike detected (40%)
02:12 AM â€“ Rollback initiated
02:15 AM â€“ Errors normalizing
02:20 AM â€“ Service restored
```

Never debug silently.

---

# âš ï¸ What NOT to Do

* âŒ Investigate deeply before rollback
* âŒ Push hotfix blindly
* âŒ Restart everything randomly
* âŒ Ignore metrics trend

---

# ğŸ“‹ Decision Flow (Production Mindset)

| Scenario                       | Action                            |
| ------------------------------ | --------------------------------- |
| Error spike right after deploy | Rollback immediately              |
| Spike but no recent deploy     | Investigate infra                 |
| Only one pod failing           | Replace pod                       |
| DB errors                      | Check migration / connection pool |
| External API                   | Fallback / Circuit breaker        |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Correlate spike with deployment
* Rollback immediately (fastest recovery)
* Verify metrics stabilize
* Do RCA after service restored
* Implement canary + auto rollback

ğŸ‘‰ In production: **Rollback first. Debug later. Protect customers.**

----
## Q3: Your team pushed a bad release to production on a Friday evening. The rollback is also failing. What do you do?

---

### ğŸ§  Overview

This is a **worst-case production scenario**:

* Bad release deployed
* Rollback failing
* High business impact
* Friday evening (reduced staffing risk)

At this point, the priority shifts to:

> ğŸš¨ **Service restoration by any safe means necessary**

You move from "standard rollback" to **disaster containment mode**.

---

# ğŸš¨ Phase 1: Stabilize Immediately

### ğŸ¯ First Question:

Is the system completely down or partially degraded?

Check:

```bash
kubectl get pods -n prod
kubectl get events -n prod
```

Check metrics:

* 5xx spike?
* Latency spike?
* DB saturation?

---

# ğŸ”¥ Phase 2: If Rollback Fails â€” Use Alternative Recovery Paths

---

## ğŸ” Option 1: Force Deploy Last Known Good Image

Maybe Helm history is broken, but Docker image exists.

```bash
kubectl set image deployment/frontend \
  frontend=123456789.dkr.ecr.ap-south-1.amazonaws.com/app:v1.2.3 \
  -n prod
```

Direct image pinning bypasses Helm history issues.

---

## ğŸ” Option 2: Scale Down New ReplicaSet Manually

```bash
kubectl get rs -n prod
kubectl scale rs frontend-abc123 --replicas=0 -n prod
kubectl scale rs frontend-oldxyz --replicas=3 -n prod
```

Manually shift traffic.

---

## ğŸ” Option 3: Switch Traffic at Load Balancer Level

If Blue/Green setup exists:

```bash
aws elbv2 modify-listener \
  --listener-arn <arn> \
  --default-actions Type=forward,TargetGroupArn=<previous-tg>
```

Bypass Kubernetes entirely.

---

## ğŸ” Option 4: Emergency Hotfix

If rollback impossible (e.g., DB migration broke backward compatibility):

* Patch config
* Disable feature flag
* Push minimal hotfix branch

Example feature flag disable:

```bash
kubectl edit configmap feature-flags -n prod
```

Restart pods:

```bash
kubectl rollout restart deployment/frontend -n prod
```

---

# ğŸ§© Common Reasons Rollback Fails

| Reason                    | Explanation                 | Action                |
| ------------------------- | --------------------------- | --------------------- |
| DB migration incompatible | Schema changed              | Forward-fix migration |
| Secrets changed           | Old pods fail auth          | Restore secret        |
| Helm state corrupted      | History lost                | Manual image deploy   |
| Dependency removed        | Old version not in registry | Re-push image         |
| Infra drift               | Terraform mismatch          | Manual infra patch    |

---

# ğŸ›‘ Phase 3: Protect Data First

If system unstable:

* Stop background jobs
* Disable writes
* Enable maintenance mode

Example:

```bash
kubectl scale deployment worker --replicas=0 -n prod
```

Protect DB from corruption.

---

# ğŸ“¢ Phase 4: Incident Command Structure

Even Friday evening:

* Declare SEV-1
* Create war room
* Assign roles:

| Role               | Responsibility   |
| ------------------ | ---------------- |
| Incident Commander | Decision making  |
| Ops Lead           | Infra recovery   |
| App Lead           | Code fix         |
| Scribe             | Timeline logging |

No chaos debugging.

---

# ğŸ” Phase 5: Identify Root Cause

Check:

```bash
kubectl describe pod <pod>
kubectl logs <pod>
```

Check:

```bash
git log --oneline -5
```

Was there:

* Breaking DB migration?
* Feature flag default change?
* Resource limit removal?
* Wrong image tag?

---

# ğŸ›¡ Phase 6: Post-Recovery Hardening

After service restored:

### âœ… Add these controls

1. **Canary deployment**
2. **Automatic rollback on error spike**
3. **Backward-compatible DB migrations**
4. **Immutable image tagging (no :latest)**
5. **Deployment freeze policy on Fridays**

---

# ğŸ”’ Production-Grade Prevention Strategy

### 1ï¸âƒ£ Safe DB Migration Pattern

* Step 1: Add new column
* Step 2: Deploy code using both
* Step 3: Remove old column later

Never break backward compatibility.

---

### 2ï¸âƒ£ Feature Flags for Risky Changes

Deploy dark â†’ enable gradually.

---

### 3ï¸âƒ£ Progressive Delivery (Argo Rollouts)

```yaml
strategy:
  canary:
    steps:
      - setWeight: 10
      - pause: {duration: 5m}
```

---

### 4ï¸âƒ£ Enforce Change Freeze Windows

Example:

* No major releases after 4 PM Friday
* Emergency fixes only

---

# âš ï¸ What NOT to Do

* âŒ Keep retrying failed rollback blindly
* âŒ Deploy random commits
* âŒ Make multiple untracked changes
* âŒ Hide the issue

---

# ğŸ“‹ Real Recovery Flow (Advanced Scenario)

| Step | Action               |
| ---- | -------------------- |
| 1    | Declare SEV-1        |
| 2    | Stop background jobs |
| 3    | Attempt rollback     |
| 4    | Rollback fails       |
| 5    | Force image redeploy |
| 6    | Traffic switch       |
| 7    | Validate metrics     |
| 8    | Stabilize            |
| 9    | RCA                  |
| 10   | Prevent recurrence   |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Stabilize system first
* If rollback fails â†’ manually redeploy last stable image
* Switch traffic if possible
* Protect database integrity
* After recovery â†’ enforce safer release strategy

ğŸ‘‰ When rollback fails, move from **automation â†’ controlled manual recovery**.

---
## Q4: CPU usage on all production servers suddenly hits 100% simultaneously. How do you diagnose and resolve this?

---

### ğŸ§  Overview

When **all production servers hit 100% CPU simultaneously**, this is usually:

* Traffic spike (legitimate or DDoS)
* Infinite loop / bad deployment
* Thundering herd problem
* DB bottleneck causing retries
* External dependency slowdown
* Crypto miner / security breach

Because it's **simultaneous across servers**, suspect:

> ğŸŒ External trigger or shared dependency
> ğŸ” Retry storm
> ğŸš¨ Malicious traffic

Priority:

1. Protect availability
2. Identify source
3. Stop amplification
4. Stabilize

---

# ğŸš¨ Phase 1: Immediate Triage (First 5 Minutes)

---

## ğŸ” 1ï¸âƒ£ Confirm CPU Pattern

```bash
top
htop
```

Or in Kubernetes:

```bash
kubectl top pods -n prod
kubectl top nodes
```

Check:

* Which pods?
* System processes?
* All services or only one?

---

## ğŸ” 2ï¸âƒ£ Check Traffic Spike

ALB metrics:

```bash
aws cloudwatch get-metric-statistics \
  --namespace AWS/ApplicationELB \
  --metric-name RequestCount
```

Prometheus:

```promql
rate(http_requests_total[1m])
```

If traffic spiked â†’ likely load or attack.

---

# ğŸ”¥ Phase 2: Identify Root Cause Pattern

---

## ğŸ§© Scenario A: Legitimate Traffic Spike

Check:

* Marketing campaign?
* Sale event?
* Cron job triggered globally?

Resolution:

```bash
kubectl scale deployment frontend --replicas=10 -n prod
```

Or enable HPA:

```bash
kubectl autoscale deployment frontend \
  --cpu-percent=70 --min=3 --max=20
```

---

## ğŸ§© Scenario B: Infinite Loop / Bad Deployment

If started after deployment:

```bash
kubectl rollout history deployment/app -n prod
kubectl rollout undo deployment/app -n prod
```

Check logs:

```bash
kubectl logs deployment/app -n prod
```

Look for:

* Tight retry loops
* Heavy computation
* Serialization issues

---

## ğŸ§© Scenario C: Retry Storm (Downstream Dependency)

If DB slow:

* App retries aggressively
* All servers spike

Check DB:

```bash
SELECT * FROM pg_stat_activity;
```

Fix:

* Add exponential backoff
* Add circuit breaker
* Temporarily scale DB

---

## ğŸ§© Scenario D: DDoS or Bot Attack

Check:

* Sudden IP spike
* Same endpoint hit repeatedly

Mitigation:

### Enable WAF rule

```bash
aws wafv2 update-web-acl ...
```

### Rate limiting at ALB / NGINX

Example NGINX:

```nginx
limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;
```

---

## ğŸ§© Scenario E: Crypto Miner / Compromised Node

Check unusual processes:

```bash
ps aux --sort=-%cpu | head
```

If unknown binary:

* Isolate node
* Remove from load balancer
* Rotate credentials
* Rebuild node

---

# ğŸ›  Phase 3: Containment Strategy

| Problem Type  | Immediate Action            |
| ------------- | --------------------------- |
| Traffic spike | Scale horizontally          |
| Bad release   | Rollback                    |
| DB bottleneck | Scale DB / optimize queries |
| Retry storm   | Reduce replicas temporarily |
| DDoS          | Enable WAF / block IPs      |
| Malware       | Isolate node                |

---

# ğŸ“ˆ Phase 4: Stabilize System

After mitigation:

Monitor:

```bash
kubectl top pods -n prod -w
```

Watch:

* CPU normalizing?
* Error rates?
* Latency?
* Pod restarts?

---

# ğŸ”’ Long-Term Preventive Controls

---

## âœ… 1ï¸âƒ£ Horizontal Pod Autoscaler

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## âœ… 2ï¸âƒ£ Resource Limits

```yaml
resources:
  requests:
    cpu: "200m"
  limits:
    cpu: "500m"
```

Prevents noisy neighbor issues.

---

## âœ… 3ï¸âƒ£ Add Rate Limiting

Protect backend from abuse.

---

## âœ… 4ï¸âƒ£ Add Circuit Breaker (Resilience)

Prevents retry storms.

---

## âœ… 5ï¸âƒ£ Add Alerting Before 100%

Alert at 70â€“80%.

```promql
node_cpu_seconds_total > 0.8
```

---

# ğŸ“‹ Diagnosis Flow (Interview Style)

| Step | Question                   |
| ---- | -------------------------- |
| 1    | Did traffic spike?         |
| 2    | Was there a deployment?    |
| 3    | Is DB or dependency slow?  |
| 4    | Is retry logic aggressive? |
| 5    | Any suspicious processes?  |

---

# âš ï¸ What NOT to Do

* âŒ Immediately restart everything
* âŒ Scale without knowing cause
* âŒ Ignore retry storms
* âŒ Debug without protecting availability

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Check traffic â†’ deployment â†’ dependencies
* Scale temporarily if needed
* Rollback if deployment-related
* Mitigate DDoS if traffic abnormal
* Add autoscaling + rate limiting

ğŸ‘‰ When CPU spikes everywhere, suspect **external trigger or shared dependency**, not random coincidence.

---
## Q5: A critical microservice is throwing 503 errors but all health checks are passing. How do you investigate?

---

### ğŸ§  Overview

A **503 (Service Unavailable)** with **healthy probes** usually means:

* The pod is alive, but **not able to serve real traffic**
* Upstream dependency failure
* Connection pool exhaustion
* Thread pool saturation
* Rate limiting / circuit breaker open
* Load balancer target issues

Key insight:

> Health check success â‰  application readiness under load

Goal:

1. Identify where 503 is generated (App vs LB)
2. Find resource or dependency bottleneck
3. Restore service safely

---

# ğŸ” Phase 1: Identify Where 503 Is Generated

---

## 1ï¸âƒ£ Check ALB / Ingress Metrics

CloudWatch:

```bash
aws cloudwatch get-metric-statistics \
  --namespace AWS/ApplicationELB \
  --metric-name HTTPCode_Target_5XX_Count
```

If ALB returning 503 â†’ target issue
If app returning 503 â†’ application logic issue

---

## 2ï¸âƒ£ Check Kubernetes Service & Endpoints

```bash
kubectl get svc -n prod
kubectl get endpoints <service> -n prod
```

If endpoints empty â†’ readiness probe misconfiguration.

---

# ğŸ” Phase 2: Inspect Pod-Level Behavior

---

## 1ï¸âƒ£ Check Resource Usage

```bash
kubectl top pods -n prod
```

Look for:

* High CPU
* Memory close to limit
* OOMKilled

Even if health probe works, CPU starvation can cause 503.

---

## 2ï¸âƒ£ Check Pod Logs

```bash
kubectl logs deployment/payment -n prod --tail=100
```

Common patterns:

* `Connection pool exhausted`
* `Timeout connecting to DB`
* `Upstream service unavailable`
* `Thread pool rejected execution`

---

# ğŸ§© Common Real-World Causes

| Cause                  | Why Health Check Passes     | Why 503 Happens                      |
| ---------------------- | --------------------------- | ------------------------------------ |
| DB pool exhaustion     | `/health` doesn't query DB  | Real requests need DB                |
| Dependency timeout     | Health endpoint lightweight | Business API depends on external API |
| Thread pool saturation | Health endpoint fast        | Worker threads busy                  |
| Rate limiter           | Health endpoint excluded    | Business APIs throttled              |
| Circuit breaker open   | Health check bypassed       | Real traffic blocked                 |

---

# ğŸ”¬ Phase 3: Validate Dependency Health

---

## 1ï¸âƒ£ Check Database

```sql
SELECT * FROM pg_stat_activity;
```

Look for:

* Too many connections
* Long-running queries

If pool exhausted â†’ increase pool or fix leak.

---

## 2ï¸âƒ£ Check Downstream Service

```bash
curl http://dependency-service:8080/health
```

If dependency failing â†’ cascading failure.

---

# ğŸ” Phase 4: Check Load Balancer & Networking

---

## 1ï¸âƒ£ Target Health

```bash
aws elbv2 describe-target-health --target-group-arn <tg-arn>
```

If:

* Intermittent unhealthy â†’ network issue
* Security group issue
* NACL issue

---

## 2ï¸âƒ£ Connection Limits

Check:

```bash
netstat -an | grep ESTABLISHED | wc -l
```

Too many open connections â†’ socket exhaustion.

---

# ğŸ›  Phase 5: Immediate Mitigation

---

## Scenario A: Connection Pool Exhaustion

Temporary fix:

```bash
kubectl scale deployment payment --replicas=6 -n prod
```

Long-term:

* Add proper pool sizing
* Add connection timeout

---

## Scenario B: Dependency Timeout

Enable circuit breaker:

Example (Spring Boot config):

```yaml
resilience4j:
  circuitbreaker:
    instances:
      paymentService:
        failureRateThreshold: 50
```

---

## Scenario C: Thread Pool Saturation

Increase worker threads OR scale horizontally.

---

## Scenario D: Rate Limiting

Check NGINX / Envoy config:

```nginx
limit_req zone=mylimit burst=20;
```

Adjust if too aggressive.

---

# ğŸ§ª Phase 6: Improve Health Checks

Often issue is poor readiness probe.

Bad:

```yaml
readinessProbe:
  httpGet:
    path: /health
```

Better:

```yaml
readinessProbe:
  httpGet:
    path: /ready
```

Where `/ready` checks:

* DB connectivity
* Redis
* Dependency ping

---

# ğŸ”’ Production Best Practices

---

## âœ… Separate Liveness vs Readiness

* Liveness â†’ container alive
* Readiness â†’ safe to serve traffic

---

## âœ… Add Metrics for:

* Connection pool usage
* Thread pool usage
* Dependency latency
* 5xx rate

---

## âœ… Use Circuit Breakers

Prevent cascading failure.

---

## âœ… Add Autoscaling on RPS, not just CPU

CPU might be normal but threads blocked.

---

# ğŸ“‹ Investigation Flow (Interview Style)

| Step | Question                    |
| ---- | --------------------------- |
| 1    | Is 503 from ALB or app?     |
| 2    | Are endpoints registered?   |
| 3    | Resource saturation?        |
| 4    | DB pool exhausted?          |
| 5    | Downstream dependency slow? |
| 6    | Rate limiting active?       |

---

# âš ï¸ What NOT to Do

* âŒ Restart all pods blindly
* âŒ Increase replicas without checking DB limits
* âŒ Assume health check means healthy
* âŒ Ignore connection pool metrics

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Identify where 503 originates
* Check resource saturation
* Check DB pool / thread pool
* Validate downstream dependencies
* Improve readiness probes

ğŸ‘‰ 503 + healthy checks usually means **application is alive but overloaded or blocked internally**.

---
## Q6: Your CI/CD pipeline is taking 2 hours to complete. The business wants it under 15 minutes. How do you approach this?

---

### ğŸ§  Overview

A 2-hour pipeline is a **delivery bottleneck**.

In DevOps terms, this impacts:

* Lead time for changes
* Deployment frequency
* Developer productivity
* Incident recovery speed

Goal:

> âš¡ Reduce pipeline time without reducing quality

Approach this as a **performance optimization problem**:

1. Measure
2. Identify bottlenecks
3. Parallelize
4. Cache
5. Optimize build strategy

---

# ğŸ” Phase 1: Measure & Break Down the Pipeline

---

## Step 1ï¸âƒ£: Identify Stage Timings

In Jenkins:

```groovy
pipeline {
    stages {
        stage('Build') { ... }
        stage('Test') { ... }
        stage('Scan') { ... }
        stage('Deploy') { ... }
    }
}
```

Check:

* Build time?
* Unit test time?
* Docker build time?
* Security scan time?
* Infra provisioning?

Create a timing table:

| Stage         | Current Time |
| ------------- | ------------ |
| Checkout      | 5 min        |
| Build         | 30 min       |
| Unit Tests    | 40 min       |
| Docker Build  | 25 min       |
| Security Scan | 15 min       |
| Deploy        | 5 min        |

Now you know where to attack.

---

# ğŸš€ Phase 2: Quick Wins (High Impact)

---

## âœ… 1ï¸âƒ£ Enable Parallel Execution

Instead of sequential:

```groovy
stage('Tests') {
    parallel {
        stage('Unit Tests') { ... }
        stage('Integration Tests') { ... }
    }
}
```

Massive time reduction if CPU available.

---

## âœ… 2ï¸âƒ£ Enable Dependency Caching

### Example: Maven Cache (Jenkins)

```groovy
environment {
    MAVEN_OPTS = "-Dmaven.repo.local=.m2/repository"
}
```

Or GitHub Actions:

```yaml
- uses: actions/cache@v3
  with:
    path: ~/.m2
    key: maven-${{ hashFiles('**/pom.xml') }}
```

Removes repeated dependency downloads.

---

## âœ… 3ï¸âƒ£ Optimize Docker Builds

Bad:

```dockerfile
COPY . .
RUN npm install
```

Better (layer caching):

```dockerfile
COPY package.json .
RUN npm install
COPY . .
```

Enable BuildKit:

```bash
DOCKER_BUILDKIT=1 docker build .
```

---

## âœ… 4ï¸âƒ£ Use Pre-Built Base Images

Instead of building from scratch every time:

```dockerfile
FROM company/base-node:1.0
```

Pre-install dependencies once.

---

# ğŸ§© Phase 3: Advanced Optimizations

---

## ğŸ”¹ 1ï¸âƒ£ Split CI & CD Pipelines

Instead of full pipeline every commit:

* PR â†’ Build + Unit Test (5â€“10 min)
* Merge to main â†’ Integration + Scan
* Tag â†’ Deploy

Reduces unnecessary heavy jobs.

---

## ğŸ”¹ 2ï¸âƒ£ Use Test Splitting

If 40 min test suite:

Split across multiple executors:

```bash
pytest --maxfail=1 --dist=loadscope -n 4
```

---

## ğŸ”¹ 3ï¸âƒ£ Skip Unchanged Components (Monorepo Optimization)

Only build affected services.

Example:

```bash
git diff --name-only origin/main...HEAD
```

Trigger selective builds.

---

## ğŸ”¹ 4ï¸âƒ£ Optimize Terraform Stage

Instead of:

```bash
terraform plan
terraform apply
```

Use:

* Targeted applies
* Remote state
* Avoid full refresh every run

---

## ğŸ”¹ 5ï¸âƒ£ Move Heavy Scans to Async Stage

Security scans (Trivy, SonarQube) can be:

* Async
* Nightly
* Non-blocking for non-prod branches

---

# ğŸ“Š Strategy Comparison

| Strategy                  | Impact | Effort | Priority |
| ------------------------- | ------ | ------ | -------- |
| Parallel stages           | High   | Low    | ğŸ”¥       |
| Dependency caching        | High   | Low    | ğŸ”¥       |
| Docker layer optimization | High   | Medium | ğŸ”¥       |
| Test splitting            | High   | Medium | High     |
| Selective builds          | High   | Medium | High     |
| Infra optimization        | Medium | Medium | Medium   |
| Async scans               | Medium | Low    | Medium   |

---

# âš ï¸ What NOT to Do

* âŒ Remove tests for speed
* âŒ Skip security scans entirely
* âŒ Add more hardware blindly
* âŒ Optimize without measuring

---

# ğŸ”’ Production-Grade Best Practices

---

## âœ… Use Ephemeral Runners

Avoid slow shared agents.

---

## âœ… Pre-Warm Build Agents

Have:

* Pre-installed Docker
* Pre-installed language runtimes
* Cached dependencies

---

## âœ… Define SLA per Pipeline Type

| Pipeline   | Target Time |
| ---------- | ----------- |
| PR Build   | <10 min     |
| Main Merge | <15 min     |
| Release    | <20 min     |

---

# ğŸ“ˆ Real-World Optimization Example

From 2 hours â†’ 18 minutes:

* Parallel tests: -30 min
* Maven cache: -20 min
* Docker caching: -15 min
* Selective build: -25 min
* Async scan: -10 min

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Measure stage timings first
* Parallelize everything possible
* Enable caching aggressively
* Optimize Docker layers
* Run heavy scans async

ğŸ‘‰ CI/CD optimization = **Measure â†’ Parallelize â†’ Cache â†’ Reduce unnecessary work**.

---
## Q7: A developer committed secrets (API keys) to a public GitHub repository 30 minutes ago. What is your step-by-step response?

---

### ğŸ§  Overview

This is a **Security Incident (SEV-1)**.

If secrets were pushed to a **public GitHub repo**, assume:

> ğŸ”“ The secret is already compromised
> ğŸ¤– Bots scan GitHub continuously
> â± 30 minutes is enough for exploitation

Priority:

1. **Revoke/rotate immediately**
2. **Contain access**
3. **Audit usage**
4. **Remove secret from Git history**
5. **Prevent recurrence**

---

# ğŸš¨ Phase 1: Immediate Containment (First 5â€“10 Minutes)

---

## 1ï¸âƒ£ Identify the Secret Type

Is it:

* AWS access key?
* Database password?
* Stripe key?
* GitHub PAT?
* Kubernetes token?

This determines impact.

---

## 2ï¸âƒ£ Revoke or Rotate Immediately (Do NOT wait)

### ğŸ” Example: AWS Access Key

Deactivate:

```bash
aws iam update-access-key \
  --access-key-id AKIAxxxx \
  --status Inactive \
  --user-name app-user
```

Then delete:

```bash
aws iam delete-access-key \
  --access-key-id AKIAxxxx \
  --user-name app-user
```

Create new key:

```bash
aws iam create-access-key --user-name app-user
```

Update secret store (SSM/Secrets Manager/K8s).

---

### ğŸ” Example: Rotate Kubernetes Secret

```bash
kubectl create secret generic api-secret \
  --from-literal=API_KEY=new_value \
  -n prod --dry-run=client -o yaml | kubectl apply -f -
```

Restart deployment:

```bash
kubectl rollout restart deployment/app -n prod
```

---

# ğŸ” Phase 2: Audit for Abuse

---

## 1ï¸âƒ£ Check CloudTrail (AWS Example)

```bash
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=AccessKeyId,AttributeValue=AKIAxxxx
```

Look for:

* Suspicious IPs
* Resource creation
* IAM changes
* Crypto mining activity

---

## 2ï¸âƒ£ Check Billing Spike

```bash
aws ce get-cost-and-usage ...
```

Unexpected EC2/GPU usage?

---

## 3ï¸âƒ£ Check GitHub Access Logs

In GitHub â†’ Settings â†’ Security â†’ Audit log

---

# ğŸ§¹ Phase 3: Remove Secret from Git History

âš ï¸ Simply deleting the file is NOT enough.

---

## Option 1ï¸âƒ£: BFG Repo-Cleaner

```bash
bfg --delete-files secrets.txt
bfg --replace-text passwords.txt
```

---

## Option 2ï¸âƒ£: git filter-repo (Preferred)

```bash
git filter-repo --path secrets.txt --invert-paths
```

Force push:

```bash
git push --force
```

âš ï¸ Notify team to re-clone repository.

---

# ğŸ”’ Phase 4: Invalidate Exposure Publicly

If repo was public:

* Make repo private temporarily
* Enable GitHub Secret Scanning
* Enable Dependabot security alerts

---

# ğŸ›¡ Phase 5: Production Hardening

---

## âœ… 1ï¸âƒ£ Enable Secret Scanning

GitHub â†’ Settings â†’ Code security â†’ Secret scanning

---

## âœ… 2ï¸âƒ£ Enforce Pre-Commit Hooks

Example using `git-secrets`:

```bash
git secrets --install
git secrets --register-aws
```

Prevents committing AWS keys.

---

## âœ… 3ï¸âƒ£ Use Environment-Based Secrets Only

Never store in:

* `.env` in repo
* Config files
* Dockerfile

Use:

* AWS Secrets Manager
* SSM Parameter Store
* Vault
* Kubernetes Secrets

---

## âœ… 4ï¸âƒ£ Use IAM Roles Instead of Access Keys

Instead of static keys:

```yaml
serviceAccountName: app-sa
```

Use IRSA (IAM Role for Service Accounts).

Removes need for AWS keys entirely.

---

# ğŸ“‹ Incident Timeline Example

| Time     | Action                     |
| -------- | -------------------------- |
| 02:00    | Secret committed           |
| 02:05    | Discovered                 |
| 02:07    | Key revoked                |
| 02:10    | New key generated          |
| 02:15    | Deployed new secret        |
| 02:30    | CloudTrail audit completed |
| Next Day | Postmortem                 |

---

# ğŸ“ Postmortem Questions

* Why was secret in local file?
* Why no pre-commit scanning?
* Why no GitHub secret scanning?
* Why not using IAM roles?

---

# âš ï¸ What NOT to Do

* âŒ Just delete the file and move on
* âŒ Assume nobody saw it
* âŒ Delay key rotation
* âŒ Ignore audit logs

---

# ğŸ”¥ Real Risk Example

Public AWS keys can be used to:

* Launch crypto mining EC2
* Exfiltrate S3 data
* Delete infrastructure
* Create IAM backdoors

Minutes matter.

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Assume compromise immediately
* Revoke/rotate secret first
* Audit logs for abuse
* Remove secret from Git history
* Implement secret scanning & IAM roles

ğŸ‘‰ With exposed secrets: **Rotate first. Investigate second. Prevent forever.**

---
## Q8: You need to deploy a hotfix to production immediately but your standard pipeline takes 45 minutes. What do you do?

---

### ğŸ§  Overview

This is a **high-urgency production change** scenario.

Constraints:

* Production impact active
* Standard CI/CD = 45 minutes
* Business needs immediate fix

Principle:

> ğŸš‘ Speed up safely, donâ€™t bypass controls blindly.

You use a **controlled emergency release path**, not random SSH + manual patching.

Goal:

1. Fix impact fast
2. Minimize risk
3. Maintain auditability

---

# ğŸš¨ Phase 1: Assess Scope (2â€“3 Minutes)

Ask:

* Is it config-only?
* Feature flag toggle?
* Small code change?
* Infra issue?
* Can we mitigate without deploy?

If possible â†’ avoid code deploy entirely.

---

# âš¡ Phase 2: Fastest Safe Mitigation Options

---

## ğŸ§© Option 1: Feature Flag Disable (Fastest)

If issue tied to new feature:

```bash
kubectl edit configmap feature-flags -n prod
```

Or update in Secrets Manager:

```bash
aws secretsmanager update-secret \
  --secret-id feature-flags \
  --secret-string '{"newFeature": false}'
```

Restart pods:

```bash
kubectl rollout restart deployment/app -n prod
```

â± Time: 3â€“5 minutes

---

## ğŸ§© Option 2: Scale or Traffic Control

If load-related:

```bash
kubectl scale deployment app --replicas=10 -n prod
```

Or reduce traffic via ALB weighting.

---

## ğŸ§© Option 3: Deploy Pre-Built Image (Bypass Full Pipeline)

If the fix is already built/tested (e.g., earlier commit):

```bash
kubectl set image deployment/app \
  app=123456.dkr.ecr.ap-south-1.amazonaws.com/app:stable-tag \
  -n prod
```

Avoids:

* Rebuilding
* Full CI
* Long scan cycle

â± Time: 2â€“5 minutes

---

## ğŸ§© Option 4: Emergency â€œHotfix Pipelineâ€

Maintain a slim pipeline for emergencies:

```groovy
pipeline {
  stages {
    stage('Build') { ... }
    stage('Unit Test') { ... }
    stage('Deploy') { ... }
  }
}
```

Skip:

* Full regression tests
* Heavy security scans
* Non-prod deployments

Only for SEV-1.

---

# ğŸ”¥ Phase 3: Controlled Emergency Deployment

---

## 1ï¸âƒ£ Create Hotfix Branch

```bash
git checkout -b hotfix/payment-timeout
```

Fix minimal code only.

---

## 2ï¸âƒ£ Peer Review (Fast but mandatory)

At least 1 approval.

---

## 3ï¸âƒ£ Build & Tag Clearly

```bash
docker build -t app:hotfix-20260214 .
docker push <ECR>/app:hotfix-20260214
```

---

## 4ï¸âƒ£ Deploy Directly to Prod

```bash
kubectl set image deployment/app \
  app=<ECR>/app:hotfix-20260214 -n prod
```

---

## 5ï¸âƒ£ Monitor Closely

```bash
kubectl rollout status deployment/app -n prod
kubectl top pods -n prod
```

Watch:

* Error rate
* CPU
* Latency
* Logs

---

# ğŸ›¡ Phase 4: After Stabilization

Once resolved:

1. Merge hotfix into main
2. Run full pipeline
3. Backfill tests if missing
4. Conduct postmortem

---

# ğŸ“‹ Decision Matrix

| Scenario                | Fastest Safe Action |
| ----------------------- | ------------------- |
| Feature issue           | Disable flag        |
| Config error            | Update ConfigMap    |
| Previous version stable | Deploy old image    |
| Code fix needed         | Hotfix branch       |
| Infra issue             | Manual patch        |

---

# ğŸ”’ Production Best Practices

---

## âœ… Maintain Pre-Built Stable Images

Always keep:

* `stable`
* `last-known-good`
* `previous-release`

---

## âœ… Keep Emergency Pipeline Ready

Document:

* How to deploy manually
* How to rollback
* Who approves

---

## âœ… Blue/Green for Instant Switch

If available:

```bash
aws elbv2 modify-listener ...
```

Switch traffic instantly.

---

## âœ… Protect Audit Trail

Even emergency deploy must have:

* Git commit
* Tagged image
* Change record
* Slack notification

---

# âš ï¸ What NOT to Do

* âŒ SSH into server and edit files
* âŒ Skip code review entirely
* âŒ Disable monitoring
* âŒ Push untested large change

---

# ğŸ“ˆ Ideal Architecture to Support This

To meet <15 min hotfix target:

* Parallel CI
* Pre-cached runners
* Canary deploy
* Feature flags
* Slim emergency pipeline

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Mitigate first (feature flag/config)
* If deploy required â†’ use minimal hotfix pipeline
* Deploy pre-built image when possible
* Monitor aggressively
* Backfill full validation later

ğŸ‘‰ Emergency deploys should be **fast, minimal, controlled, and auditable** â€” never chaotic.

---
## Q9: Your deployment pipeline works perfectly in staging but consistently fails in production. How do you debug this?

---

### ğŸ§  Overview

When **staging succeeds but production fails**, the issue is almost always:

* Environment drift
* Config / secret mismatch
* IAM / permission difference
* Infra scale difference
* Data difference
* Networking restriction

Principle:

> If code is identical, the environment is not.

Goal:

1. Identify what differs
2. Compare infra + config
3. Reproduce prod-like conditions
4. Fix root cause, not symptom

---

# ğŸ” Phase 1: Confirm It's Truly Identical Code

---

## 1ï¸âƒ£ Verify Image Tag

```bash
kubectl get deployment app -n staging -o=jsonpath='{.spec.template.spec.containers[0].image}'
kubectl get deployment app -n prod -o=jsonpath='{.spec.template.spec.containers[0].image}'
```

If different tags â†’ not same build.

---

## 2ï¸âƒ£ Verify Git SHA

```bash
kubectl describe pod <pod> -n prod | grep image:
```

Confirm same commit deployed.

---

# ğŸ” Phase 2: Compare Environment Differences

---

## ğŸ” 1ï¸âƒ£ Compare Secrets

```bash
kubectl get secret app-secret -n staging -o yaml
kubectl get secret app-secret -n prod -o yaml
```

Common issues:

* Missing env variable
* Different DB endpoint
* Wrong API key
* Expired token

---

## âš™ï¸ 2ï¸âƒ£ Compare ConfigMaps

```bash
kubectl get configmap app-config -n staging -o yaml
kubectl get configmap app-config -n prod -o yaml
```

Look for:

* Feature flags
* Timeout settings
* Memory configs
* Rate limits

---

## ğŸ”’ 3ï¸âƒ£ Compare IAM Permissions (AWS Example)

Production often stricter.

```bash
aws iam simulate-principal-policy \
  --policy-source-arn arn:aws:iam::123456:role/prod-role \
  --action-names s3:PutObject
```

Common prod failures:

* AccessDenied
* Missing KMS decrypt
* Missing S3 permissions

---

# ğŸ§© Phase 3: Check Infrastructure Differences

---

## ğŸ–¥ 1ï¸âƒ£ Resource Limits

```bash
kubectl describe pod <pod> -n prod
```

Compare:

```yaml
resources:
  requests:
    cpu: 200m
  limits:
    cpu: 300m
```

Staging may have higher limits â†’ prod throttling.

---

## ğŸŒ 2ï¸âƒ£ Networking Differences

Check:

* Security groups
* NACL
* VPC peering
* Private endpoints

Example:

```bash
kubectl exec -it <pod> -n prod -- curl db.internal:5432
```

Does prod have network restriction?

---

## ğŸ—„ 3ï¸âƒ£ Database Differences

Production DB might:

* Have more data
* Slower queries
* Different schema

Run:

```sql
SELECT COUNT(*) FROM large_table;
```

Query plan differences?

---

# ğŸ”¬ Phase 4: Examine Production Logs

---

```bash
kubectl logs deployment/app -n prod --tail=200
```

Look for:

* Timeout
* AccessDenied
* Connection refused
* Resource exhaustion
* 503 from dependency

---

# ğŸ”¥ Phase 5: Reproduce in Staging

To validate hypothesis:

* Use prod config in staging
* Use prod-like dataset
* Apply prod resource limits

Simulate failure intentionally.

---

# ğŸ“Š Common Root Causes Table

| Category        | Example Issue              |
| --------------- | -------------------------- |
| Secrets         | Missing API key            |
| IAM             | AccessDenied               |
| Networking      | Security group block       |
| Resource limits | CPU throttling             |
| DB              | Slow query under real load |
| External API    | Prod firewall restriction  |
| TLS             | Certificate mismatch       |
| Feature flag    | Enabled only in prod       |

---

# ğŸ›  Phase 6: Add Debug Instrumentation

If unclear:

* Enable debug logs temporarily
* Add request tracing (Jaeger, X-Ray)
* Check Prometheus metrics

Example:

```promql
rate(http_requests_total{status=~"5.."}[5m])
```

---

# ğŸ”’ Long-Term Prevention

---

## âœ… 1ï¸âƒ£ Infrastructure as Code Only

Use same Terraform modules:

```hcl
module "app" {
  environment = var.env
}
```

Avoid manual prod changes.

---

## âœ… 2ï¸âƒ£ Environment Parity

* Same Kubernetes version
* Same node types
* Same resource limits
* Same Helm chart

---

## âœ… 3ï¸âƒ£ Automated Config Drift Detection

Use:

* Terraform plan in CI
* Drift detection tools

---

## âœ… 4ï¸âƒ£ Production-Like Staging

* Similar data size
* Same IAM policies
* Same scaling settings

---

# ğŸ“‹ Debug Flow (Interview Style)

| Step | Action                |
| ---- | --------------------- |
| 1    | Verify same image     |
| 2    | Compare secrets       |
| 3    | Compare config        |
| 4    | Check IAM permissions |
| 5    | Check networking      |
| 6    | Check resource limits |
| 7    | Analyze logs          |
| 8    | Reproduce in staging  |

---

# âš ï¸ What NOT to Do

* âŒ Assume code issue immediately
* âŒ Change production randomly
* âŒ Debug without comparing environments
* âŒ Ignore IAM differences

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Same code? Then environment differs
* Compare secrets, IAM, config, networking
* Check resource limits & DB scale
* Reproduce prod conditions in staging

ğŸ‘‰ When staging works and prod fails, itâ€™s almost always **environment drift or stricter production controls**.

---
## Q10: You need to deploy to 500 servers simultaneously with zero downtime. How do you architect this?

---

### ğŸ§  Overview

Deploying to **500 servers with zero downtime** requires:

* No single point of failure
* Progressive traffic shifting
* Health-based rollout
* Automated rollback

Principle:

> Never update all 500 at once.
> Update in controlled waves behind a load balancer.

Architecture must support:

* Rolling updates or Blue/Green
* Health checks
* Auto scaling
* Observability
* Fast rollback

---

# ğŸ— High-Level Architecture

```
Users
   â†“
Route53
   â†“
ALB / NLB
   â†“
Target Groups (Blue / Green)
   â†“
Auto Scaling Group (500 servers)
```

OR in Kubernetes:

```
Users â†’ Ingress â†’ Service â†’ 500 Pods â†’ Rolling Update Strategy
```

---

# ğŸ” Strategy 1: Blue/Green Deployment (Safest)

---

## âš™ï¸ How It Works

* 500 servers running â†’ Blue
* Deploy new version â†’ Green (separate ASG)
* Validate Green
* Switch traffic instantly

---

## ğŸ§© AWS Example

Create new Auto Scaling Group:

```bash
aws autoscaling create-auto-scaling-group \
  --auto-scaling-group-name app-green \
  --launch-template LaunchTemplateId=lt-123
```

Attach to new target group.

Switch traffic:

```bash
aws elbv2 modify-listener \
  --listener-arn <arn> \
  --default-actions Type=forward,TargetGroupArn=<green-tg>
```

â± Downtime: 0 seconds
Rollback: Switch back to Blue

---

## ğŸ“Š Pros vs Cons

| Feature    | Blue/Green                      |
| ---------- | ------------------------------- |
| Downtime   | Zero                            |
| Risk       | Low                             |
| Infra Cost | High (double infra temporarily) |
| Rollback   | Instant                         |

---

# ğŸ” Strategy 2: Rolling Update (Controlled Waves)

---

## âš™ï¸ Auto Scaling Rolling Update

Instead of 500 at once:

Update in batches of 10â€“20.

Example (ASG):

```json
"MaxBatchSize": 20,
"MinInstancesInService": 480
```

Ensures:

* 480 always running
* 20 updating at a time

---

## ğŸ§© Kubernetes Example

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 5%
    maxSurge: 5%
```

With 500 pods:

* 25 max unavailable
* 25 surge pods

Zero downtime maintained.

---

# ğŸ” Strategy 3: Canary Deployment (Advanced)

Deploy to 5% first:

```yaml
strategy:
  canary:
    steps:
      - setWeight: 5
      - pause: {duration: 5m}
      - setWeight: 50
```

If metrics healthy â†’ continue
If not â†’ auto rollback

Best for high-risk changes.

---

# ğŸ” Critical Zero-Downtime Requirements

---

## âœ… 1ï¸âƒ£ Proper Health Checks

ALB:

```bash
--health-check-path /health
```

Kubernetes:

```yaml
readinessProbe:
  httpGet:
    path: /ready
```

Only healthy instances receive traffic.

---

## âœ… 2ï¸âƒ£ Graceful Shutdown

Avoid dropping active requests.

Example:

```yaml
terminationGracePeriodSeconds: 30
```

App must handle SIGTERM properly.

---

## âœ… 3ï¸âƒ£ Connection Draining

ALB deregistration delay:

```bash
--deregistration-delay 30
```

Allows in-flight requests to finish.

---

## âœ… 4ï¸âƒ£ Stateless Application

All 500 servers must be:

* Stateless
* Shared DB
* Shared cache (Redis)
* No local session storage

---

## âœ… 5ï¸âƒ£ Backward-Compatible Deployments

Database migrations must be:

* Expand first
* Deploy code
* Contract later

Never break running version.

---

# ğŸ›  CI/CD Architecture for 500 Nodes

---

## Jenkins Example

```groovy
stage('Deploy') {
    steps {
        sh './deploy.sh --strategy=rolling'
    }
}
```

Deploy script should:

* Wait for health
* Validate batch
* Continue next batch

---

# ğŸ“ˆ Monitoring During Deployment

Monitor in real time:

* 5xx error rate
* Latency
* CPU
* Memory
* Pod restarts

Example:

```promql
rate(http_requests_total{status=~"5.."}[1m])
```

Abort if spike.

---

# ğŸ“‹ Strategy Comparison

| Strategy   | Risk   | Cost   | Rollback Speed | Recommended When  |
| ---------- | ------ | ------ | -------------- | ----------------- |
| Rolling    | Medium | Low    | Medium         | Normal releases   |
| Blue/Green | Low    | High   | Instant        | Critical systems  |
| Canary     | Lowest | Medium | Fast           | High-risk changes |

---

# âš ï¸ What NOT to Do

* âŒ Deploy to all 500 simultaneously
* âŒ Disable health checks
* âŒ Skip readiness probes
* âŒ Ignore graceful shutdown
* âŒ Run incompatible DB migration

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Use Blue/Green or controlled Rolling updates
* Always deploy in batches
* Use health checks + connection draining
* Ensure stateless architecture
* Monitor metrics and auto rollback

ğŸ‘‰ Zero downtime at scale = **traffic control + health validation + controlled rollout + instant rollback**.

---
## Q11: Your Kubernetes cluster has 200 pods stuck in "Pending" state during peak traffic. What caused this and how do you fix it?

---

### ğŸ§  Overview

`Pending` means:

> Scheduler cannot place pods on any node.

At peak traffic, this is usually a **capacity or scheduling problem**.

Common causes:

* Insufficient CPU/Memory
* Node group maxed out
* Resource requests too high
* Taints/Tolerations mismatch
* PVC not bound
* IP address exhaustion (VPC CNI)
* HPA scaling faster than Cluster Autoscaler

Goal:

1. Identify scheduling failure reason
2. Restore capacity immediately
3. Prevent recurrence

---

# ğŸ” Phase 1: Identify Exact Scheduling Error

---

## 1ï¸âƒ£ Describe One Pending Pod

```bash
kubectl describe pod <pod-name> -n prod
```

Look at:

```
Events:
0/10 nodes are available: 
10 Insufficient cpu
```

OR

```
node(s) had taint {dedicated: gpu}, that the pod didn't tolerate
```

OR

```
pod has unbound immediate PersistentVolumeClaims
```

This message tells you root cause directly.

---

# ğŸ§© Common Causes & Fixes

---

# ğŸ”¥ Scenario 1: Insufficient CPU/Memory (Most Common)

Message:

```
Insufficient cpu
Insufficient memory
```

### Immediate Fix

Scale node group:

```bash
aws autoscaling update-auto-scaling-group \
  --auto-scaling-group-name eks-prod-ng \
  --desired-capacity 20
```

If using Cluster Autoscaler:

```bash
kubectl get pods -n kube-system | grep cluster-autoscaler
```

Check logs:

```bash
kubectl logs deployment/cluster-autoscaler -n kube-system
```

---

### Long-Term Fix

* Increase node size
* Tune pod requests
* Enable cluster autoscaler

---

# ğŸ”¥ Scenario 2: Resource Requests Too High

Example:

```yaml
resources:
  requests:
    cpu: "4"
    memory: "8Gi"
```

Even if actual usage is low, scheduler blocks placement.

### Fix

Reduce request:

```yaml
requests:
  cpu: "500m"
```

Redeploy.

---

# ğŸ”¥ Scenario 3: Node Group Max Limit Reached

Check:

```bash
kubectl get nodes
```

If count stable during scale event â†’ autoscaler max reached.

Fix:

Update ASG max size.

---

# ğŸ”¥ Scenario 4: Taints & Tolerations Mismatch

Check node taints:

```bash
kubectl describe node <node-name> | grep Taints
```

Pod spec must include:

```yaml
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "app"
    effect: "NoSchedule"
```

---

# ğŸ”¥ Scenario 5: PVC / Storage Issue

Error:

```
pod has unbound immediate PersistentVolumeClaims
```

Check:

```bash
kubectl get pvc -n prod
```

Fix:

* StorageClass issue
* Volume quota exceeded
* EBS limit reached

---

# ğŸ”¥ Scenario 6: IP Address Exhaustion (EKS Specific)

In AWS EKS, pods require ENI IPs.

If exhausted:

```
failed to assign IP address to pod
```

Check:

```bash
kubectl logs aws-node -n kube-system
```

Fix:

Enable prefix delegation or increase instance size.

---

# ğŸš¨ Immediate Recovery Playbook

| Cause             | Fastest Fix           |
| ----------------- | --------------------- |
| Insufficient CPU  | Scale node group      |
| Requests too high | Patch deployment      |
| ASG max reached   | Increase max capacity |
| PVC issue         | Fix storage class     |
| IP exhaustion     | Add larger nodes      |

---

# ğŸ“ˆ Phase 2: Validate After Fix

Watch:

```bash
kubectl get pods -n prod -w
```

Ensure:

* Pending â†’ Running
* HPA stabilizes
* CPU normalizes

---

# ğŸ”’ Long-Term Preventive Controls

---

## âœ… 1ï¸âƒ£ Enable Cluster Autoscaler

Ensure:

```bash
--balance-similar-node-groups
--expander=least-waste
```

---

## âœ… 2ï¸âƒ£ Use HPA Properly

```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      averageUtilization: 70
```

---

## âœ… 3ï¸âƒ£ Right-Size Resource Requests

Use Prometheus data to tune.

---

## âœ… 4ï¸âƒ£ Add Capacity Buffer

Never run cluster at 100%.

Keep ~20% headroom.

---

## âœ… 5ï¸âƒ£ Monitor Scheduling Failures

Prometheus alert:

```promql
kube_pod_status_phase{phase="Pending"} > 10
```

---

# ğŸ“‹ Diagnosis Flow (Interview Style)

| Step | Action                    |
| ---- | ------------------------- |
| 1    | Describe pod              |
| 2    | Identify scheduling error |
| 3    | Check node capacity       |
| 4    | Check autoscaler          |
| 5    | Check resource requests   |
| 6    | Check IP/storage limits   |

---

# âš ï¸ What NOT to Do

* âŒ Restart pods blindly
* âŒ Delete random nodes
* âŒ Increase replicas without node capacity
* âŒ Ignore autoscaler logs

---

# ğŸ’¡ In Short (Interview Quick Recall)

* `Pending` = scheduler cannot place pod
* Check `kubectl describe pod` first
* Most common cause = insufficient resources
* Scale nodes or tune requests
* Maintain headroom + autoscaling

ğŸ‘‰ 200 Pending pods during peak traffic usually means **capacity planning failure or autoscaler misconfiguration**.

---
## Q12: A containerized application works locally but crashes in Kubernetes with OOMKilled errors. How do you resolve this?

---

### ğŸ§  Overview

`OOMKilled` in Kubernetes means:

> The container exceeded its **memory limit**, and the kernel terminated it.

Why it works locally:

* No strict memory limits
* Docker Desktop often has higher memory
* No cgroup enforcement like in K8s

Why it fails in K8s:

* `resources.limits.memory` enforced
* Production workload heavier
* Memory leak
* Incorrect JVM / runtime memory settings

Goal:

1. Confirm OOM root cause
2. Identify whether limit too low or leak
3. Fix resource configuration properly
4. Prevent recurrence

---

# ğŸ” Phase 1: Confirm OOMKilled

---

## 1ï¸âƒ£ Check Pod Status

```bash
kubectl get pods -n prod
```

Look for:

```
CrashLoopBackOff
OOMKilled
```

---

## 2ï¸âƒ£ Describe Pod

```bash
kubectl describe pod <pod-name> -n prod
```

Look for:

```
Last State: Terminated
Reason: OOMKilled
```

---

# ğŸ” Phase 2: Check Resource Configuration

---

## 1ï¸âƒ£ Inspect Deployment YAML

```bash
kubectl get deployment app -n prod -o yaml
```

Check:

```yaml
resources:
  requests:
    memory: "256Mi"
  limits:
    memory: "256Mi"
```

If limit = 256Mi and app needs 600Mi â†’ crash guaranteed.

---

# ğŸ§© Common Root Causes

| Cause                   | Why It Happens              |
| ----------------------- | --------------------------- |
| Memory limit too low    | Production load higher      |
| Memory leak             | Objects not released        |
| JVM not container-aware | Uses host memory assumption |
| Large dataset in prod   | Local test data small       |
| Cache misconfiguration  | No eviction policy          |
| Too many worker threads | High heap usage             |

---

# ğŸ”¥ Scenario 1: Limit Too Low (Most Common)

---

## Check Actual Usage

```bash
kubectl top pod <pod-name> -n prod
```

If usage near limit:

Increase limit:

```yaml
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"
```

Apply:

```bash
kubectl apply -f deployment.yaml
```

---

# ğŸ”¥ Scenario 2: JVM Container Memory Issue (Java Apps)

Older JVMs ignore container limits.

Fix:

```bash
JAVA_TOOL_OPTIONS="-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0"
```

Or explicitly:

```bash
-Xmx512m
```

Without this, JVM may try to use full node memory.

---

# ğŸ”¥ Scenario 3: Memory Leak

Symptoms:

* Memory usage gradually increases
* OOM after some time

Check metrics:

```promql
container_memory_usage_bytes
```

If steadily rising â†’ leak.

Fix:

* Heap dump
* Profiling (VisualVM, JProfiler)
* Fix code

---

# ğŸ”¥ Scenario 4: High Traffic in Production

Local tests:

* 10 requests

Prod:

* 500 RPS

Fix:

* Scale horizontally:

```bash
kubectl scale deployment app --replicas=5 -n prod
```

* Add HPA:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 2
  maxReplicas: 10
```

---

# ğŸ”¥ Scenario 5: Cache or Buffer Misconfiguration

Example: Redis cache fallback storing too much data.

Add limits or eviction:

```yaml
maxmemory-policy: allkeys-lru
```

---

# ğŸ›  Phase 3: Short-Term Mitigation

| Action                | When to Use             |
| --------------------- | ----------------------- |
| Increase memory limit | Immediate stabilization |
| Reduce concurrency    | High traffic issue      |
| Scale replicas        | Traffic-driven          |
| Restart pod           | Temporary relief        |

---

# ğŸ”’ Long-Term Prevention

---

## âœ… 1ï¸âƒ£ Proper Requests vs Limits

Best practice:

```yaml
requests:
  memory: "500Mi"
limits:
  memory: "1Gi"
```

Avoid setting equal values unless intentional.

---

## âœ… 2ï¸âƒ£ Monitor Memory Trends

Prometheus alert:

```promql
container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
```

Alert before OOM.

---

## âœ… 3ï¸âƒ£ Load Testing Before Production

Simulate real traffic.

---

## âœ… 4ï¸âƒ£ Right-Sized Node Types

Small nodes increase OOM risk cluster-wide.

---

## âœ… 5ï¸âƒ£ Enable VPA (Vertical Pod Autoscaler)

Automatically adjust requests.

---

# ğŸ“‹ Debug Flow (Interview Style)

| Step | Action                            |
| ---- | --------------------------------- |
| 1    | Confirm OOMKilled                 |
| 2    | Check memory limits               |
| 3    | Check actual memory usage         |
| 4    | Compare prod vs local load        |
| 5    | Check JVM settings                |
| 6    | Investigate memory leak if needed |

---

# âš ï¸ What NOT to Do

* âŒ Just increase memory blindly forever
* âŒ Ignore memory growth trend
* âŒ Set unlimited memory
* âŒ Assume local behavior matches production

---

# ğŸ’¡ In Short (Interview Quick Recall)

* OOMKilled = container exceeded memory limit
* Check limits vs actual usage
* Increase limit or fix memory leak
* Tune JVM/runtime for container awareness
* Monitor and autoscale properly

ğŸ‘‰ Works locally but fails in K8s usually means **resource limits + real production load expose memory constraints**.

----
## Q13: Your Kubernetes cluster is running out of IP addresses in production. How do you solve this without downtime?

---

### ğŸ§  Overview

In production (especially **AWS EKS**), IP exhaustion usually means:

> Pods cannot get IPs â†’ New pods stay `Pending` â†’ Scaling fails

Root causes:

* Small VPC CIDR
* Subnet IP exhaustion
* ENI/IP limits per instance
* No prefix delegation enabled
* Too many small instance types

Critical fact:

> Each Pod consumes a VPC IP (AWS VPC CNI).

Goal:

1. Stop immediate impact
2. Expand IP capacity without downtime
3. Prevent future exhaustion

---

# ğŸ” Phase 1: Confirm IP Exhaustion

---

## 1ï¸âƒ£ Check Pending Pods

```bash
kubectl get pods -A | grep Pending
```

---

## 2ï¸âƒ£ Check AWS CNI Logs

```bash
kubectl logs -n kube-system daemonset/aws-node
```

Look for:

```
failed to assign IP address to pod
Insufficient free IP addresses
```

---

## 3ï¸âƒ£ Check Subnet Usage

```bash
aws ec2 describe-subnets --subnet-ids subnet-xxxx \
  --query 'Subnets[*].AvailableIpAddressCount'
```

If low (<10â€“20 IPs), you're exhausted.

---

# ğŸš¨ Immediate Mitigation (No Downtime)

---

# ğŸ”¥ Option 1: Enable Prefix Delegation (Best Quick Fix)

Instead of 1 IP per ENI slot, prefix delegation allows multiple IPs per ENI.

Enable:

```bash
kubectl set env daemonset aws-node \
  -n kube-system ENABLE_PREFIX_DELEGATION=true
```

Restart:

```bash
kubectl rollout restart daemonset aws-node -n kube-system
```

âœ… Increases IP capacity dramatically
âœ… No cluster downtime

---

# ğŸ”¥ Option 2: Add New Subnets with Larger CIDR

If subnets too small:

1. Create larger subnet:

```bash
aws ec2 create-subnet \
  --vpc-id vpc-xxxx \
  --cidr-block 10.0.64.0/18
```

2. Add subnet to EKS node group.

3. Scale node group.

New nodes launch in new subnet automatically.

Zero downtime if rolling.

---

# ğŸ”¥ Option 3: Add Secondary CIDR Block to VPC

If entire VPC exhausted:

```bash
aws ec2 associate-vpc-cidr-block \
  --vpc-id vpc-xxxx \
  --cidr-block 100.64.0.0/16
```

Then create new subnets from new CIDR.

No downtime required.

---

# ğŸ”¥ Option 4: Use Larger Instance Types

Small instances:

* Fewer ENIs
* Fewer IPs

Check limits:

```bash
aws ec2 describe-instance-types \
  --instance-types t3.medium
```

Switch to:

* m5.large
* c5.large

Update node group â†’ rolling update.

---

# ğŸ”¥ Option 5: Reduce IP Consumption

Short-term:

* Scale down non-critical workloads
* Reduce replicas temporarily

```bash
kubectl scale deployment batch-job --replicas=0
```

Buys time.

---

# ğŸ“Š IP Limits Example (AWS)

| Instance Type | Max Pods (Without Prefix Delegation) |
| ------------- | ------------------------------------ |
| t3.medium     | ~17                                  |
| m5.large      | ~29                                  |
| m5.xlarge     | ~58                                  |

With prefix delegation â†’ significantly higher.

---

# ğŸ”’ Long-Term Architecture Fixes

---

## âœ… 1ï¸âƒ£ Design Large Enough VPC CIDR

Bad:

```
10.0.0.0/24
```

Better:

```
10.0.0.0/16
```

Plan for growth.

---

## âœ… 2ï¸âƒ£ Always Enable Prefix Delegation in EKS

Best practice for production clusters.

---

## âœ… 3ï¸âƒ£ Monitor Available IPs

CloudWatch alert:

```bash
AvailableIpAddressCount < 50
```

Alert before exhaustion.

---

## âœ… 4ï¸âƒ£ Use Separate Node Groups

* System pods
* App workloads
* Batch jobs

Prevents IP starvation.

---

## âœ… 5ï¸âƒ£ Consider Alternate CNI (Advanced)

Calico overlay:

* Pod IPs not tied to VPC

Tradeoff: More complexity.

---

# ğŸ“‹ Zero-Downtime Resolution Strategy

| Step | Action                   |
| ---- | ------------------------ |
| 1    | Confirm IP exhaustion    |
| 2    | Enable prefix delegation |
| 3    | Add new subnet if needed |
| 4    | Scale node group         |
| 5    | Monitor pending pods     |

---

# âš ï¸ What NOT to Do

* âŒ Recreate cluster
* âŒ Drain all nodes simultaneously
* âŒ Resize VPC incorrectly
* âŒ Ignore subnet sizing in planning

---

# ğŸ’¡ In Short (Interview Quick Recall)

* EKS pods consume VPC IPs
* Check CNI logs for IP exhaustion
* Enable prefix delegation
* Add larger subnets or secondary CIDR
* Monitor IP availability proactively

ğŸ‘‰ IP exhaustion in Kubernetes is a **network capacity planning issue**, not an application issue â€” fix by expanding IP pool without stopping traffic.

----
## Q14: A critical pod is being evicted every 10 minutes due to resource pressure. What is your resolution strategy?

---

### ğŸ§  Overview

If a pod is being **Evicted**, Kubernetes is protecting node stability.

Common eviction reasons:

* `MemoryPressure`
* `DiskPressure`
* `PIDPressure`
* Node ephemeral storage full

Important:

> Eviction â‰  Crash
> Itâ€™s the kubelet proactively removing pods to protect the node.

If it happens every 10 minutes â†’ underlying node pressure is persistent.

Goal:

1. Identify pressure type
2. Stabilize node
3. Protect critical workload
4. Prevent recurrence

---

# ğŸ” Phase 1: Identify Exact Eviction Reason

---

## 1ï¸âƒ£ Describe the Pod

```bash
kubectl describe pod <pod-name> -n prod
```

Look for:

```
Status: Failed
Reason: Evicted
Message: The node had condition: MemoryPressure
```

---

## 2ï¸âƒ£ Check Node Conditions

```bash
kubectl describe node <node-name>
```

Look for:

```
Conditions:
  MemoryPressure   True
  DiskPressure     True
```

---

# ğŸ§© Common Causes & Fixes

---

# ğŸ”¥ Scenario 1: MemoryPressure (Most Common)

Node memory is exhausted.

Check:

```bash
kubectl top nodes
```

If usage ~90â€“100% â†’ pressure confirmed.

---

## Immediate Fix

### 1ï¸âƒ£ Scale Node Group

```bash
aws autoscaling update-auto-scaling-group \
  --auto-scaling-group-name eks-prod-ng \
  --desired-capacity 15
```

---

### 2ï¸âƒ£ Reduce Non-Critical Workloads

```bash
kubectl scale deployment batch-jobs --replicas=0
```

Protect critical service first.

---

## Long-Term Fix

* Increase node size
* Tune pod memory requests
* Enable HPA
* Add cluster autoscaler

---

# ğŸ”¥ Scenario 2: DiskPressure

Check:

```bash
df -h
```

Or inside node:

```bash
kubectl debug node/<node-name> -it --image=busybox
```

Common causes:

* Large logs
* Image buildup
* Ephemeral storage overflow

---

## Immediate Fix

Clean unused images:

```bash
docker system prune -af
```

Or configure kubelet:

```yaml
--image-gc-high-threshold=80
--image-gc-low-threshold=60
```

---

# ğŸ”¥ Scenario 3: Critical Pod Has Low Priority

If node under pressure, low-priority pods evicted first.

Check:

```bash
kubectl get priorityclass
```

Set higher priority:

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-priority
value: 100000
```

Apply to pod:

```yaml
priorityClassName: critical-priority
```

---

# ğŸ”¥ Scenario 4: Resource Requests Misconfigured

If critical pod has low request:

```yaml
requests:
  memory: "100Mi"
```

Scheduler underestimates.

Fix:

```yaml
requests:
  memory: "1Gi"
limits:
  memory: "2Gi"
```

---

# ğŸš¨ Phase 2: Protect Critical Pod

---

## 1ï¸âƒ£ Use PodDisruptionBudget (PDB)

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
spec:
  minAvailable: 1
```

Prevents voluntary eviction during drain.

---

## 2ï¸âƒ£ Taint Dedicated Nodes

```bash
kubectl taint nodes node1 dedicated=critical:NoSchedule
```

Add toleration to pod:

```yaml
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "critical"
  effect: "NoSchedule"
```

---

# ğŸ“Š Diagnosis Matrix

| Eviction Type     | Immediate Action  | Long-Term Fix              |
| ----------------- | ----------------- | -------------------------- |
| MemoryPressure    | Scale nodes       | Right-size resources       |
| DiskPressure      | Clean images/logs | Larger disk / log rotation |
| PIDPressure       | Reduce processes  | Tune limits                |
| Ephemeral storage | Increase storage  | Set storage limits         |

---

# ğŸ”’ Preventive Controls

---

## âœ… 1ï¸âƒ£ Maintain Headroom

Never run cluster at 100%.

Target: 60â€“70% utilization.

---

## âœ… 2ï¸âƒ£ Proper Resource Requests

Avoid zero requests.

---

## âœ… 3ï¸âƒ£ Enable Cluster Autoscaler

Must scale before pressure occurs.

---

## âœ… 4ï¸âƒ£ Add Alerts

Prometheus:

```promql
kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
```

Alert before eviction wave.

---

## âœ… 5ï¸âƒ£ Separate Critical & Batch Workloads

Use:

* Node groups
* Taints
* Priority classes

---

# ğŸ“‹ Resolution Strategy (Interview Flow)

| Step | Action                                |
| ---- | ------------------------------------- |
| 1    | Identify eviction reason              |
| 2    | Check node conditions                 |
| 3    | Scale nodes immediately               |
| 4    | Protect critical pod (priority class) |
| 5    | Tune resource requests                |
| 6    | Implement autoscaling                 |

---

# âš ï¸ What NOT to Do

* âŒ Just restart pod repeatedly
* âŒ Ignore node-level pressure
* âŒ Increase limits without increasing node capacity
* âŒ Run critical and batch workloads on same nodes

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Eviction = node resource protection
* Identify Memory/Disk/PID pressure
* Scale nodes immediately
* Use priority classes for critical pods
* Maintain cluster headroom

ğŸ‘‰ Frequent eviction means **node capacity planning failure or workload misconfiguration**, not just a pod problem.

----
## Q15: Your team wants to migrate a monolithic application to microservices on Kubernetes. How do you plan and execute this?

---

### ğŸ§  Overview

Migrating from **Monolith â†’ Microservices on Kubernetes** is not just a tech change â€” it's:

* Architecture redesign
* Deployment model change
* CI/CD transformation
* Data strategy evolution

Principle:

> Do not rewrite everything at once.
> Use **Strangler Pattern** and migrate incrementally.

Goal:

1. Reduce risk
2. Maintain business continuity
3. Improve scalability & release velocity

---

# ğŸ— Phase 1: Assessment & Planning

---

## ğŸ” 1ï¸âƒ£ Analyze the Monolith

Identify:

* Modules (Auth, Orders, Payments, Reporting)
* DB schema dependencies
* High-change areas
* High-traffic components

Questions:

* Which modules change most often?
* Which modules scale independently?
* Which modules are bottlenecks?

---

## ğŸ” 2ï¸âƒ£ Identify Extraction Candidates

Good first microservices:

* Authentication
* Notifications
* Payments
* Reporting

Avoid extracting tightly coupled core logic first.

---

# ğŸ§± Phase 2: Containerize the Monolith (Step 0)

Before microservices:

* Dockerize monolith
* Deploy monolith to Kubernetes
* Establish CI/CD pipeline

Example Dockerfile:

```dockerfile
FROM openjdk:17
COPY target/app.jar app.jar
ENTRYPOINT ["java","-jar","/app.jar"]
```

Deploy to K8s:

```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
```

This gives:

* Zero-downtime deploys
* Autoscaling
* Observability

---

# ğŸ” Phase 3: Strangler Pattern Execution

Architecture:

```
Users
  â†“
Ingress / API Gateway
  â†“
Monolith + New Microservices
```

Gradually redirect traffic.

---

## ğŸ”¹ Step 1: Add API Gateway

Use:

* NGINX Ingress
* Kong
* AWS API Gateway

Route:

```
/auth â†’ auth-service
/orders â†’ monolith
```

---

## ğŸ”¹ Step 2: Extract First Service

Example: Auth Service

1. Copy auth logic
2. Expose REST API
3. Monolith calls new service
4. Deploy independently

---

## ğŸ”¹ Step 3: Database Strategy

Avoid shared DB long-term.

Options:

| Strategy          | Risk          | Recommended     |
| ----------------- | ------------- | --------------- |
| Shared DB         | High coupling | Temporary only  |
| DB per service    | Best          | âœ…               |
| Event-driven sync | Advanced      | Ideal long-term |

Start with shared DB, then gradually separate.

---

# ğŸš€ Phase 4: CI/CD Architecture Upgrade

---

## Old (Monolith)

* Single pipeline
* One deployment

## New (Microservices)

* Pipeline per service
* Independent deployment
* Independent versioning

Example GitHub Actions:

```yaml
on:
  push:
    paths:
      - "auth-service/**"
```

Only build changed service.

---

# ğŸ§© Phase 5: Introduce Service Mesh (Optional Advanced)

Use:

* Istio
* Linkerd

Benefits:

* Traffic splitting
* mTLS
* Observability
* Canary deploys

---

# ğŸ“Š Phase 6: Observability & Monitoring

Mandatory before scaling microservices:

* Prometheus
* Grafana
* Distributed tracing (Jaeger)
* Central logging (ELK)

Example tracing:

```
User â†’ API Gateway â†’ Auth â†’ Order â†’ Payment
```

Without tracing â†’ debugging nightmare.

---

# ğŸ”’ Phase 7: Production Hardening

---

## âœ… 1ï¸âƒ£ Implement Circuit Breakers

Prevents cascading failure.

---

## âœ… 2ï¸âƒ£ Add Resource Limits

```yaml
resources:
  requests:
    cpu: 200m
  limits:
    cpu: 500m
```

---

## âœ… 3ï¸âƒ£ Add HPA

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
```

---

## âœ… 4ï¸âƒ£ Secure with RBAC & Network Policies

```yaml
kind: NetworkPolicy
```

---

# ğŸ“‹ Migration Roadmap (Realistic Timeline)

| Phase                 | Timeline     |
| --------------------- | ------------ |
| Containerize monolith | 2â€“4 weeks    |
| Deploy to K8s         | 2 weeks      |
| Extract 1st service   | 4â€“6 weeks    |
| Add API gateway       | Parallel     |
| Gradual extraction    | 6â€“12 months  |
| Full decomposition    | 12â€“18 months |

Never a 2-month project.

---

# âš ï¸ Risks to Manage

* Distributed system complexity
* Data consistency issues
* Increased operational overhead
* Latency between services
* Dev team readiness

---

# ğŸ”¥ Anti-Patterns

* âŒ Big-bang rewrite
* âŒ Shared database forever
* âŒ No observability
* âŒ Too many tiny services
* âŒ Ignoring DevOps maturity

---

# ğŸ“ˆ Target Architecture (Final State)

```
Route53
   â†“
ALB / Ingress
   â†“
API Gateway
   â†“
Microservices (independent)
   â†“
DB per service
   â†“
Shared Observability Stack
```

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Containerize monolith first
* Use Strangler Pattern
* Extract services incrementally
* Separate databases gradually
* Build strong CI/CD & observability

ğŸ‘‰ Monolith â†’ Microservices is a **gradual, controlled architectural evolution**, not a rewrite project.

---
## Q16: Your AWS bill doubled unexpectedly this month. How do you identify the cause and prevent it from happening again?

---

### ğŸ§  Overview

If AWS bill suddenly doubled:

> Assume misconfiguration, scaling issue, or abuse â€” not randomness.

Most common causes:

* Auto-scaling misfire
* Orphaned resources
* Data transfer spike
* NAT Gateway costs
* EBS / snapshot growth
* RDS storage autoscaling
* Exposed credentials (crypto mining)

Goal:

1. Identify cost driver quickly
2. Stop bleeding immediately
3. Prevent recurrence with guardrails

---

# ğŸ” Phase 1: Identify Where the Cost Increased

---

## 1ï¸âƒ£ Use Cost Explorer

CLI:

```bash
aws ce get-cost-and-usage \
  --time-period Start=2026-02-01,End=2026-02-28 \
  --granularity DAILY \
  --metrics "UnblendedCost" \
  --group-by Type=DIMENSION,Key=SERVICE
```

Look for:

* EC2?
* RDS?
* Data Transfer?
* NAT Gateway?
* S3?
* EKS?

---

## 2ï¸âƒ£ Compare Month-over-Month

Check:

* Daily trend
* Service breakdown
* Region breakdown

---

# ğŸ§© Common Cost Spike Scenarios

---

# ğŸ”¥ Scenario 1: EC2 / EKS Node Explosion

Check EC2:

```bash
aws ec2 describe-instances \
  --query 'Reservations[*].Instances[*].[InstanceId,InstanceType,State.Name]'
```

Check Auto Scaling:

```bash
aws autoscaling describe-auto-scaling-groups
```

Possible causes:

* HPA misconfigured
* Infinite scaling loop
* Test environment left running
* Cluster Autoscaler max set too high

---

### Immediate Fix

Reduce capacity:

```bash
aws autoscaling update-auto-scaling-group \
  --auto-scaling-group-name prod-ng \
  --desired-capacity 5
```

---

# ğŸ”¥ Scenario 2: NAT Gateway Data Transfer

NAT Gateway can cost heavily.

Check:

* Data transfer out
* Cross-AZ traffic

Solution:

* Use VPC endpoints (S3, DynamoDB)
* Reduce cross-AZ calls
* Consolidate NAT

---

# ğŸ”¥ Scenario 3: Data Transfer Spike

Check:

```bash
aws ce get-cost-and-usage --group-by Type=DIMENSION,Key=USAGE_TYPE
```

Look for:

* DataTransfer-Out-Bytes
* Inter-AZ traffic

Common reason:

* Large S3 downloads
* Logs exported externally
* Misconfigured backup jobs

---

# ğŸ”¥ Scenario 4: RDS Storage / IOPS

Check:

```bash
aws rds describe-db-instances
```

Possible issues:

* Storage autoscaling triggered
* Provisioned IOPS too high
* Large snapshot accumulation

---

# ğŸ”¥ Scenario 5: S3 Growth

Check bucket size:

```bash
aws s3 ls s3://bucket-name --recursive --summarize
```

Common causes:

* Log retention not configured
* CI artifacts stored indefinitely
* Backup duplication

---

# ğŸ”¥ Scenario 6: Compromised Credentials (Critical)

If EC2 GPU instances launched unexpectedly:

Check CloudTrail:

```bash
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=EventName,AttributeValue=RunInstances
```

Look for:

* Unknown IP
* Unusual regions

Immediate action:

* Rotate keys
* Disable compromised IAM user
* Terminate suspicious instances

---

# ğŸ“Š Cost Breakdown Table

| Service       | Common Cause                 |
| ------------- | ---------------------------- |
| EC2           | Scaling misconfig            |
| EKS           | Node group expansion         |
| NAT Gateway   | High data transfer           |
| RDS           | Storage/IOPS                 |
| S3            | No lifecycle policy          |
| Data Transfer | Cross-AZ or internet traffic |

---

# ğŸ›  Phase 2: Stop Immediate Cost Leak

| Problem           | Action                |
| ----------------- | --------------------- |
| Over-scaled nodes | Reduce ASG capacity   |
| Unused instances  | Terminate             |
| Large S3 bucket   | Enable lifecycle      |
| NAT cost          | Add VPC endpoints     |
| Idle RDS          | Resize                |
| Orphan EBS        | Delete unused volumes |

---

# ğŸ”’ Phase 3: Prevent Recurrence

---

## âœ… 1ï¸âƒ£ Enable AWS Budgets

```bash
aws budgets create-budget ...
```

Alert at:

* 80%
* 100%
* 120%

---

## âœ… 2ï¸âƒ£ Enable Cost Anomaly Detection

AWS â†’ Cost Anomaly Detection â†’ Configure monitor

Detects sudden spikes automatically.

---

## âœ… 3ï¸âƒ£ Tag Everything

Enforce tagging policy:

```json
{
  "CostCenter": "prod",
  "Environment": "production"
}
```

Allows cost allocation visibility.

---

## âœ… 4ï¸âƒ£ Set ASG Scaling Limits

Never leave:

```json
"MaxSize": 1000
```

Use realistic limits.

---

## âœ… 5ï¸âƒ£ Use Reserved Instances / Savings Plans

For stable workloads.

---

## âœ… 6ï¸âƒ£ Implement Cleanup Automation

Lambda job:

* Delete unattached EBS
* Delete old snapshots
* Delete old AMIs

---

# ğŸ“‹ Investigation Flow (Interview Style)

| Step | Action                                |
| ---- | ------------------------------------- |
| 1    | Check Cost Explorer                   |
| 2    | Identify top service increase         |
| 3    | Drill down by region & usage type     |
| 4    | Inspect scaling & resource count      |
| 5    | Check for credential abuse            |
| 6    | Implement budgets & anomaly detection |

---

# âš ï¸ What NOT to Do

* âŒ Panic-delete resources blindly
* âŒ Ignore cross-AZ traffic
* âŒ Leave test environments running
* âŒ Avoid tagging

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Use Cost Explorer to find spike source
* Most common: EC2 scaling or data transfer
* Reduce capacity immediately
* Enable budgets + anomaly detection
* Enforce tagging and scaling limits

ğŸ‘‰ Sudden AWS bill increase is almost always **auto-scaling misconfiguration, data transfer explosion, or credential misuse** â€” measure first, then fix systematically.

---
## Q17: You need to migrate a large on-premises database to AWS with less than 1 hour of downtime. How do you approach this?

---

### ğŸ§  Overview

Migrating a **large on-prem DB â†’ AWS** with < 1 hour downtime requires:

> Continuous replication + short final cutover window.

You do NOT:

* Take full backup â†’ restore â†’ wait (too slow)
* Do big-bang migration

Correct approach:

1. Initial full load
2. Continuous Change Data Capture (CDC)
3. Short cutover freeze
4. DNS/app switch

Goal:

* Data consistency
* Minimal downtime
* Validated rollback plan

---

# ğŸ— High-Level Migration Architecture

```
On-Prem DB
     â†“
AWS DMS (Replication Instance)
     â†“
AWS RDS / Aurora
```

Tools:

* AWS DMS (Database Migration Service)
* Native replication (MySQL/Postgres logical replication)
* Backup + restore (only for small DBs)

---

# ğŸš€ Phase 1: Pre-Migration Planning

---

## ğŸ” 1ï¸âƒ£ Assess Database

Check:

* DB size
* Write rate (TPS)
* Large tables
* Stored procedures
* Extensions compatibility

---

## ğŸ” 2ï¸âƒ£ Choose Target

Options:

| Option           | Use Case           |
| ---------------- | ------------------ |
| RDS              | Standard migration |
| Aurora           | High scalability   |
| EC2 self-managed | Complex custom DB  |
| RDS Multi-AZ     | HA requirement     |

---

## ğŸ” 3ï¸âƒ£ Network Connectivity

Options:

* VPN
* Direct Connect (preferred for large DB)
* Secure replication tunnel

Ensure low-latency stable link.

---

# ğŸ”„ Phase 2: Continuous Replication Setup

---

## ğŸ”¹ 1ï¸âƒ£ Create Target DB

Example (RDS):

```bash
aws rds create-db-instance \
  --db-instance-identifier prod-db \
  --engine postgres \
  --multi-az \
  --allocated-storage 500
```

---

## ğŸ”¹ 2ï¸âƒ£ Configure AWS DMS

Create replication instance:

```bash
aws dms create-replication-instance \
  --replication-instance-class dms.r5.large
```

---

## ğŸ”¹ 3ï¸âƒ£ Create Source & Target Endpoints

```bash
aws dms create-endpoint --endpoint-type source
aws dms create-endpoint --endpoint-type target
```

---

## ğŸ”¹ 4ï¸âƒ£ Enable CDC Mode

DMS Task:

* Full load + ongoing replication

This:

* Copies entire DB
* Then streams live changes

Downtime = 0 during sync phase.

---

# ğŸ“Š Phase 3: Monitor Replication Lag

Critical metric:

* `CDCLatencySource`
* `CDCLatencyTarget`

Ensure replication lag = near zero before cutover.

---

# ğŸ”¥ Phase 4: Cutover Strategy (<1 Hour Downtime)

---

## Step 1ï¸âƒ£ Announce Maintenance Window

Switch application to:

* Read-only mode
* Or disable writes

---

## Step 2ï¸âƒ£ Stop Application Writes

Prevent new transactions.

---

## Step 3ï¸âƒ£ Wait for Replication Catch-Up

Check DMS lag = 0 seconds.

---

## Step 4ï¸âƒ£ Final Validation

* Row counts
* Checksums
* Critical queries

---

## Step 5ï¸âƒ£ Switch Application DB Endpoint

Update:

* RDS endpoint
* Kubernetes secret

```bash
kubectl create secret generic db-secret \
  --from-literal=DB_HOST=new-rds-endpoint
```

Restart pods:

```bash
kubectl rollout restart deployment/app -n prod
```

---

## Step 6ï¸âƒ£ Monitor Closely

* Query performance
* Error rate
* Connection count

Downtime typically: 10â€“30 minutes.

---

# ğŸ”„ Rollback Plan (Critical)

Keep:

* On-prem DB running
* Replication paused, not deleted

If failure:

* Re-point app back to on-prem
* Resume traffic

Always test rollback beforehand.

---

# ğŸ“¦ Special Cases

---

## ğŸ”¹ Large Databases (TBs)

Use:

* Parallel load
* Table partition migration
* Pre-warm indexes

---

## ğŸ”¹ Zero Downtime Migration (Advanced)

Use:

* Dual-write pattern
* Or bidirectional replication temporarily

Complex but near-zero downtime.

---

# ğŸ”’ Production Best Practices

---

## âœ… 1ï¸âƒ£ Dry Run Migration in Staging

Simulate full process.

---

## âœ… 2ï¸âƒ£ Validate Schema Compatibility

Check:

* Data types
* Collation
* Extensions

---

## âœ… 3ï¸âƒ£ Tune RDS Parameters

Match on-prem settings:

* max_connections
* shared_buffers
* innodb_buffer_pool_size

---

## âœ… 4ï¸âƒ£ Enable Multi-AZ in AWS

Avoid migrating into single point of failure.

---

## âœ… 5ï¸âƒ£ Load Test Before Switching

Ensure AWS DB handles expected traffic.

---

# ğŸ“‹ Migration Timeline Example

| Phase             | Duration     |
| ----------------- | ------------ |
| Initial full load | Hours / Days |
| CDC sync          | Continuous   |
| Final cutover     | 15â€“45 mins   |
| Monitoring        | 24â€“48 hours  |

---

# âš ï¸ What NOT to Do

* âŒ Big-bang restore during downtime
* âŒ Skip replication lag monitoring
* âŒ Ignore rollback plan
* âŒ Migrate without load testing

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Use AWS DMS with CDC
* Sync continuously before cutover
* Freeze writes briefly
* Switch endpoint
* Keep rollback ready

ğŸ‘‰ Large DB migration with <1 hour downtime = **replicate first, switch later**, never copy during downtime window.

----
## Q18: Your entire AWS us-east-1 region is experiencing an outage. How does your architecture handle this?

---

### ğŸ§  Overview

If an entire AWS region (e.g., **us-east-1**) is down:

> AZ redundancy is NOT enough.
> You need Multi-Region architecture.

This is a **Disaster Recovery (DR)** scenario.

Your design must already include:

* Cross-region infrastructure
* Cross-region data replication
* Automated failover
* Tested DR runbooks

Goal:

1. Maintain availability
2. Protect data
3. Fail over with minimal downtime

---

# ğŸ— Production-Grade Multi-Region Architecture

```
                Route53 (Health Check + Failover)
                        â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                             â”‚
     us-east-1 (Primary)         us-west-2 (Secondary)
         â”‚                             â”‚
      ALB + EKS                      ALB + EKS
         â”‚                             â”‚
      RDS (Primary)            RDS Read Replica / Global DB
```

---

# ğŸ” Strategy Options (DR Models)

---

# ğŸ”¹ 1ï¸âƒ£ Backup & Restore (Lowest Cost)

* Only backups stored cross-region
* Manual restore in secondary

â± RTO: Hours
ğŸ“‰ RPO: Minutesâ€“Hours

Not suitable for critical systems.

---

# ğŸ”¹ 2ï¸âƒ£ Pilot Light

* Core infra running in secondary
* App scaled down

Failover = scale up + switch DNS.

â± RTO: 30â€“60 mins
ğŸ“‰ RPO: Low

---

# ğŸ”¹ 3ï¸âƒ£ Warm Standby (Recommended for Critical Systems)

* Full stack running in secondary
* Smaller capacity
* Near real-time DB replication

Failover = DNS switch.

â± RTO: 5â€“15 mins
ğŸ“‰ RPO: Seconds

---

# ğŸ”¹ 4ï¸âƒ£ Active-Active (Highest Availability)

* Both regions serving traffic
* Global load balancing

â± RTO: Near zero
ğŸ“‰ RPO: Near zero

More complex and expensive.

---

# ğŸš€ How Architecture Handles Region Outage

---

# 1ï¸âƒ£ DNS Failover (Route53)

Health checks:

```bash
aws route53 create-health-check ...
```

Failover routing policy:

* Primary record â†’ us-east-1
* Secondary record â†’ us-west-2

If primary fails â†’ traffic shifts automatically.

---

# 2ï¸âƒ£ Kubernetes in Secondary Region

EKS cluster pre-provisioned via Terraform:

```hcl
module "eks_secondary" {
  region = "us-west-2"
}
```

Infra already exists.

---

# 3ï¸âƒ£ Database Replication

Options:

### ğŸ”¹ RDS Cross-Region Read Replica

```bash
aws rds create-db-instance-read-replica \
  --source-db-instance-identifier prod-db
```

Promote on failover:

```bash
aws rds promote-read-replica
```

---

### ğŸ”¹ Aurora Global Database (Better)

* Multi-region replication
* Faster failover

---

### ğŸ”¹ DynamoDB Global Tables

Auto multi-region replication.

---

# 4ï¸âƒ£ S3 Cross-Region Replication

```bash
aws s3api put-bucket-replication ...
```

Ensures object availability.

---

# 5ï¸âƒ£ Container Images (ECR Replication)

Enable ECR cross-region replication.

Prevents image pull failure.

---

# ğŸ“‹ Failover Execution Flow

| Step | Action                  |
| ---- | ----------------------- |
| 1    | Detect regional outage  |
| 2    | Promote DB replica      |
| 3    | Scale secondary cluster |
| 4    | Route53 shifts traffic  |
| 5    | Monitor stability       |
| 6    | Communicate status      |

---

# ğŸ”„ After Primary Region Recovery

* Decide failback strategy
* Sync data
* Gradually shift traffic back

Never flip instantly without validation.

---

# ğŸ”’ Production Best Practices

---

## âœ… 1ï¸âƒ£ Infrastructure as Code in Both Regions

Terraform example:

```hcl
provider "aws" {
  alias  = "secondary"
  region = "us-west-2"
}
```

---

## âœ… 2ï¸âƒ£ Automated DR Testing

Quarterly game-day exercises.

Simulate region failure.

---

## âœ… 3ï¸âƒ£ Separate State Stores

Avoid single-region dependencies like:

* Single S3 backend
* Single Redis
* Single Kafka cluster

---

## âœ… 4ï¸âƒ£ Global Monitoring

Use external monitoring (Datadog, NewRelic).

If monitoring is only in us-east-1 â†’ blind during outage.

---

## âœ… 5ï¸âƒ£ Define RTO & RPO Clearly

| System Type   | RTO      | RPO     |
| ------------- | -------- | ------- |
| Banking       | <5 min   | <1 min  |
| SaaS app      | <30 min  | <5 min  |
| Internal tool | <4 hours | <30 min |

Design based on business need.

---

# âš ï¸ What NOT to Do

* âŒ Rely only on Multi-AZ
* âŒ Store Terraform state in single region
* âŒ Assume AWS never fails region-wide
* âŒ Skip DR testing

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Multi-AZ â‰  Multi-Region
* Use cross-region replication
* Route53 health-based failover
* Promote replica and shift traffic
* Regularly test DR

ğŸ‘‰ Region outage resilience requires **pre-built multi-region architecture**, not last-minute recovery steps.

---
## Q19: A Terraform apply accidentally destroyed production infrastructure. What is your recovery process?

---

### ğŸ§  Overview

If `terraform apply` destroyed production infra:

> Treat it as SEV-1 incident.

Root causes usually:

* Wrong workspace
* Wrong backend
* `terraform destroy` misuse
* State file corruption
* Missing state lock
* Drift between state & actual infra

Goal:

1. Stop further damage
2. Restore infrastructure fast
3. Recover state safely
4. Prevent recurrence

---

# ğŸš¨ Phase 1: Immediate Containment

---

## 1ï¸âƒ£ Stop All Terraform Activity

Disable CI pipeline immediately.

Revoke access temporarily if needed.

---

## 2ï¸âƒ£ Identify Scope of Damage

Check:

* What resources destroyed?
* Entire VPC? Only ASG?
* DB affected?

Use:

```bash
terraform state list
```

Check CloudTrail for deletions:

```bash
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=EventName,AttributeValue=Delete*
```

---

# ğŸ”¥ Phase 2: Check If Data Is Lost

Critical priority:

* RDS
* S3
* EBS
* DynamoDB

---

## 1ï¸âƒ£ RDS Recovery

If RDS deleted but backup enabled:

```bash
aws rds restore-db-instance-to-point-in-time \
  --source-db-instance-identifier prod-db \
  --target-db-instance-identifier prod-db-restore \
  --restore-time 2026-02-14T10:00:00Z
```

---

## 2ï¸âƒ£ EBS Snapshot Recovery

```bash
aws ec2 create-volume \
  --snapshot-id snap-xxxx \
  --availability-zone us-east-1a
```

---

## 3ï¸âƒ£ S3 Versioning Recovery

If versioning enabled:

```bash
aws s3api list-object-versions --bucket prod-bucket
```

Restore previous version.

---

# ğŸ”„ Phase 3: Restore Infrastructure

---

# ğŸ”¹ Scenario 1: State File Still Intact

If state backend (S3) exists:

```bash
terraform plan
terraform apply
```

Terraform will recreate destroyed resources.

---

# ğŸ”¹ Scenario 2: State Corrupted or Missing

Restore state from S3 versioning:

```bash
aws s3api list-object-versions --bucket terraform-state-bucket
```

Download previous version.

Replace state:

```bash
terraform init
terraform state push terraform.tfstate
```

---

# ğŸ”¹ Scenario 3: Entire Infra Destroyed

Reapply from clean state:

```bash
terraform init
terraform apply
```

Requires:

* Modular design
* Proper variable files
* No manual infra

---

# ğŸ“‹ Recovery Flow Example

| Step | Action                      |
| ---- | --------------------------- |
| 1    | Stop pipelines              |
| 2    | Assess destroyed resources  |
| 3    | Recover databases first     |
| 4    | Restore infra via Terraform |
| 5    | Validate connectivity       |
| 6    | Re-enable traffic           |

---

# ğŸ” Phase 4: Restore Application Traffic

If using:

* ALB
* Route53

Ensure:

```bash
aws elbv2 describe-target-health
```

Pods healthy:

```bash
kubectl get pods -n prod
```

Gradually restore traffic.

---

# ğŸ”’ Phase 5: Root Cause Analysis

Check:

* Was wrong workspace used?

```bash
terraform workspace show
```

* Was prod backend misconfigured?
* Was state locking disabled?
* Was CI variable incorrect?

---

# ğŸ›¡ Prevent Recurrence

---

## âœ… 1ï¸âƒ£ Use Remote Backend with Locking

Example:

```hcl
backend "s3" {
  bucket         = "terraform-prod-state"
  key            = "prod/terraform.tfstate"
  region         = "us-east-1"
  dynamodb_table = "terraform-locks"
}
```

Prevents concurrent applies.

---

## âœ… 2ï¸âƒ£ Use Separate AWS Accounts for Prod

Never share prod and staging account.

---

## âœ… 3ï¸âƒ£ Use `-auto-approve` Carefully

Remove from prod pipeline.

Require manual approval.

---

## âœ… 4ï¸âƒ£ Add `prevent_destroy`

For critical resources:

```hcl
lifecycle {
  prevent_destroy = true
}
```

Protects DB, VPC, etc.

---

## âœ… 5ï¸âƒ£ CI Guardrails

In pipeline:

* `terraform plan` output must be reviewed
* Block destroy unless explicitly approved

Example:

```bash
terraform plan -detailed-exitcode
```

Fail if destroy detected.

---

## âœ… 6ï¸âƒ£ Enable S3 Versioning for State

Prevents permanent loss.

---

# âš ï¸ What NOT to Do

* âŒ Panic-run more Terraform commands
* âŒ Modify state manually without backup
* âŒ Ignore database backup strategy
* âŒ Use same workspace for all environments

---

# ğŸ“Š Common Root Causes

| Cause               | Example                               |
| ------------------- | ------------------------------------- |
| Wrong workspace     | Running prod in dev                   |
| State misconfigured | Local state instead of S3             |
| Manual drift        | Terraform deletes unmanaged resources |
| CI variable error   | Wrong AWS credentials                 |
| No locking          | Parallel apply collision              |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Stop further changes immediately
* Restore data first (RDS, S3, EBS)
* Reapply infra using state or restore state backup
* Add prevent_destroy + locking
* Enforce CI approval gates

ğŸ‘‰ Terraform destroying prod is a **process failure**, not just technical â€” fix guardrails after recovery.

----
## Q20: You need to provision identical infrastructure across 50 AWS accounts automatically. How do you design this?

---

### ğŸ§  Overview

Provisioning identical infra across **50 AWS accounts** requires:

> Centralized governance + automated multi-account orchestration.

You do NOT:

* Manually run Terraform 50 times
* Share one IAM user across accounts
* Copy-paste configs

Correct approach:

1. Use **AWS Organizations**
2. Use **centralized IaC (Terraform)**
3. Use **cross-account IAM roles**
4. Use **automation pipeline**

Goal:

* Consistency
* Security
* Scalability
* Auditability

---

# ğŸ— High-Level Architecture

```
AWS Organizations
        â†“
Management Account (CI/CD + Terraform)
        â†“ (AssumeRole)
Member Accounts (50)
        â†“
Provisioned Infrastructure
```

---

# ğŸš€ Step 1: AWS Organizations Setup

---

## Use AWS Organizations

Create:

* Organizational Units (OUs)
* Separate accounts per environment

Example:

```
Root
 â”œâ”€â”€ Prod OU
 â”œâ”€â”€ Dev OU
 â”œâ”€â”€ Sandbox OU
```

---

# ğŸ” Step 2: Cross-Account Role Setup

Each member account has a role:

```json
{
  "RoleName": "TerraformExecutionRole",
  "AssumeRolePolicyDocument": {
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::<management-account-id>:root"
        },
        "Action": "sts:AssumeRole"
      }
    ]
  }
}
```

Terraform assumes role dynamically.

---

# ğŸ§± Step 3: Terraform Multi-Account Design

---

## ğŸ”¹ Provider with Assume Role

```hcl
provider "aws" {
  alias  = "account1"
  region = "us-east-1"

  assume_role {
    role_arn = "arn:aws:iam::123456789012:role/TerraformExecutionRole"
  }
}
```

---

## ğŸ”¹ Dynamic Account Loop

```hcl
variable "accounts" {
  type = list(string)
}

provider "aws" {
  for_each = toset(var.accounts)

  alias  = each.key
  region = "us-east-1"

  assume_role {
    role_arn = "arn:aws:iam::${each.key}:role/TerraformExecutionRole"
  }
}
```

---

# ğŸ”„ Step 4: Use Terraform Modules

Reusable module:

```hcl
module "vpc" {
  source = "./modules/vpc"

  providers = {
    aws = aws.account1
  }

  cidr_block = "10.0.0.0/16"
}
```

Apply same module to all accounts.

---

# ğŸ”¥ Step 5: CI/CD Automation

Pipeline (Jenkins/GitHub Actions):

```groovy
stage('Terraform Apply') {
    steps {
        sh 'terraform init'
        sh 'terraform apply -auto-approve'
    }
}
```

Pipeline runs:

* For each account
* Or using dynamic providers

---

# ğŸ›¡ State Management Strategy

---

## Option 1ï¸âƒ£: Separate State per Account (Recommended)

S3 backend key structure:

```hcl
key = "prod/account-123/terraform.tfstate"
```

Ensures isolation.

---

## Option 2ï¸âƒ£: Terraform Cloud (Enterprise Scale)

Benefits:

* Centralized state
* RBAC
* Policy enforcement (Sentinel)

---

# ğŸ”’ Governance & Control

---

## âœ… 1ï¸âƒ£ Use SCPs (Service Control Policies)

Prevent:

* Root user misuse
* Unapproved regions
* Dangerous services

---

## âœ… 2ï¸âƒ£ Enforce Tagging

Terraform:

```hcl
default_tags {
  tags = {
    Environment = "production"
  }
}
```

---

## âœ… 3ï¸âƒ£ Central Logging

All accounts send:

* CloudTrail â†’ Central S3
* GuardDuty â†’ Security account

---

# ğŸ“Š Design Options Comparison

| Approach                 | Scale      | Control   | Complexity |
| ------------------------ | ---------- | --------- | ---------- |
| Manual Terraform         | Low        | Low       | Low        |
| Multi-provider Terraform | Medium     | High      | Medium     |
| Terraform Cloud          | High       | Very High | Medium     |
| AWS Control Tower        | Enterprise | High      | Medium     |

---

# ğŸ¢ Enterprise-Grade Architecture (Best Practice)

Use:

* AWS Control Tower
* Terraform modules
* Central CI/CD
* Role assumption
* Separate state per account

---

# âš ï¸ What NOT to Do

* âŒ Use same IAM user for all accounts
* âŒ Share same Terraform state
* âŒ Hardcode account IDs
* âŒ Allow manual drift

---

# ğŸ“‹ Execution Flow

| Step | Action                            |
| ---- | --------------------------------- |
| 1    | Create accounts via Organizations |
| 2    | Create cross-account IAM role     |
| 3    | Setup Terraform remote backend    |
| 4    | Use reusable modules              |
| 5    | Automate via CI/CD                |
| 6    | Monitor and enforce policies      |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Use AWS Organizations
* Assume cross-account roles
* Use reusable Terraform modules
* Separate state per account
* Automate via centralized pipeline

ğŸ‘‰ Multi-account provisioning at scale = **Organizations + AssumeRole + Terraform modules + centralized automation**.

---
## Q21: A security audit found that 60% of your EC2 instances have publicly exposed ports. How do you remediate this at scale?

---

### ğŸ§  Overview

60% of EC2 instances exposing public ports = **critical security posture failure**.

This usually means:

* Security Groups allow `0.0.0.0/0`
* Instances have public IPs unnecessarily
* No centralized governance
* Drift from IaC
* Lack of guardrails (SCP, Config rules)

Goal:

1. Identify exposed ports quickly
2. Reduce exposure immediately
3. Enforce centralized network design
4. Prevent recurrence

This is not a â€œpatch one serverâ€ problem â€” itâ€™s a **governance + architecture problem**.

---

# ğŸš¨ Phase 1: Identify All Publicly Exposed Instances

---

## 1ï¸âƒ£ Find Security Groups with Open Ingress

```bash
aws ec2 describe-security-groups \
  --query "SecurityGroups[?IpPermissions[?IpRanges[?CidrIp=='0.0.0.0/0']]]"
```

Look for:

* Port 22 (SSH)
* Port 3389 (RDP)
* Port 80/443 (if not behind ALB)
* Custom app ports

---

## 2ï¸âƒ£ Identify Instances with Public IP

```bash
aws ec2 describe-instances \
  --query "Reservations[*].Instances[?PublicIpAddress!=null].[InstanceId,PublicIpAddress]"
```

Combine both findings.

---

# ğŸ”¥ Phase 2: Immediate Risk Mitigation

---

# ğŸ”¹ Scenario A: SSH / RDP Open to World

Replace with SSM Session Manager.

---

### Step 1ï¸âƒ£ Remove Public Ingress

```bash
aws ec2 revoke-security-group-ingress \
  --group-id sg-xxxx \
  --protocol tcp \
  --port 22 \
  --cidr 0.0.0.0/0
```

---

### Step 2ï¸âƒ£ Use AWS SSM Instead of SSH

Attach IAM role:

```json
{
  "Effect": "Allow",
  "Action": [
    "ssm:StartSession"
  ],
  "Resource": "*"
}
```

Access:

```bash
aws ssm start-session --target i-xxxx
```

No public SSH needed.

---

# ğŸ”¹ Scenario B: Applications Directly Exposed

Proper architecture:

```
Internet â†’ ALB â†’ Private EC2
```

Steps:

1. Remove public IP from instances
2. Move instances to private subnets
3. Expose only ALB publicly

---

## Remove Public IP

```bash
aws ec2 modify-instance-attribute \
  --instance-id i-xxxx \
  --no-source-dest-check
```

(Or relaunch without public IP)

---

# ğŸ” Phase 3: Fix at Scale (Automated)

---

# ğŸ”¹ 1ï¸âƒ£ Use AWS Config Rule

Enable managed rule:

```
INCOMING_SSH_DISABLED
```

Auto-detect and flag violations.

---

# ğŸ”¹ 2ï¸âƒ£ Use AWS Firewall Manager

Centralized SG policy enforcement across accounts.

---

# ğŸ”¹ 3ï¸âƒ£ Use SCP to Block Public Ports

Example SCP:

```json
{
  "Effect": "Deny",
  "Action": "ec2:AuthorizeSecurityGroupIngress",
  "Condition": {
    "IpAddress": {
      "ec2:SourceIp": "0.0.0.0/0"
    }
  }
}
```

Prevents future public exposure.

---

# ğŸ”¹ 4ï¸âƒ£ Terraform Guardrail

Add validation:

```hcl
variable "allowed_cidr" {
  validation {
    condition     = var.allowed_cidr != "0.0.0.0/0"
    error_message = "Public access is not allowed."
  }
}
```

---

# ğŸ”¹ 5ï¸âƒ£ Run Bulk Remediation Script

Example script:

```bash
for sg in $(aws ec2 describe-security-groups \
  --query "SecurityGroups[?IpPermissions[?IpRanges[?CidrIp=='0.0.0.0/0']]].GroupId" \
  --output text); do
  aws ec2 revoke-security-group-ingress --group-id $sg ...
done
```

Run in controlled manner (non-prod first).

---

# ğŸ”’ Phase 4: Redesign Network Architecture

---

## Target Architecture

```
Public Subnet:
  - ALB
  - NAT Gateway

Private Subnet:
  - EC2
  - RDS
  - Internal services
```

Principles:

* No direct public IP on app servers
* Bastionless architecture (SSM)
* Only load balancers exposed

---

# ğŸ“Š Risk Categorization

| Port             | Risk Level | Action             |
| ---------------- | ---------- | ------------------ |
| 22               | Critical   | Remove immediately |
| 3389             | Critical   | Remove             |
| 80/443           | Medium     | Allow only via ALB |
| Custom App Ports | High       | Restrict CIDR      |

---

# ğŸ›¡ Preventive Controls

---

## âœ… 1ï¸âƒ£ Enforce Private Subnets by Default

Disable auto-assign public IP.

---

## âœ… 2ï¸âƒ£ Mandatory IaC

No manual console security group changes.

---

## âœ… 3ï¸âƒ£ Continuous Security Scanning

Use:

* AWS Security Hub
* Prowler
* ScoutSuite

---

## âœ… 4ï¸âƒ£ Network Segmentation

Use:

* Security Groups
* NACLs
* Network Policies (if EKS)

---

## âœ… 5ï¸âƒ£ Zero Trust Model

No open inbound traffic unless required.

---

# âš ï¸ What NOT to Do

* âŒ Remove rules blindly in production without traffic validation
* âŒ Break live services
* âŒ Assume port 80 is always safe
* âŒ Leave manual access paths

---

# ğŸ“‹ Remediation Strategy Summary

| Step | Action                      |
| ---- | --------------------------- |
| 1    | Identify exposed SGs        |
| 2    | Remove SSH/RDP from public  |
| 3    | Replace with SSM            |
| 4    | Move EC2 to private subnets |
| 5    | Enforce SCP + Config rules  |
| 6    | Automate detection          |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Identify SGs allowing `0.0.0.0/0`
* Remove public SSH/RDP
* Use SSM instead of SSH
* Move EC2 to private subnets
* Enforce SCP + Config guardrails

ğŸ‘‰ 60% exposure means **architecture and governance failure**, not just misconfiguration â€” fix at policy level, not instance level.

---
## Q22: GuardDuty detected unusual API calls being made from an IAM role at 3 AM. What is your incident response?

---

### ğŸ§  Overview

Unusual API activity from an IAM role at 3 AM = **possible credential compromise or abuse**.

Could be:

* Stolen temporary credentials
* SSRF attack (EC2 metadata abuse)
* CI/CD token leak
* Privilege escalation attempt
* Crypto mining setup

Principle:

> Contain first. Investigate second. Preserve evidence.

Goal:

1. Stop malicious activity immediately
2. Determine scope of compromise
3. Prevent lateral movement
4. Harden IAM posture

---

# ğŸš¨ Phase 1: Immediate Containment (First 5â€“10 Minutes)

---

## 1ï¸âƒ£ Identify the IAM Role

From GuardDuty finding:

* Role ARN
* Source IP
* API calls
* Region

---

## 2ï¸âƒ£ Disable the Role Temporarily

Fastest safe containment:

### Option A: Detach Policies

```bash
aws iam detach-role-policy \
  --role-name suspicious-role \
  --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
```

---

### Option B: Add Explicit Deny Policy

```json
{
  "Effect": "Deny",
  "Action": "*",
  "Resource": "*"
}
```

Attach immediately.

Stops active abuse without deleting role.

---

## 3ï¸âƒ£ If EC2 Role â†’ Isolate Instance

If role attached to EC2:

```bash
aws ec2 modify-instance-attribute \
  --instance-id i-xxxx \
  --groups sg-quarantine
```

Or:

* Remove from load balancer
* Restrict outbound traffic

---

# ğŸ” Phase 2: Investigate the Activity

---

## 1ï¸âƒ£ Check CloudTrail Logs

```bash
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=Username,AttributeValue=suspicious-role
```

Look for:

* `CreateUser`
* `AttachRolePolicy`
* `RunInstances`
* `PutBucketPolicy`
* `CreateAccessKey`

Red flags:

* IAM changes
* New admin users
* GPU instance launches

---

## 2ï¸âƒ£ Check Source IP

Was it:

* Known corporate IP?
* Unknown external IP?
* Foreign country?

If unknown â†’ high probability compromise.

---

## 3ï¸âƒ£ Check for Resource Creation

```bash
aws ec2 describe-instances
aws iam list-users
aws iam list-access-keys
```

Look for:

* Backdoor accounts
* Suspicious instances
* Crypto mining patterns

---

# ğŸ”¥ Phase 3: Eradication

---

## ğŸ”¹ 1ï¸âƒ£ Rotate Credentials

If IAM user involved:

```bash
aws iam update-access-key --status Inactive
```

Delete and recreate.

If EC2 role abused:

* Recreate role
* Reattach minimal permissions

---

## ğŸ”¹ 2ï¸âƒ£ Remove Backdoors

Delete:

* Suspicious IAM users
* Unknown policies
* Malicious EC2 instances

---

## ğŸ”¹ 3ï¸âƒ£ Check for Persistence Mechanisms

Attackers often:

* Modify bucket policies
* Add IAM trust relationships
* Create new roles
* Modify CloudTrail

Verify:

```bash
aws iam list-roles
aws s3api get-bucket-policy
```

---

# ğŸ›¡ Phase 4: Root Cause Analysis

Possible attack vectors:

| Vector               | Example                       |
| -------------------- | ----------------------------- |
| SSRF                 | App exposed EC2 metadata      |
| Leaked token         | Code pushed to GitHub         |
| CI/CD breach         | Exposed GitHub Actions secret |
| Over-permissive role | `*:*` permissions             |
| No MFA               | IAM user login                |

---

# ğŸ”’ Phase 5: Harden the Environment

---

## âœ… 1ï¸âƒ£ Enable GuardDuty Organization-Wide

Cover all accounts.

---

## âœ… 2ï¸âƒ£ Restrict IAM Role Permissions

Avoid:

```json
"Action": "*"
```

Use least privilege.

---

## âœ… 3ï¸âƒ£ Enforce IMDSv2

Prevent metadata theft:

```bash
aws ec2 modify-instance-metadata-options \
  --http-tokens required
```

---

## âœ… 4ï¸âƒ£ Enable SCP to Prevent Dangerous APIs

Example deny:

```json
{
  "Effect": "Deny",
  "Action": "iam:CreateUser",
  "Resource": "*"
}
```

---

## âœ… 5ï¸âƒ£ Enable CloudTrail in All Regions

Ensure logging cannot be disabled.

---

## âœ… 6ï¸âƒ£ Enable AWS Security Hub + Config

Central visibility.

---

# ğŸ“‹ Incident Response Timeline

| Step | Action                     |
| ---- | -------------------------- |
| 1    | Disable role               |
| 2    | Isolate instance           |
| 3    | Review CloudTrail          |
| 4    | Remove malicious resources |
| 5    | Rotate credentials         |
| 6    | Harden IAM                 |
| 7    | Postmortem                 |

---

# âš ï¸ What NOT to Do

* âŒ Immediately delete role without investigating
* âŒ Ignore possible lateral movement
* âŒ Skip credential rotation
* âŒ Disable logging accidentally

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Contain role immediately
* Investigate via CloudTrail
* Remove malicious resources
* Rotate credentials
* Harden IAM + enforce least privilege

ğŸ‘‰ Unusual API activity at odd hours is almost always **credential compromise or abuse** â€” contain fast and audit thoroughly.

---
## Q23: Your company needs to achieve SOC2 compliance in 3 months. What DevSecOps processes do you implement?

---

### ğŸ§  Overview

SOC2 (Service Organization Control 2) focuses on:

* ğŸ” Security
* ğŸ“¦ Availability
* ğŸ” Processing Integrity
* ğŸ”’ Confidentiality
* ğŸ§¾ Privacy

For most SaaS companies, the primary focus is **Security + Availability**.

In 3 months, the goal is:

> Implement auditable controls + automate evidence collection.

This is not a documentation-only exercise â€” it's **process + automation + monitoring**.

---

# ğŸ— Phase 1: Governance & Access Control (Weeks 1â€“3)

---

## ğŸ” 1ï¸âƒ£ Enforce IAM Best Practices

### Implement:

* Least privilege IAM
* No root usage
* MFA mandatory
* Role-based access (no shared users)

Example IAM policy:

```json
{
  "Effect": "Allow",
  "Action": "s3:GetObject",
  "Resource": "arn:aws:s3:::prod-bucket/*"
}
```

---

## ğŸ”¹ 2ï¸âƒ£ SSO + Central Identity

Use:

* AWS IAM Identity Center
* Okta / Azure AD

Enforce:

* MFA
* Offboarding automation

---

## ğŸ”¹ 3ï¸âƒ£ Access Review Process

Quarterly review:

```bash
aws iam list-users
aws iam list-roles
```

Document approval evidence.

---

# ğŸ›¡ Phase 2: Infrastructure Security Controls (Weeks 2â€“6)

---

## ğŸ”¥ 1ï¸âƒ£ Infrastructure as Code Only

All infra must be Terraform.

Enable state locking:

```hcl
backend "s3" {
  bucket         = "tf-prod-state"
  dynamodb_table = "tf-lock"
}
```

---

## ğŸ”¥ 2ï¸âƒ£ Security Scanning in CI/CD

Add:

* SAST (SonarQube)
* SCA (Dependency-Check / Trivy)
* Container scanning
* Terraform scanning (tfsec, checkov)

Example pipeline step:

```bash
trivy image app:latest
```

Fail build on high severity.

---

## ğŸ”¥ 3ï¸âƒ£ Secrets Management

Replace:

* `.env` files
* Hardcoded secrets

Use:

* AWS Secrets Manager
* SSM Parameter Store

Never store secrets in Git.

---

# ğŸ” Phase 3: Logging & Monitoring (Weeks 3â€“8)

---

## ğŸ”¹ 1ï¸âƒ£ Enable Organization-wide CloudTrail

All regions enabled.

Send logs to centralized S3.

---

## ğŸ”¹ 2ï¸âƒ£ Enable GuardDuty + Security Hub

Organization-level detection.

---

## ğŸ”¹ 3ï¸âƒ£ Centralized Logging

Use:

* ELK / OpenSearch
* CloudWatch Logs

Retention policy:

* 1 year minimum (based on policy)

---

## ğŸ”¹ 4ï¸âƒ£ Monitoring & Alerts

Prometheus / Datadog:

Alert on:

* High 5xx
* CPU spikes
* Unauthorized API calls

---

# ğŸ”’ Phase 4: Application Security (Weeks 4â€“9)

---

## âœ… 1ï¸âƒ£ Secure SDLC Process

Implement:

* PR reviews mandatory
* Code owner approvals
* No direct pushes to main

Example branch protection:

* Require 2 approvals
* Require passing CI

---

## âœ… 2ï¸âƒ£ Dependency Management

Automate:

* Dependabot
* Snyk

---

## âœ… 3ï¸âƒ£ Container Hardening

* Use minimal base images
* Run as non-root
* Add resource limits

```yaml
securityContext:
  runAsNonRoot: true
```

---

# ğŸ“¦ Phase 5: Backup & Disaster Recovery

---

## ğŸ”¹ 1ï¸âƒ£ Automated Backups

* RDS automated backups
* S3 versioning enabled
* EBS snapshots scheduled

---

## ğŸ”¹ 2ï¸âƒ£ Disaster Recovery Plan

Document:

* RTO
* RPO
* Multi-region setup
* Failover process

Run DR test once before audit.

---

# ğŸ“‹ Phase 6: Policies & Documentation

SOC2 requires documented controls:

* Access control policy
* Incident response policy
* Change management policy
* Backup policy
* Vendor risk policy

Create runbooks:

```
Incident Response Runbook
Terraform Deployment Runbook
Access Review SOP
```

Auditors check evidence, not theory.

---

# ğŸ” Phase 7: Change Management Control

---

## CI/CD Enforcement

No manual production changes.

Pipeline:

```groovy
stage('Terraform Plan')
stage('Approval')
stage('Apply')
```

Require approval before production apply.

Log all changes.

---

# ğŸ“Š SOC2 Control Mapping (Technical)

| SOC2 Area         | DevSecOps Control               |
| ----------------- | ------------------------------- |
| Access Control    | IAM + MFA + SSO                 |
| Change Management | PR review + CI approval         |
| Monitoring        | CloudTrail + Alerts             |
| Data Protection   | Encryption at rest & in transit |
| Availability      | Multi-AZ + Backups              |
| Incident Response | Documented runbook              |

---

# ğŸ”’ Critical Must-Haves for Audit

* MFA enforced
* Encryption enabled (RDS, S3)
* Backups verified
* Logging enabled
* No shared credentials
* Evidence of vulnerability scans
* Access review documentation

---

# ğŸ“† 3-Month Execution Timeline

| Month   | Focus                                   |
| ------- | --------------------------------------- |
| Month 1 | IAM + Logging + IaC enforcement         |
| Month 2 | CI/CD security + monitoring + backups   |
| Month 3 | Documentation + DR testing + audit prep |

---

# âš ï¸ What NOT to Do

* âŒ Treat SOC2 as paperwork only
* âŒ Allow manual prod changes
* âŒ Skip logging centralization
* âŒ Ignore access review process
* âŒ Forget evidence collection

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Enforce IAM + MFA + SSO
* All infra via Terraform
* Add security scanning in CI/CD
* Enable CloudTrail + GuardDuty
* Document policies + collect evidence

ğŸ‘‰ SOC2 readiness = **Security controls + automation + logging + documented processes**, not just compliance paperwork.

----
## Q24: A third-party dependency in your application has a critical CVE. You have 24 hours to patch 20 microservices. How do you handle this?

---

### ğŸ§  Overview

Critical CVE + 20 microservices + 24 hours = **coordinated security incident**.

Priorities:

1. Assess exploitability (is it reachable in your context?)
2. Prioritize internet-facing services
3. Patch fast but safely
4. Prove remediation to security team

This is not 20 manual fixes â€” it must be **automated and parallelized**.

---

# ğŸš¨ Phase 1: Immediate Risk Assessment (Hour 0â€“2)

---

## 1ï¸âƒ£ Understand the CVE

Identify:

* CVSS score
* Remote code execution?
* Requires authentication?
* Is it in runtime path or dev dependency?

---

## 2ï¸âƒ£ Identify Affected Services

Run scan across repos:

Example (Trivy):

```bash
trivy fs .
```

Or container scan:

```bash
trivy image <image>
```

Generate list:

| Service | Version | Severity | Internet Facing? |
| ------- | ------- | -------- | ---------------- |

Prioritize public services first.

---

# ğŸ”¥ Phase 2: Containment Strategy (Hour 2â€“6)

---

## ğŸ”¹ Option A: Upgrade Dependency (Preferred)

Example (Maven):

```xml
<dependency>
  <version>2.3.4</version>
</dependency>
```

---

## ğŸ”¹ Option B: Override Transitive Dependency

```xml
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>...</groupId>
      <artifactId>...</artifactId>
      <version>patched-version</version>
    </dependency>
  </dependencies>
</dependencyManagement>
```

---

## ğŸ”¹ Option C: Base Image CVE (Container Issue)

If vulnerability in base image:

```dockerfile
FROM node:20-alpine
```

Upgrade:

```dockerfile
FROM node:20.10-alpine
```

Rebuild all images.

---

# âš¡ Phase 3: Automate the Patching (Hour 6â€“16)

---

## ğŸ” 1ï¸âƒ£ Create Patch Branch Across Repos

Use script:

```bash
for repo in service1 service2 service3; do
  git checkout -b cve-patch
  # update dependency
  git commit -am "CVE-2026-XXXX patch"
  git push origin cve-patch
done
```

---

## ğŸ” 2ï¸âƒ£ Trigger CI/CD in Parallel

Pipeline must:

* Run unit tests
* Build container
* Run vulnerability scan
* Deploy to staging

---

## ğŸ” 3ï¸âƒ£ Parallel Deploy Strategy

Use:

* Canary deployment
* Rolling update

Example (K8s):

```yaml
strategy:
  rollingUpdate:
    maxUnavailable: 10%
    maxSurge: 10%
```

Deploy high-risk services first.

---

# ğŸ” Phase 4: Validate Before Production

---

## 1ï¸âƒ£ Confirm CVE No Longer Present

```bash
trivy image <new-image>
```

Ensure vulnerability gone.

---

## 2ï¸âƒ£ Smoke Tests

Run:

* Health checks
* Core API flows
* Authentication

---

# ğŸš€ Phase 5: Production Deployment (Hour 16â€“22)

---

Deploy in waves:

| Priority | Service Type         |
| -------- | -------------------- |
| P1       | Internet-facing APIs |
| P2       | Internal services    |
| P3       | Batch jobs           |

Monitor:

```promql
rate(http_requests_total{status=~"5.."}[5m])
```

Abort if error spike.

---

# ğŸ”’ Phase 6: Temporary Mitigation (If Patch Delayed)

If upgrade not possible:

* Disable vulnerable feature
* Add WAF rule
* Restrict access via firewall
* Add rate limiting

Example WAF block:

```bash
aws wafv2 update-web-acl ...
```

---

# ğŸ›¡ Phase 7: Post-Patch Hardening

---

## âœ… Enable Automated Dependency Updates

Use:

* Dependabot
* Renovate

---

## âœ… Enforce CI Failure on Critical CVEs

Example pipeline step:

```bash
trivy image --exit-code 1 --severity CRITICAL
```

---

## âœ… Maintain Central Dependency Versioning

If using Maven multi-module:

* Parent POM controls versions

Prevents 20 independent upgrades next time.

---

## âœ… Use Base Image Version Pinning

Avoid:

```dockerfile
FROM node:latest
```

Pin versions strictly.

---

# ğŸ“‹ 24-Hour Action Plan

| Time   | Action                   |
| ------ | ------------------------ |
| 0â€“2h   | Assess impact            |
| 2â€“6h   | Patch dependency         |
| 6â€“16h  | CI + staging validation  |
| 16â€“22h | Production rollout       |
| 22â€“24h | Verification & reporting |

---

# âš ï¸ What NOT to Do

* âŒ Panic deploy without testing
* âŒ Ignore transitive dependencies
* âŒ Patch only some services
* âŒ Skip vulnerability re-scan

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Assess exploitability first
* Patch dependency centrally
* Automate parallel CI/CD
* Deploy in waves
* Enforce CVE scanning permanently

ğŸ‘‰ Large-scale CVE remediation requires **automation + prioritization + controlled rollout**, not manual patching across services.

----
## Q25: Your Docker images have been running in production for 6 months with unpatched vulnerabilities. How do you fix this safely?

---

### ğŸ§  Overview

If production images are **6 months old**, assume:

* Base image CVEs accumulated
* OS packages outdated
* Language runtime vulnerable
* No automated rebuild policy

Goal:

> Rebuild safely, validate thoroughly, and prevent drift permanently.

This is a **controlled rebuild + phased rollout**, not `docker pull latest && deploy`.

---

# ğŸ” Phase 1: Assess Risk & Scope

---

## 1ï¸âƒ£ Scan Existing Images

Use Trivy:

```bash
trivy image <prod-image>
```

Focus on:

* CRITICAL
* HIGH

Categorize:

| Service | Critical CVEs | Internet-Facing? | Priority |
| ------- | ------------- | ---------------- | -------- |

Patch public-facing services first.

---

## 2ï¸âƒ£ Identify Root Cause

Common reasons:

* `FROM ubuntu:20.04` not updated
* `apt-get update` missing
* No rebuild automation
* No image scanning gate

---

# ğŸ”¥ Phase 2: Rebuild Strategy (Safe Approach)

---

# ğŸ”¹ Step 1: Update Base Image

Bad:

```dockerfile
FROM node:18
```

Better:

```dockerfile
FROM node:18.19-alpine
```

Avoid `latest`.

---

# ğŸ”¹ Step 2: Rebuild With Clean Layer

If using OS packages:

```dockerfile
RUN apt-get update && \
    apt-get upgrade -y && \
    rm -rf /var/lib/apt/lists/*
```

For Alpine:

```dockerfile
RUN apk update && apk upgrade
```

---

# ğŸ”¹ Step 3: Use Multi-Stage Builds

Reduce attack surface:

```dockerfile
FROM node:18-alpine AS builder
RUN npm install

FROM node:18-alpine
COPY --from=builder /app /app
```

Removes build tools from runtime.

---

# ğŸ” Phase 3: Automated Bulk Rebuild

---

## Script to Rebuild All Services

```bash
for svc in service1 service2 service3; do
  docker build -t $svc:patched .
  docker push <ECR>/$svc:patched
done
```

Or trigger CI for all repos.

---

# ğŸ” Phase 4: Validate Before Production

---

## 1ï¸âƒ£ Re-Scan New Image

```bash
trivy image <patched-image>
```

Ensure critical CVEs removed.

---

## 2ï¸âƒ£ Run Regression Tests

* Unit tests
* Integration tests
* Smoke tests

---

## 3ï¸âƒ£ Deploy to Staging

```bash
kubectl set image deployment/app \
  app=<ECR>/app:patched -n staging
```

Monitor:

* CPU
* Memory
* Error rate

---

# ğŸš€ Phase 5: Controlled Production Rollout

---

## Rolling Update

```yaml
strategy:
  rollingUpdate:
    maxUnavailable: 10%
    maxSurge: 10%
```

Or canary deploy first.

Monitor:

```promql
rate(http_requests_total{status=~"5.."}[5m])
```

Rollback if error spike.

---

# ğŸ”’ Phase 6: Prevent Future Drift

---

## âœ… 1ï¸âƒ£ Implement Scheduled Image Rebuild

Even without code change:

* Weekly rebuild job
* Monthly forced rebuild

CI example:

```yaml
on:
  schedule:
    - cron: "0 3 * * 0"
```

---

## âœ… 2ï¸âƒ£ Enforce CI Scan Gate

Fail build if:

```bash
trivy image --exit-code 1 --severity CRITICAL
```

---

## âœ… 3ï¸âƒ£ Centralize Base Images

Maintain internal hardened base:

```dockerfile
FROM company/node-base:1.0
```

Patch base once â†’ rebuild all apps.

---

## âœ… 4ï¸âƒ£ Use Slim / Distroless Images

Example:

```dockerfile
FROM gcr.io/distroless/nodejs
```

Reduces vulnerability surface.

---

## âœ… 5ï¸âƒ£ Enable ECR Scan on Push

```bash
aws ecr put-image-scanning-configuration \
  --scan-on-push
```

---

# ğŸ“Š Remediation Plan

| Phase | Action                   |
| ----- | ------------------------ |
| 1     | Scan existing images     |
| 2     | Update base images       |
| 3     | Rebuild & re-scan        |
| 4     | Deploy to staging        |
| 5     | Rollout to prod          |
| 6     | Implement rebuild policy |

---

# âš ï¸ What NOT to Do

* âŒ Upgrade everything blindly without testing
* âŒ Deploy all services simultaneously
* âŒ Ignore transitive OS packages
* âŒ Continue using `latest`

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Scan images first
* Update base images
* Rebuild & re-scan
* Deploy via rolling update
* Automate recurring rebuilds

ğŸ‘‰ Old Docker images are a **supply chain risk** â€” fix with controlled rebuild + CI scan gates + scheduled image refresh policy.

----
## Q26: Your monitoring system is generating 500 alerts per day and the team is experiencing alert fatigue. How do you fix this?

---

### ğŸ§  Overview

500 alerts/day = **monitoring system failure**, not system failure.

Alert fatigue causes:

* Ignored critical alerts
* Slower incident response
* Burnout
* Missed outages

Principle:

> Alert only on actionable signals.

Goal:

1. Reduce noise drastically
2. Prioritize true incidents
3. Improve signal-to-noise ratio
4. Maintain SLO-based alerting

---

# ğŸ” Phase 1: Audit Existing Alerts

---

## 1ï¸âƒ£ Categorize Alerts

Export alert data:

| Alert Type  | Count/Day | Action Taken? |
| ----------- | --------- | ------------- |
| CPU High    | 150       | No            |
| Pod Restart | 120       | Rare          |
| Disk Usage  | 80        | No            |
| 5xx Errors  | 50        | Yes           |

Identify:

* Which alerts triggered real action?
* Which alerts are ignored?

---

## 2ï¸âƒ£ Identify Non-Actionable Alerts

Examples:

* CPU > 70% (normal)
* Pod restarted once
* Temporary spike alerts
* Duplicate alerts per pod

---

# ğŸ”¥ Phase 2: Implement SLO-Based Alerting

---

Instead of raw metrics, alert on **impact**.

Bad:

```promql
node_cpu_usage > 70%
```

Better:

```promql
rate(http_requests_total{status=~"5.."}[5m]) > 0.05
```

Alert on:

* Error rate
* Latency SLO breach
* Service unavailable

Not internal noise.

---

# ğŸ” Phase 3: Add Alert Aggregation & Grouping

---

## Use Alertmanager Grouping

```yaml
group_by: ['alertname']
group_wait: 30s
group_interval: 5m
repeat_interval: 4h
```

Instead of 200 alerts:

* 1 aggregated alert

---

# ğŸ” Phase 4: Add Threshold Stabilization

---

Avoid spike-based alerts.

Bad:

```promql
cpu_usage > 80%
```

Better:

```promql
avg_over_time(cpu_usage[10m]) > 85%
```

Add:

```
for: 10m
```

Only alert if sustained.

---

# ğŸ” Phase 5: Use Multi-Level Severity

---

| Severity | When to Trigger           |
| -------- | ------------------------- |
| Critical | Service down / SLO breach |
| Warning  | Resource trending high    |
| Info     | Observability only        |

Only **Critical** pages on-call.

Warnings â†’ Slack channel only.

---

# ğŸ” Phase 6: Eliminate Duplicate Pod Alerts

Instead of per-pod:

Bad:

```promql
kube_pod_status_phase{phase="Failed"}
```

Better:

```promql
sum(kube_pod_status_phase{phase="Failed"}) > 5
```

Alert on cluster-level impact.

---

# ğŸ”’ Phase 7: Implement Runbooks

Every alert must answer:

* What happened?
* Why?
* What action?

Example alert:

```
Service Error Rate > 5% for 10 mins
Runbook: https://wiki/runbooks/service-error
```

If no action defined â†’ remove alert.

---

# ğŸ”¥ Phase 8: Introduce Alert Ownership

Each alert must have:

* Owner
* Review date
* Justification

Quarterly cleanup.

---

# ğŸ“Š Alert Optimization Strategy

| Problem               | Fix                            |
| --------------------- | ------------------------------ |
| Too many CPU alerts   | Alert on saturation, not usage |
| Frequent pod restarts | Alert only if > threshold      |
| Duplicate alerts      | Group by service               |
| Flapping alerts       | Add time window                |
| Infra noise           | Move to warning channel        |

---

# ğŸ›¡ Long-Term Monitoring Model

---

## Shift from Metric Alerts â†’ SLO Alerts

Monitor:

* Availability
* Latency
* Error rate
* Saturation

Golden Signals model.

---

## Implement Error Budget Alerts

Alert only when burn rate high.

Example:

```promql
error_rate > 5% for 10m
```

---

## Use Synthetic Monitoring

Alert when:

* Real user experience impacted

Not when CPU is 75%.

---

# ğŸ“‹ Cleanup Plan

| Week   | Action                       |
| ------ | ---------------------------- |
| Week 1 | Audit alerts                 |
| Week 2 | Remove non-actionable alerts |
| Week 3 | Implement grouping           |
| Week 4 | Introduce SLO alerts         |

Reduce 500/day â†’ <20/day.

---

# âš ï¸ What NOT to Do

* âŒ Disable all alerts
* âŒ Increase thresholds blindly
* âŒ Alert on every metric
* âŒ Page for warnings

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Audit which alerts are actionable
* Alert on SLO impact, not raw metrics
* Add grouping and stabilization
* Separate severity levels
* Assign alert ownership

ğŸ‘‰ Monitoring should generate **actionable signals, not noise** â€” optimize for impact, not metrics.

----

## Q27: Users are reporting slow application performance but all your metrics show everything is green. How do you investigate?

---

### ğŸ§  Overview

If users say **â€œitâ€™s slowâ€** but dashboards are green:

> Your monitoring is incomplete or measuring the wrong thing.

This usually means:

* Youâ€™re monitoring system metrics, not user experience
* Latency percentiles (p95/p99) not tracked
* Regional/CDN issue
* External dependency slowdown
* Frontend issue not backend
* Network path issue

Goal:

1. Validate user impact
2. Identify blind spots
3. Trace request end-to-end
4. Improve observability

---

# ğŸ” Phase 1: Validate the Complaint

---

## 1ï¸âƒ£ Reproduce from User Region

Use synthetic test:

```bash
curl -w "@curl-format.txt" -o /dev/null -s https://app.example.com
```

Or test from different region:

```bash
curl --resolve app.example.com:443:<ip>
```

Check:

* DNS time
* TLS handshake
* TTFB (Time to First Byte)

---

## 2ï¸âƒ£ Check Latency Percentiles

Dashboards often show **average latency**, which hides tail issues.

Bad metric:

```promql
avg(request_duration_seconds)
```

Better:

```promql
histogram_quantile(0.95, rate(request_duration_seconds_bucket[5m]))
```

Check:

* p95
* p99

Slow users often hit p99.

---

# ğŸ” Phase 2: Check External Dependencies

---

## 1ï¸âƒ£ Database Query Latency

```sql
SELECT query, total_time FROM pg_stat_statements
ORDER BY total_time DESC LIMIT 10;
```

Check:

* Slow queries
* Lock contention

---

## 2ï¸âƒ£ Third-Party APIs

Check outbound latency:

```promql
rate(http_client_request_duration_seconds_bucket[5m])
```

If dependency slow â†’ your API slow.

---

# ğŸ” Phase 3: Check Regional or CDN Issues

---

## 1ï¸âƒ£ CDN Health

If using CloudFront:

Check:

* Cache hit ratio
* Origin latency

High origin calls â†’ slower.

---

## 2ï¸âƒ£ DNS Latency

Check:

```bash
dig app.example.com
```

High resolution time?

---

## 3ï¸âƒ£ Specific Geography Impact

Users in India slow?
Users in EU fine?

Check multi-region latency.

---

# ğŸ”¬ Phase 4: Enable Distributed Tracing

If not already enabled, add:

* Jaeger
* AWS X-Ray
* Datadog APM

Trace flow:

```
User â†’ CDN â†’ ALB â†’ Service A â†’ Service B â†’ DB
```

Find which hop slow.

---

# ğŸ”¥ Phase 5: Check Resource Saturation (Hidden)

Even if CPU looks normal:

* Thread pool saturation?
* Connection pool exhaustion?
* GC pauses?
* I/O wait?

---

## Example: Check I/O Wait

```bash
iostat -x 1
```

High disk wait = slow app.

---

## Example: JVM GC Pauses

Check logs for:

```
Full GC
```

---

# ğŸ” Phase 6: Frontend Investigation

Sometimes backend is fine.

Check:

* Large JS bundle
* Slow client rendering
* API waterfall calls
* Browser console errors

Use Chrome DevTools â†’ Network tab.

---

# ğŸ”’ Phase 7: Improve Observability

---

## Add:

* Real User Monitoring (RUM)
* Synthetic monitoring
* Tail latency metrics
* Dependency latency metrics

---

## Golden Signals Model

Track:

* Latency
* Errors
* Traffic
* Saturation

Most teams track only CPU.

---

# ğŸ“Š Investigation Matrix

| Area       | What to Check         |
| ---------- | --------------------- |
| Backend    | p95/p99 latency       |
| DB         | Slow queries          |
| Network    | DNS/TLS latency       |
| CDN        | Cache miss            |
| Dependency | External API response |
| Infra      | I/O wait              |
| Frontend   | Bundle size           |

---

# âš ï¸ Common Root Causes

* Monitoring only averages
* Hidden p99 latency
* Regional routing issue
* Slow third-party API
* Lock contention in DB
* Garbage collection pause
* CDN misconfiguration

---

# ğŸ“‹ Debug Flow (Interview Style)

| Step | Action             |
| ---- | ------------------ |
| 1    | Reproduce issue    |
| 2    | Check p95/p99      |
| 3    | Check dependencies |
| 4    | Trace request path |
| 5    | Check DB locks     |
| 6    | Validate frontend  |

---

# âš ï¸ What NOT to Do

* âŒ Assume users are wrong
* âŒ Trust only CPU metrics
* âŒ Look only at averages
* âŒ Ignore external dependencies

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Reproduce from user region
* Check p95/p99 latency
* Trace request end-to-end
* Validate DB and external APIs
* Improve observability

ğŸ‘‰ If dashboards are green but users complain, youâ€™re likely **monitoring infrastructure, not user experience**.

----
## Q28: You have no monitoring on a legacy application that just moved to your team. How do you implement observability from scratch?

---

### ğŸ§  Overview

No monitoring = **flying blind in production**.

Your goal is not to â€œadd dashboards.â€
Your goal is to implement **observability based on user impact**.

Follow this order:

> Logs â†’ Metrics â†’ Alerts â†’ Tracing â†’ SLOs

Focus on:

* Availability
* Latency
* Error rate
* Saturation

Golden Signals first, deep telemetry later.

---

# ğŸ— Phase 1: Establish Basic Visibility (Week 1)

---

# ğŸ”¹ 1ï¸âƒ£ Centralized Logging

If on EC2:

Install CloudWatch Agent.

If on Kubernetes:

Deploy FluentBit or Fluentd.

Example (EKS FluentBit DaemonSet):

```bash
kubectl apply -f fluentbit.yaml
```

Logs â†’ CloudWatch or OpenSearch.

Ensure:

* Application logs
* Access logs
* Error logs
* System logs

---

# ğŸ”¹ 2ï¸âƒ£ Basic Infrastructure Metrics

Enable:

* EC2 CPU
* Memory
* Disk
* Network

In Kubernetes:

```bash
kubectl top pods
kubectl top nodes
```

Install metrics-server if missing.

---

# ğŸš€ Phase 2: Application-Level Metrics (Week 2)

---

# ğŸ”¹ 1ï¸âƒ£ Expose /metrics Endpoint

Add Prometheus client.

Example (Spring Boot):

```yaml
management:
  endpoints:
    web:
      exposure:
        include: prometheus
```

---

# ğŸ”¹ 2ï¸âƒ£ Deploy Prometheus + Grafana

```bash
helm install prometheus prometheus-community/kube-prometheus-stack
```

Track:

* Request rate
* Error rate
* Latency histogram
* DB connections

---

# ğŸ“Š Implement Golden Signals

---

## 1ï¸âƒ£ Latency

```promql
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
```

---

## 2ï¸âƒ£ Error Rate

```promql
rate(http_requests_total{status=~"5.."}[5m])
```

---

## 3ï¸âƒ£ Traffic

```promql
rate(http_requests_total[1m])
```

---

## 4ï¸âƒ£ Saturation

```promql
container_cpu_usage_seconds_total
```

---

# ğŸš¨ Phase 3: Implement Alerting

---

## Critical Alerts Only Initially

Example:

```yaml
- alert: HighErrorRate
  expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
  for: 10m
```

Page only on:

* Service unavailable
* Error rate > threshold
* DB down

Avoid CPU > 70% alerts.

---

# ğŸ” Phase 4: Add Distributed Tracing (Week 3)

---

Install:

* Jaeger
* AWS X-Ray
* Datadog APM

Trace request path:

```
User â†’ API â†’ Service â†’ DB
```

Helps identify bottlenecks.

---

# ğŸŒ Phase 5: Add Synthetic Monitoring

Use:

* Pingdom
* Datadog Synthetics
* UptimeRobot

Test:

* Login
* Critical APIs
* Checkout flow

Monitors real user experience.

---

# ğŸ“ˆ Phase 6: Define SLOs

---

Example SLO:

* 99.9% availability
* p95 latency < 500ms
* Error rate < 1%

Add burn-rate alerts:

```promql
(error_rate / error_budget) > threshold
```

---

# ğŸ›¡ Phase 7: Long-Term Enhancements

---

## âœ… Log Structuring

Switch to JSON logs.

---

## âœ… Correlation IDs

Add request ID header.

---

## âœ… Dashboards Per Service

Include:

* Latency
* Errors
* DB
* External APIs

---

## âœ… Runbooks Linked to Alerts

Each alert must include:

* Root cause steps
* Mitigation steps

---

# ğŸ“‹ Implementation Timeline

| Week   | Focus                       |
| ------ | --------------------------- |
| Week 1 | Logs + Infra metrics        |
| Week 2 | App metrics + dashboards    |
| Week 3 | Alerting + tracing          |
| Week 4 | SLOs + synthetic monitoring |

---

# âš ï¸ What NOT to Do

* âŒ Alert on every metric
* âŒ Only monitor CPU
* âŒ Skip application-level metrics
* âŒ Ignore log centralization

---

# ğŸ“Š Observability Stack Example

```
Application â†’ Prometheus â†’ Grafana
Logs â†’ FluentBit â†’ OpenSearch
Tracing â†’ Jaeger
Alerts â†’ Alertmanager â†’ PagerDuty
Synthetic â†’ External Monitor
```

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Start with logs + infra metrics
* Add app metrics (Golden Signals)
* Implement actionable alerts
* Add tracing for deep debugging
* Define SLOs

ğŸ‘‰ Observability from scratch means building **visibility around user experience**, not just server health.

----
## Q29: Your Grafana dashboards show normal metrics but customers are reporting intermittent failures. What's missing in your observability?

---

### ğŸ§  Overview

If dashboards are green but customers see intermittent failures:

> You are monitoring averages and infrastructure â€” not user experience or tail behavior.

Most likely missing:

* p95/p99 latency visibility
* Error rate per endpoint
* Synthetic monitoring
* Distributed tracing
* Correlation IDs
* Dependency-level metrics
* SLO-based alerting

This is an **observability gap**, not necessarily a system outage.

---

# ğŸ” Whatâ€™s Likely Missing

---

# ğŸ”¹ 1ï¸âƒ£ Tail Latency (p95 / p99)

Most dashboards show:

```promql
avg(http_request_duration_seconds)
```

Average hides slow requests.

Instead use:

```promql
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

Intermittent issues often affect only 1â€“5% of traffic â†’ visible only in p99.

---

# ğŸ”¹ 2ï¸âƒ£ Per-Endpoint Error Rates

If monitoring:

```
total error rate
```

You may miss:

* One failing endpoint
* One failing region
* One specific customer segment

Better:

```promql
rate(http_requests_total{status=~"5.."}[5m]) by (path)
```

---

# ğŸ”¹ 3ï¸âƒ£ Synthetic Monitoring (User Perspective)

Dashboards show internal metrics.

Missing:

* Login success rate
* Checkout flow test
* External uptime check

Add:

* Datadog Synthetics
* Pingdom
* UptimeRobot

This validates real user experience.

---

# ğŸ”¹ 4ï¸âƒ£ Distributed Tracing

Without tracing, you canâ€™t see:

```
User â†’ API â†’ Service A â†’ Service B â†’ DB â†’ External API
```

Intermittent failure could be:

* One dependency timeout
* One microservice overloaded
* One retry storm

Add:

* Jaeger
* AWS X-Ray
* OpenTelemetry

---

# ğŸ”¹ 5ï¸âƒ£ Dependency Metrics

You may monitor:

* CPU
* Memory
* Overall latency

But not:

* DB query latency
* Connection pool exhaustion
* External API timeout rate

Example:

```promql
rate(http_client_request_duration_seconds_bucket[5m])
```

---

# ğŸ”¹ 6ï¸âƒ£ Error Budget Burn Rate

Instead of monitoring raw error count, track:

```promql
(error_rate / allowed_error_budget)
```

Intermittent bursts may not trigger basic threshold alerts.

---

# ğŸ”¹ 7ï¸âƒ£ Correlation IDs in Logs

Without request IDs:

* Impossible to trace failing request
* Logs disconnected

Add middleware:

```
X-Request-ID
```

Log it across services.

---

# ğŸ”¹ 8ï¸âƒ£ Regional or CDN Monitoring

Issue might be:

* Only India region slow
* CDN cache misses
* DNS resolution delay

Need geo-distributed monitoring.

---

# ğŸ”¹ 9ï¸âƒ£ Retry Storm Visibility

Retries can mask failures:

* First call fails
* Retry succeeds
* User experiences delay

Metrics look â€œsuccessful.â€

Track:

```promql
rate(retry_attempts_total[5m])
```

---

# ğŸ“Š Observability Gaps Table

| Missing Signal       | Why It Matters                  |
| -------------------- | ------------------------------- |
| p99 latency          | Detect tail issues              |
| Per-endpoint metrics | Detect localized failure        |
| Synthetic tests      | User experience validation      |
| Tracing              | Identify dependency bottlenecks |
| Dependency metrics   | Catch hidden bottlenecks        |
| Correlation IDs      | Cross-service debugging         |
| Retry metrics        | Detect masked failures          |

---

# ğŸ” Investigation Flow

| Step | Action                      |
| ---- | --------------------------- |
| 1    | Check p95/p99               |
| 2    | Break down by endpoint      |
| 3    | Check dependency latency    |
| 4    | Run synthetic test          |
| 5    | Inspect trace for slow span |
| 6    | Review retry behavior       |

---

# âš ï¸ Common Real Causes

* One DB replica slow
* External API intermittent 500s
* Thread pool saturation
* Connection pool exhaustion
* GC pause
* One Kubernetes node overloaded
* DNS resolution delay
* CDN misrouting

---

# ğŸ”’ How to Fix Observability Properly

---

## âœ… Implement Golden Signals per Service

* Latency (p95/p99)
* Errors
* Traffic
* Saturation

---

## âœ… Add Distributed Tracing

Instrument with OpenTelemetry.

---

## âœ… Add Synthetic Monitoring

Monitor real user journeys.

---

## âœ… Implement SLO-Based Alerts

Alert when SLO breach likely.

---

## âœ… Add Dependency Dashboards

Every service must show:

* DB latency
* External API latency
* Cache hit ratio

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Averages hide intermittent failures
* Monitor p95/p99, not just avg
* Add synthetic monitoring
* Add distributed tracing
* Track dependency-level metrics

ğŸ‘‰ If Grafana is green but customers complain, youâ€™re likely missing **tail latency, dependency visibility, or user-experience monitoring**.

---
## Q30: You need to implement distributed tracing across 30 microservices with minimal code changes. How do you approach this?

---

### ğŸ§  Overview

Goal: **End-to-end tracing across 30 services** with minimal code changes.

Principle:

> Prefer auto-instrumentation + sidecars + service mesh over manual code changes.

Best approach today:

* **OpenTelemetry (OTel)** as standard
* Auto-instrumentation agents
* Central collector
* Backend: Jaeger / Tempo / Datadog / X-Ray

Avoid rewriting 30 codebases manually.

---

# ğŸ— Target Architecture

```
Service Pods
   â†“ (OTel SDK / Auto Agent)
OpenTelemetry Collector (DaemonSet / Deployment)
   â†“
Tracing Backend (Jaeger / Tempo / X-Ray / Datadog)
   â†“
Grafana / UI
```

---

# ğŸš€ Step 1: Choose Standard (OpenTelemetry)

Use **OpenTelemetry** because:

* Vendor neutral
* Supports Java, Node, Python, Go
* Works with all major backends

---

# ğŸ” Step 2: Use Auto-Instrumentation (Minimal Code Change)

---

# ğŸ”¹ Java Services

Use Java Agent:

Add to container startup:

```bash
-javaagent:/otel/opentelemetry-javaagent.jar
```

Environment variables:

```bash
OTEL_SERVICE_NAME=payment-service
OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
```

No code change required.

---

# ğŸ”¹ Node.js Services

Install:

```bash
npm install @opentelemetry/auto-instrumentations-node
```

Startup:

```bash
node -r @opentelemetry/auto-instrumentations-node/register app.js
```

Minimal change.

---

# ğŸ”¹ Python Services

```bash
pip install opentelemetry-distro
opentelemetry-instrument python app.py
```

---

# ğŸ” Step 3: Deploy OpenTelemetry Collector in Kubernetes

---

Install via Helm:

```bash
helm install otel-collector open-telemetry/opentelemetry-collector
```

Example config:

```yaml
receivers:
  otlp:
    protocols:
      grpc:

exporters:
  jaeger:
    endpoint: jaeger:14250

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [jaeger]
```

Collector handles:

* Exporting
* Sampling
* Centralized config

---

# ğŸ” Step 4: Add Context Propagation

Ensure HTTP headers propagate:

* `traceparent`
* `tracestate`

Most auto-instrumentation handles this automatically.

For custom calls, ensure:

```bash
Traceparent header forwarded
```

Without this â†’ traces break across services.

---

# ğŸ” Step 5: Choose Tracing Backend

---

## Option A: Jaeger (Self-hosted)

Install:

```bash
helm install jaeger jaegertracing/jaeger
```

Good for internal K8s clusters.

---

## Option B: Grafana Tempo

Integrates with Grafana.

---

## Option C: AWS X-Ray

If on AWS.

---

## Option D: Datadog / NewRelic

Managed APM.

---

# ğŸ”¥ Step 6: Sampling Strategy (Critical at Scale)

30 services â†’ huge trace volume.

Use:

```yaml
sampling:
  probabilistic:
    sampling_percentage: 10
```

Or dynamic sampling for:

* Errors
* High latency

---

# ğŸ” Step 7: Validate Trace Flow

Test flow:

```
User â†’ API Gateway â†’ Service A â†’ Service B â†’ DB
```

In UI, confirm:

* Full trace visible
* Span breakdown correct
* Latency per service visible

---

# ğŸ›¡ Best Practices

---

## âœ… Add Service Name Consistency

Set:

```bash
OTEL_SERVICE_NAME
```

Correct naming avoids chaos.

---

## âœ… Add Environment Tags

```
env=production
region=us-east-1
```

---

## âœ… Monitor Trace Errors

Add dashboard for:

* Error spans
* Long spans
* Dependency latency

---

## âœ… Secure Collector

Use mTLS between services and collector.

---

# ğŸ“Š Implementation Strategy

| Step | Action                      |
| ---- | --------------------------- |
| 1    | Deploy OTel Collector       |
| 2    | Enable auto-instrumentation |
| 3    | Configure backend           |
| 4    | Enable sampling             |
| 5    | Validate end-to-end traces  |
| 6    | Add dashboards              |

---

# âš ï¸ What NOT to Do

* âŒ Manually instrument 30 services fully
* âŒ Enable 100% sampling in production
* âŒ Forget header propagation
* âŒ Skip performance impact testing

---

# ğŸ“‹ Minimal-Code Rollout Plan

| Week   | Focus                       |
| ------ | --------------------------- |
| Week 1 | Collector + Backend         |
| Week 2 | Enable auto-instrumentation |
| Week 3 | Tune sampling               |
| Week 4 | Add dashboards              |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Use OpenTelemetry
* Prefer auto-instrumentation agents
* Deploy central collector
* Use sampling to control cost
* Validate header propagation

ğŸ‘‰ Distributed tracing at scale requires **standardization + automation + sampling control**, not manual code rewrites across 30 services.

---
## Q31: Your application needs to handle a 10x traffic spike during a product launch in 2 weeks. How do you prepare?

---

### ğŸ§  Overview

10x traffic spike in 2 weeks = **capacity + resilience + validation problem**.

Principle:

> Donâ€™t guess capacity. Measure, test, and scale safely.

Focus areas:

* Load testing
* Horizontal scaling
* Database readiness
* Caching strategy
* Auto-scaling tuning
* Failure protection

Goal:

* No downtime
* Controlled degradation if needed
* Real-time observability

---

# ğŸš€ Phase 1: Baseline & Capacity Planning (Days 1â€“3)

---

## 1ï¸âƒ£ Measure Current Metrics

Capture:

* Current RPS
* Peak RPS
* CPU usage
* Memory usage
* DB connections
* p95 latency

Example PromQL:

```promql
rate(http_requests_total[5m])
```

If current peak = 1,000 RPS â†’ prepare for 10,000 RPS.

---

## 2ï¸âƒ£ Identify Bottlenecks

Check:

* DB CPU
* Slow queries
* Cache hit ratio
* Thread pool size
* Connection pool size

---

# ğŸ”¥ Phase 2: Load Testing (Days 3â€“7)

---

Use:

* k6
* JMeter
* Locust

Example k6 test:

```javascript
import http from 'k6/http';
export default function () {
  http.get('https://app.example.com');
}
```

Run progressively:

* 2x
* 5x
* 10x

Identify:

* Breaking point
* Latency increase
* Resource saturation

---

# ğŸ— Phase 3: Scale Architecture

---

# ğŸ”¹ 1ï¸âƒ£ Horizontal Pod Autoscaler (K8s)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 60
```

Test autoscaling during load test.

---

# ğŸ”¹ 2ï¸âƒ£ Cluster Autoscaler

Ensure node groups can scale.

Check ASG max:

```bash
aws autoscaling describe-auto-scaling-groups
```

Increase max capacity before event.

---

# ğŸ”¹ 3ï¸âƒ£ Database Scaling

Options:

* Increase instance size
* Add read replicas
* Optimize queries
* Enable connection pooling

Example (RDS):

```bash
aws rds modify-db-instance --db-instance-class db.r6g.2xlarge
```

---

# ğŸ”¹ 4ï¸âƒ£ Add Caching Layer

Use Redis for:

* Session data
* Frequently read data
* API responses

Goal: reduce DB load.

---

# ğŸ”¹ 5ï¸âƒ£ Enable CDN

If static content heavy:

* Use CloudFront
* Cache aggressively

Reduce backend load.

---

# ğŸ”¥ Phase 4: Protect Against Failure

---

# ğŸ”¹ 1ï¸âƒ£ Rate Limiting

Prevent overload:

```nginx
limit_req zone=api burst=20 nodelay;
```

---

# ğŸ”¹ 2ï¸âƒ£ Circuit Breakers

Prevent cascading failures.

---

# ğŸ”¹ 3ï¸âƒ£ Graceful Degradation

If needed:

* Disable non-critical features
* Reduce analytics processing
* Queue background jobs

---

# ğŸ“Š Phase 5: Monitoring Upgrade

---

Add dashboards for:

* p95/p99 latency
* DB connections
* Cache hit ratio
* Error rate
* Pod scaling events

Alert example:

```promql
rate(http_requests_total{status=~"5.."}[5m]) > 0.05
```

---

# ğŸ”„ Phase 6: Game Day Simulation

Simulate:

* 10x load
* One DB replica failure
* One node failure

Ensure system survives.

---

# ğŸ”’ Pre-Launch Checklist

---

| Check                 | Status |
| --------------------- | ------ |
| HPA configured        | âœ…      |
| ASG max increased     | âœ…      |
| DB scaled             | âœ…      |
| Load tested at 10x    | âœ…      |
| CDN configured        | âœ…      |
| Rate limiting enabled | âœ…      |
| Monitoring updated    | âœ…      |

---

# âš ï¸ Common Failure Points

* DB becomes bottleneck
* Connection pool exhaustion
* Cache misconfiguration
* Autoscaler too slow
* Cold starts
* DNS rate limits

---

# ğŸ›¡ Emergency Plan

If traffic exceeds 10x:

* Enable queue
* Temporarily restrict access
* Increase read replicas
* Scale nodes manually

---

# ğŸ“‹ 2-Week Execution Plan

| Week   | Focus                                           |
| ------ | ----------------------------------------------- |
| Week 1 | Load test + scaling changes                     |
| Week 2 | Stress test + monitoring tuning + failover test |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Measure baseline
* Load test at 10x
* Scale horizontally (pods + nodes)
* Optimize DB + add caching
* Add rate limiting + monitoring

ğŸ‘‰ Handling 10x traffic spike requires **testing, horizontal scaling, caching, and failure protection**, not just adding more servers.

----
## Q32: Your database is becoming a bottleneck as traffic grows. You cannot change the application code. What infrastructure solutions do you implement?

---

### ğŸ§  Overview

If the **database is the bottleneck** and you **cannot change application code**, your only levers are:

* Vertical scaling
* Read scaling
* Connection management
* Caching (transparent)
* Storage/IO optimization
* Traffic shaping

Principle:

> Reduce DB pressure without modifying application logic.

Goal:

* Improve throughput
* Reduce latency
* Prevent connection exhaustion
* Maintain availability

---

# ğŸ” Phase 1: Identify Bottleneck Type

Check:

* CPU saturation
* Memory pressure
* IOPS limits
* Connection limit reached
* Slow queries
* Lock contention

Example (Postgres):

```sql
SELECT count(*) FROM pg_stat_activity;
```

CloudWatch metrics:

* CPUUtilization
* ReadIOPS
* DatabaseConnections
* FreeableMemory

---

# ğŸ”¥ Scenario 1: CPU Bottleneck

---

## ğŸ”¹ Vertical Scaling (Fastest Fix)

Upgrade instance type:

```bash
aws rds modify-db-instance \
  --db-instance-identifier prod-db \
  --db-instance-class db.r6g.4xlarge \
  --apply-immediately
```

Pros:

* No code change
* Immediate impact

Cons:

* Expensive
* Limited scalability

---

# ğŸ”¥ Scenario 2: Read Bottleneck

If reads dominate.

---

## ğŸ”¹ Add Read Replicas

```bash
aws rds create-db-instance-read-replica \
  --db-instance-identifier prod-db-replica \
  --source-db-instance-identifier prod-db
```

Then route read traffic using:

* RDS Proxy (if supported)
* Load balancer at DB layer
* DNS-based read endpoint (Aurora)

If using Aurora:

Use reader endpoint automatically:

```
cluster.cluster-ro-xxxxx.rds.amazonaws.com
```

---

# ğŸ”¥ Scenario 3: Connection Exhaustion

Common in high-traffic systems.

---

## ğŸ”¹ Add RDS Proxy

```bash
aws rds create-db-proxy \
  --db-proxy-name prod-proxy
```

Benefits:

* Connection pooling
* Reduced DB load
* Faster connection reuse

No app code change if endpoint updated.

---

# ğŸ”¥ Scenario 4: IOPS / Storage Bottleneck

---

## ğŸ”¹ Increase Provisioned IOPS

```bash
aws rds modify-db-instance \
  --iops 10000
```

Or switch to:

* gp3
* io2

Improves disk latency significantly.

---

# ğŸ”¥ Scenario 5: Memory Pressure

---

## ğŸ”¹ Increase Buffer Cache

Upgrade to memory-optimized instance:

* r6g
* r5

More RAM = more cached data = fewer disk reads.

---

# ğŸ”¥ Scenario 6: Heavy Write Load

---

## ğŸ”¹ Partition Traffic

Infrastructure-level:

* Add queue (SQS / Kafka)
* Offload background writes
* Introduce write throttling

Without changing app logic directly.

---

# ğŸ”¥ Scenario 7: Hot Tables / Locking

If single table hot:

---

## ğŸ”¹ Use Read Replica for Analytics

Move reporting workloads off primary DB.

---

## ğŸ”¹ Enable Query Cache (if supported)

MySQL:

```sql
SET GLOBAL query_cache_size = ...
```

---

# ğŸ”¥ Scenario 8: Add Caching Layer (Transparent)

If app repeatedly queries same data.

---

## ğŸ”¹ Introduce Redis (If App Already Uses It)

Or enable DB-level caching.

---

# ğŸ”¥ Scenario 9: Multi-AZ and Failover

Ensure:

```bash
aws rds modify-db-instance --multi-az
```

Prevents downtime under load failure.

---

# ğŸ“Š Solution Comparison

| Bottleneck     | Solution                  | Code Change Required? |
| -------------- | ------------------------- | --------------------- |
| CPU            | Bigger instance           | âŒ                     |
| Reads          | Read replicas             | âŒ                     |
| Connections    | RDS Proxy                 | âŒ                     |
| IOPS           | Provisioned IOPS          | âŒ                     |
| Memory         | Memory-optimized instance | âŒ                     |
| Analytics load | Dedicated replica         | âŒ                     |

---

# ğŸ”’ Long-Term Strategy

---

## âœ… 1ï¸âƒ£ Enable Performance Insights

Identify top queries.

---

## âœ… 2ï¸âƒ£ Monitor Connection Count

Alert before hitting max.

---

## âœ… 3ï¸âƒ£ Implement Auto-Scaling (Aurora)

Aurora can auto-scale read replicas.

---

## âœ… 4ï¸âƒ£ Plan Sharding (Future)

If traffic continues growing.

---

# âš ï¸ What NOT to Do

* âŒ Restart DB frequently
* âŒ Increase max connections blindly
* âŒ Ignore slow query metrics
* âŒ Scale app without scaling DB

---

# ğŸ“‹ Immediate Action Plan

| Step | Action                     |
| ---- | -------------------------- |
| 1    | Identify bottleneck metric |
| 2    | Add RDS Proxy              |
| 3    | Add read replica           |
| 4    | Increase instance size     |
| 5    | Optimize IOPS              |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Identify bottleneck type
* Scale vertically first
* Add read replicas
* Use RDS Proxy for connection pooling
* Increase IOPS if storage-bound

ğŸ‘‰ If you cannot change application code, solve DB bottlenecks with **scaling, replication, pooling, and storage optimization at infrastructure level**.

---
## Q33: Your auto-scaling is too slow and users experience degraded performance before new instances come online. How do you fix this?

---

### ğŸ§  Overview

If users feel degradation **before autoscaling reacts**, the issue is:

> Reactive scaling + slow instance startup + no buffer capacity.

Typical reasons:

* CPU-based scaling reacts too late
* Cooldown period too high
* Node boot time slow (3â€“5 minutes)
* Containers cold start
* No pre-warmed capacity
* Scaling on wrong metric

Goal:

1. Reduce reaction time
2. Pre-scale before spike
3. Maintain buffer capacity
4. Improve startup speed

---

# ğŸ” Phase 1: Identify Bottleneck in Scaling Path

Scaling chain:

```
Traffic spike â†’ Metric crosses threshold â†’ HPA triggers â†’
Cluster Autoscaler triggers â†’ Node boots â†’
Pod schedules â†’ Container starts â†’ Ready
```

Where is delay?

Check:

* HPA metrics delay?
* Node startup time?
* Image pull time?
* Readiness probe delay?

---

# ğŸ”¥ Phase 2: Improve Scaling Responsiveness

---

# ğŸ”¹ 1ï¸âƒ£ Scale on Better Metrics (Not CPU)

CPU is slow to reflect overload.

Better metrics:

* Requests per second
* Queue depth
* Latency
* Custom business metrics

Example HPA (RPS-based):

```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: http_requests_per_second
    target:
      type: AverageValue
      averageValue: 100
```

RPS-based scaling reacts faster than CPU.

---

# ğŸ”¹ 2ï¸âƒ£ Reduce Cooldown & Stabilization Window

Check HPA behavior:

```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 0
    policies:
    - type: Percent
      value: 100
      periodSeconds: 15
```

Allow aggressive scale-up.

---

# ğŸ”¹ 3ï¸âƒ£ Maintain Buffer Capacity (Headroom Strategy)

Never run at 90â€“100% utilization.

Keep:

* 20â€“30% idle capacity

Example:

```yaml
minReplicas: 5
```

Even if average load needs only 3.

Prevents performance dip during scaling.

---

# ğŸ”¹ 4ï¸âƒ£ Pre-Warm Nodes

Increase ASG minimum capacity:

```bash
aws autoscaling update-auto-scaling-group \
  --min-size 10
```

Keep spare nodes ready.

---

# ğŸ”¹ 5ï¸âƒ£ Use Overprovisioning Pods (Advanced)

Deploy low-priority placeholder pods:

```yaml
priorityClassName: low-priority
```

When real workload spikes:

* Placeholder pods evicted instantly
* Real pods scheduled immediately

Prevents scheduling delay.

---

# ğŸ”¥ Phase 3: Reduce Instance Startup Time

---

# ğŸ”¹ 1ï¸âƒ£ Use Smaller AMIs

Avoid large startup scripts.

Pre-bake AMI with:

* Docker installed
* App image cached

Use Packer.

---

# ğŸ”¹ 2ï¸âƒ£ Use Warm Pools (EC2)

```bash
aws autoscaling put-warm-pool ...
```

Pre-initialized instances reduce startup time.

---

# ğŸ”¹ 3ï¸âƒ£ Optimize Container Startup

* Reduce image size
* Use distroless base
* Pre-pull images

K8s:

```bash
kubectl rollout restart daemonset pre-pull
```

---

# ğŸ”¥ Phase 4: Predictive Scaling

---

## ğŸ”¹ Scheduled Scaling

If launch at 9 AM:

```bash
aws autoscaling put-scheduled-update-group-action ...
```

Scale before event.

---

## ğŸ”¹ Predictive Scaling (AWS ASG)

Enable predictive scaling policy.

Uses historical patterns.

---

# ğŸ”¥ Phase 5: Protect System During Spike

---

# ğŸ”¹ 1ï¸âƒ£ Rate Limiting

Prevent overload:

```nginx
limit_req zone=api burst=50;
```

---

# ğŸ”¹ 2ï¸âƒ£ Queue Traffic

Introduce SQS/Kafka buffering.

Absorb traffic spike.

---

# ğŸ“Š Strategy Comparison

| Solution              | Impact | Speed                | Risk        |
| --------------------- | ------ | -------------------- | ----------- |
| RPS-based scaling     | High   | Fast                 | Low         |
| Increase min replicas | High   | Instant              | Medium cost |
| Warm pool             | Medium | Faster boot          | Low         |
| Overprovisioning      | High   | Immediate scheduling | Medium      |
| Predictive scaling    | High   | Preemptive           | Low         |

---

# ğŸ“‹ Fix Strategy Summary

| Step | Action                     |
| ---- | -------------------------- |
| 1    | Scale on RPS, not CPU      |
| 2    | Reduce scale-up delay      |
| 3    | Maintain headroom          |
| 4    | Use warm pools             |
| 5    | Pre-scale for known events |

---

# âš ï¸ What NOT to Do

* âŒ Increase thresholds blindly
* âŒ Scale only after CPU 90%
* âŒ Ignore cold start time
* âŒ Run cluster at 100% utilization

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Reactive CPU scaling is too slow
* Scale on traffic metrics
* Keep buffer capacity
* Use warm pools + faster startup
* Implement predictive scaling

ğŸ‘‰ Fast scaling requires **proactive headroom + traffic-based scaling + reduced startup latency**, not just bigger instances.

----
## Q34: A single microservice is causing cascading failures across the entire system. How do you isolate and fix this?

---

### ğŸ§  Overview

Cascading failure means:

> One failing service overwhelms others â†’ retries spike â†’ thread pools saturate â†’ entire system degrades.

Common causes:

* Dependency timeout
* Retry storm
* Unbounded connection pool
* Circuit breaker missing
* Synchronous blocking calls

Goal:

1. Stop propagation immediately
2. Isolate failing service
3. Protect healthy services
4. Fix root cause
5. Add resilience controls

---

# ğŸš¨ Phase 1: Immediate Containment

---

## 1ï¸âƒ£ Identify Failing Service

Check:

* Error rate by service
* Latency per service
* Dependency traces

Example:

```promql
rate(http_requests_total{status=~"5.."}[5m]) by (service)
```

Find spike source.

---

## 2ï¸âƒ£ Disable or Isolate Service

If non-critical:

```bash
kubectl scale deployment failing-service --replicas=0
```

If critical:

* Route traffic away
* Switch to degraded mode

---

## 3ï¸âƒ£ Reduce Retry Storm

Retries amplify failures.

If using Envoy / Istio:

Reduce retry policy.

Example:

```yaml
retries:
  attempts: 1
```

Avoid infinite retry loops.

---

# ğŸ”¥ Phase 2: Stop Resource Saturation

---

## 1ï¸âƒ£ Increase Timeout Protection

Fail fast instead of hanging.

Example:

```yaml
timeout: 2s
```

Short timeouts prevent thread blocking.

---

## 2ï¸âƒ£ Apply Rate Limiting

Protect upstream services.

NGINX:

```nginx
limit_req zone=api burst=20;
```

---

## 3ï¸âƒ£ Enable Circuit Breaker

If using service mesh:

```yaml
outlierDetection:
  consecutive5xxErrors: 5
```

Trips breaker after repeated failures.

Prevents full collapse.

---

# ğŸ” Phase 3: Root Cause Analysis

---

Check:

* DB dependency?
* External API slow?
* Memory leak?
* CPU spike?
* Recent deployment?

Trace request:

```
Service A â†’ Service B (failing) â†’ DB
```

Common root causes:

| Cause                 | Effect            |
| --------------------- | ----------------- |
| Slow DB query         | Request backlog   |
| Unbounded retries     | Thread exhaustion |
| Connection pool limit | Timeout           |
| Deadlock              | Blocking calls    |
| Misconfigured timeout | Long hang         |

---

# ğŸ”§ Phase 4: Structural Fix

---

# ğŸ”¹ 1ï¸âƒ£ Add Circuit Breakers

Example (Resilience4j):

```yaml
failureRateThreshold: 50
waitDurationInOpenState: 10s
```

---

# ğŸ”¹ 2ï¸âƒ£ Add Bulkhead Isolation

Separate thread pools:

* Critical path threads
* Background job threads

Prevents total exhaustion.

---

# ğŸ”¹ 3ï¸âƒ£ Add Request Timeouts Everywhere

Never allow:

```
timeout = unlimited
```

---

# ğŸ”¹ 4ï¸âƒ£ Use Async Queues

Instead of synchronous dependency calls:

* Publish event
* Process async

Reduces tight coupling.

---

# ğŸ”¹ 5ï¸âƒ£ Add Fallback Responses

Example:

If recommendation service fails:

Return empty recommendation list.

Not 500 error.

---

# ğŸ”’ Phase 5: Protect Entire System

---

## Implement Global Rate Limiting

Protect core APIs.

---

## Add Retry Budget

Limit total retries globally.

---

## Monitor Dependency Health

Alert on:

```promql
rate(dependency_errors_total[5m])
```

Before cascade starts.

---

# ğŸ“Š Isolation Strategy

| Action                     | Purpose               |
| -------------------------- | --------------------- |
| Scale down failing service | Stop spread           |
| Disable retries            | Prevent amplification |
| Enable circuit breaker     | Contain failure       |
| Rate limit                 | Protect core services |
| Add fallback               | Maintain availability |

---

# âš ï¸ What NOT to Do

* âŒ Increase replicas blindly
* âŒ Increase retry count
* âŒ Restart entire cluster
* âŒ Ignore timeout configuration

---

# ğŸ“‹ Incident Flow

| Step | Action                   |
| ---- | ------------------------ |
| 1    | Identify failing service |
| 2    | Isolate traffic          |
| 3    | Reduce retries           |
| 4    | Apply circuit breaker    |
| 5    | Fix root cause           |
| 6    | Add resilience patterns  |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Identify failing service
* Stop retry amplification
* Apply circuit breaker + rate limiting
* Add timeouts + bulkheads
* Implement fallback mechanisms

ğŸ‘‰ Cascading failures happen when **resilience patterns are missing** â€” isolate, contain, then harden the system permanently.

---
## Q35: Your team needs to reduce infrastructure costs by 40% without impacting performance or reliability. What strategies do you implement?

---

### ğŸ§  Overview

Reducing infra cost by **40% without hurting performance or reliability** is possible â€” but only if you:

> Optimize usage, eliminate waste, and improve efficiency before cutting capacity.

Principles:

* Measure before cutting
* Optimize architecture
* Right-size aggressively
* Use pricing models smartly
* Automate cleanup

Goal:

* Same SLO
* Lower bill
* No surprise outages

---

# ğŸ” Phase 1: Cost Visibility & Waste Identification

---

## 1ï¸âƒ£ Identify Top Cost Drivers

Use Cost Explorer:

```bash
aws ce get-cost-and-usage \
  --granularity MONTHLY \
  --metrics UnblendedCost \
  --group-by Type=DIMENSION,Key=SERVICE
```

Breakdown:

* EC2
* RDS
* EKS
* NAT Gateway
* Data transfer
* S3

Focus on top 3 services first.

---

# ğŸ”¥ Phase 2: Compute Optimization (Biggest Savings)

---

# ğŸ”¹ 1ï¸âƒ£ Rightsizing EC2 / EKS Nodes

Check utilization:

* CPU < 30%?
* Memory < 40%?

Switch instance types:

* m5 â†’ m6g (Graviton)
* r5 â†’ r6g

Graviton gives ~20â€“30% savings.

---

## ğŸ”¹ 2ï¸âƒ£ Use Spot Instances for Non-Critical Workloads

For:

* Batch jobs
* CI runners
* Dev environments

EKS node group example:

```bash
--capacity-type SPOT
```

Savings: up to 70%.

---

## ğŸ”¹ 3ï¸âƒ£ Consolidate Node Groups

Avoid many small underutilized nodes.

Use:

* Larger nodes
* Better bin-packing

---

## ğŸ”¹ 4ï¸âƒ£ Auto Shutdown Non-Prod

Lambda scheduler:

* Stop dev EC2 at 8 PM
* Start at 8 AM

Savings: ~50% in non-prod.

---

# ğŸ”¥ Phase 3: Database Cost Optimization

---

## ğŸ”¹ 1ï¸âƒ£ Right-size RDS

Check:

* CPU < 20%?
* Memory low usage?

Downsize instance.

---

## ğŸ”¹ 2ï¸âƒ£ Use Reserved Instances / Savings Plans

If workload stable:

* Commit 1â€“3 years
* Save 30â€“50%

---

## ğŸ”¹ 3ï¸âƒ£ Use Aurora Serverless (If Spiky)

Scale to zero in non-peak.

---

## ğŸ”¹ 4ï¸âƒ£ Reduce IOPS Overprovisioning

Check CloudWatch:

* Are you using all provisioned IOPS?

Switch to gp3 if possible.

---

# ğŸ”¥ Phase 4: Storage & Data Transfer

---

## ğŸ”¹ 1ï¸âƒ£ S3 Lifecycle Policies

```json
{
  "Transition": {
    "StorageClass": "GLACIER"
  }
}
```

Archive logs older than 30 days.

---

## ğŸ”¹ 2ï¸âƒ£ Delete Orphaned EBS Volumes

```bash
aws ec2 describe-volumes \
  --filters Name=status,Values=available
```

Delete unattached volumes.

---

## ğŸ”¹ 3ï¸âƒ£ Reduce NAT Gateway Cost

NAT is expensive.

Fix:

* Use VPC endpoints (S3, DynamoDB)
* Reduce cross-AZ traffic

---

## ğŸ”¹ 4ï¸âƒ£ Reduce Cross-AZ Traffic

Place services in same AZ where possible.

---

# ğŸ”¥ Phase 5: Container & Kubernetes Optimization

---

## ğŸ”¹ 1ï¸âƒ£ Tune Resource Requests

Over-requested CPU = waste.

Example:

```yaml
requests:
  cpu: "500m"
```

If actual usage 100m â†’ reduce.

---

## ğŸ”¹ 2ï¸âƒ£ Enable Cluster Autoscaler

Scale nodes down when idle.

---

## ğŸ”¹ 3ï¸âƒ£ Use HPA Properly

Avoid running 10 replicas when 3 enough.

---

# ğŸ”¥ Phase 6: CI/CD & DevOps Savings

---

## ğŸ”¹ 1ï¸âƒ£ Use Ephemeral Runners

Avoid always-on CI servers.

---

## ğŸ”¹ 2ï¸âƒ£ Delete Old ECR Images

Retention policy:

* Keep last 10 versions.

---

# ğŸ“Š Savings Strategy Table

| Area              | Strategy               | Estimated Savings |
| ----------------- | ---------------------- | ----------------- |
| EC2               | Graviton + rightsizing | 20â€“30%            |
| Spot              | Non-prod workloads     | 50â€“70%            |
| RDS               | Resize + reserved      | 20â€“40%            |
| S3                | Lifecycle policies     | 10â€“20%            |
| NAT               | VPC endpoints          | 20â€“50%            |
| Non-prod shutdown | Scheduler              | 30â€“50%            |

Combine â†’ 40% achievable.

---

# ğŸ”’ Protect Performance While Cutting Costs

---

## Always:

* Load test after resizing
* Monitor p95 latency
* Check error rate
* Roll back if degradation

---

## Never:

* Cut capacity blindly
* Reduce HA (Multi-AZ)
* Remove backups
* Remove monitoring

---

# ğŸ“‹ 30-Day Cost Reduction Plan

| Week   | Action                     |
| ------ | -------------------------- |
| Week 1 | Identify waste             |
| Week 2 | Resize compute + DB        |
| Week 3 | Implement Spot + lifecycle |
| Week 4 | Purchase Savings Plans     |

---

# âš ï¸ What NOT to Do

* âŒ Remove redundancy
* âŒ Disable Multi-AZ
* âŒ Ignore monitoring impact
* âŒ Overcommit Reserved Instances without analysis

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Measure top cost drivers first
* Right-size compute & DB
* Use Graviton + Spot
* Implement lifecycle + cleanup
* Add Savings Plans

ğŸ‘‰ 40% cost reduction is achievable by **rightsizing, eliminating waste, optimizing pricing models, and automating non-prod shutdown**, not by sacrificing reliability.

----
## Q36: Your development team is pushing 50 commits per day but releases only happen monthly. How do you implement continuous delivery?

---

### ğŸ§  Overview

50 commits/day + monthly releases = **process bottleneck, not engineering velocity issue**.

Problem usually caused by:

* Long-lived branches
* Manual testing
* Manual deployment approvals
* Risky big-bang releases
* No automated quality gates

Goal:

> Ship small, safe changes continuously with low risk.

Continuous Delivery =

* Every commit deployable
* Automated testing
* Automated pipelines
* Safe rollout strategy

---

# ğŸ— Phase 1: Fix Source Control Strategy

---

## ğŸ”¹ 1ï¸âƒ£ Move to Trunk-Based Development

Avoid long-lived feature branches.

Flow:

```
feature â†’ short PR â†’ main â†’ auto pipeline
```

Rules:

* Small PRs
* Merge daily
* No â€œrelease branch freezeâ€

---

## ğŸ”¹ 2ï¸âƒ£ Enforce PR Quality Gates

Require:

* 1â€“2 approvals
* CI passing
* No direct pushes to main

Example GitHub rule:

* Require status checks
* Require reviews

---

# ğŸš€ Phase 2: Build Automated CI Pipeline

---

Pipeline must run on every commit:

```groovy
stage('Build')
stage('Unit Test')
stage('Security Scan')
stage('Build Docker Image')
stage('Push to Registry')
```

Must complete in <15 mins.

Fail fast on:

* Test failure
* Security vulnerability
* Lint errors

---

# ğŸ” Phase 3: Implement CD with Safe Deployment

---

# ğŸ”¹ 1ï¸âƒ£ Automatic Deployment to Staging

```yaml
on:
  push:
    branches: [main]
```

Deploy automatically to staging.

Run:

* Integration tests
* API tests
* Smoke tests

---

# ğŸ”¹ 2ï¸âƒ£ Use Feature Flags

Instead of monthly release:

* Merge incomplete features
* Hide behind flag

Example:

```yaml
FEATURE_NEW_CHECKOUT=true
```

Allows continuous deploy without user exposure.

---

# ğŸ”¹ 3ï¸âƒ£ Production Deployment Strategy

Use:

* Rolling updates
* Blue/Green
* Canary deployments

Example (Kubernetes rolling):

```yaml
strategy:
  rollingUpdate:
    maxUnavailable: 10%
    maxSurge: 10%
```

---

# ğŸ”¥ Phase 4: Reduce Release Risk

---

# ğŸ”¹ 1ï¸âƒ£ Deploy Small Changes Frequently

Instead of:

* 200 changes monthly

Do:

* 5â€“10 changes daily

Smaller blast radius.

---

# ğŸ”¹ 2ï¸âƒ£ Add Automated Regression Tests

Cover:

* Core APIs
* Auth
* Checkout
* Payment

---

# ğŸ”¹ 3ï¸âƒ£ Add Monitoring-Based Rollback

If:

```promql
rate(http_requests_total{status=~"5.."}[5m]) > 0.05
```

Auto rollback.

---

# ğŸ”’ Phase 5: Approval Simplification

---

Instead of:

* Manual CAB approval monthly

Use:

* Automated pipeline gates
* Change log auto-generated
* Slack notification

Compliance can review logs, not block deployment.

---

# ğŸ“Š Deployment Maturity Model

| Stage          | Release Frequency |
| -------------- | ----------------- |
| Manual         | Monthly           |
| Semi-automated | Weekly            |
| CD             | Daily             |
| Advanced CD    | Multiple per day  |

Target: Daily at minimum.

---

# ğŸ“‹ CD Implementation Roadmap (8 Weeks)

| Week | Focus                    |
| ---- | ------------------------ |
| 1â€“2  | Trunk-based dev          |
| 3â€“4  | Full CI automation       |
| 5â€“6  | Automated staging deploy |
| 7    | Feature flags            |
| 8    | Canary/Blue-Green prod   |

---

# ğŸ›¡ Risk Controls

---

## Always:

* Monitor p95 latency
* Track deployment frequency
* Track lead time
* Track MTTR

DORA metrics.

---

## Avoid:

* Big-bang releases
* Manual deployment steps
* Long code freezes
* Feature branches lasting weeks

---

# ğŸ“ˆ Benefits After CD

* Faster feedback
* Reduced risk per deploy
* Faster bug fixes
* Higher confidence

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Move to trunk-based development
* Automate CI fully
* Deploy automatically to staging
* Use feature flags
* Use rolling or canary deployments

ğŸ‘‰ Continuous Delivery is about **small, safe, automated releases**, not just faster deployments.

---
## Q37: Two teams are breaking each other's deployments because they share the same pipeline. How do you solve this?

---

### ğŸ§  Overview

Shared pipeline + multiple teams = **coupling + blast radius problem**.

Symptoms:

* One teamâ€™s change breaks another teamâ€™s deploy
* Shared variables override each other
* Environment conflicts
* Unclear ownership
* Release coordination chaos

Root cause:

> Lack of isolation in CI/CD design.

Goal:

1. Isolate deployment paths
2. Enforce ownership boundaries
3. Reduce cross-team impact
4. Improve deployment reliability

---

# ğŸš¨ Phase 1: Immediate Containment

---

## 1ï¸âƒ£ Identify Coupling Points

Check:

* Shared deployment job?
* Shared Docker image?
* Shared Terraform state?
* Shared Helm values?
* Shared environment namespace?

Find whatâ€™s common.

---

# ğŸ”¥ Phase 2: Separate Pipelines Per Service

---

# ğŸ”¹ 1ï¸âƒ£ One Pipeline Per Service (Best Practice)

Instead of:

```
monorepo â†’ single pipeline â†’ deploy all services
```

Move to:

```
service-a â†’ pipeline-a
service-b â†’ pipeline-b
```

Trigger based on path:

```yaml
on:
  push:
    paths:
      - "service-a/**"
```

Each team owns its pipeline.

---

# ğŸ”¹ 2ï¸âƒ£ Use Separate Deployment Jobs

If monorepo must stay:

```groovy
stage('Deploy Service A') {
    when {
        changeset "service-a/**"
    }
}
```

Only deploy changed service.

---

# ğŸ”’ Phase 3: Isolate Infrastructure

---

# ğŸ”¹ 1ï¸âƒ£ Separate Kubernetes Namespaces

Example:

```bash
kubectl create namespace team-a
kubectl create namespace team-b
```

Avoid shared namespace.

---

# ğŸ”¹ 2ï¸âƒ£ Separate Helm Releases

```bash
helm install team-a-app ./chart -n team-a
```

No shared release names.

---

# ğŸ”¹ 3ï¸âƒ£ Separate Terraform State

Bad:

```
terraform.tfstate (shared)
```

Better:

```
team-a/terraform.tfstate
team-b/terraform.tfstate
```

S3 backend example:

```hcl
key = "team-a/prod/terraform.tfstate"
```

Prevents cross-destroy incidents.

---

# ğŸ” Phase 4: Access & Ownership Controls

---

## ğŸ”¹ 1ï¸âƒ£ Role-Based Access in CI

Team A:

* Can deploy service A
* Cannot deploy service B

Use:

* GitHub environments
* Jenkins folder-level permissions

---

## ğŸ”¹ 2ï¸âƒ£ Code Ownership

Define:

```
CODEOWNERS
```

Prevent cross-team accidental merges.

---

# ğŸ” Phase 5: Improve Deployment Strategy

---

## ğŸ”¹ 1ï¸âƒ£ Independent Versioning

Each service versioned independently.

Avoid:

```
Release v1.0 includes all services
```

Instead:

```
Service A v1.3
Service B v2.7
```

---

## ğŸ”¹ 2ï¸âƒ£ Decouple Build from Deploy

Artifact once â†’ deploy many times.

Pipeline structure:

```
Build â†’ Store artifact â†’ Deploy separately
```

Avoid rebuilding entire system for one change.

---

# ğŸ”¥ Phase 6: Implement Environment Promotion

Instead of direct prod deploy:

```
Dev â†’ Staging â†’ Prod
```

Each service promotes independently.

---

# ğŸ“Š Isolation Strategy Summary

| Problem              | Fix                      |
| -------------------- | ------------------------ |
| Shared pipeline      | Split per service        |
| Shared namespace     | Separate namespaces      |
| Shared state         | Separate Terraform state |
| Shared access        | RBAC                     |
| Shared release cycle | Independent versioning   |

---

# âš ï¸ What NOT to Do

* âŒ Add more coordination meetings
* âŒ Add manual approvals as workaround
* âŒ Keep shared state file
* âŒ Allow cross-service deployments in same job

---

# ğŸ“‹ Final Architecture

```
Team A
 â”œâ”€â”€ Repo A
 â”œâ”€â”€ Pipeline A
 â”œâ”€â”€ Namespace A
 â””â”€â”€ State A

Team B
 â”œâ”€â”€ Repo B
 â”œâ”€â”€ Pipeline B
 â”œâ”€â”€ Namespace B
 â””â”€â”€ State B
```

Isolation eliminates cross-breakage.

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Split pipelines per service
* Isolate namespaces & state files
* Enforce RBAC
* Version independently
* Decouple build and deploy

ğŸ‘‰ Deployment conflicts happen due to **shared pipeline coupling** â€” fix with isolation, ownership boundaries, and independent delivery flows.

----
## Q38: A junior developer accidentally ran `terraform destroy` on the production environment. What processes do you put in place to prevent this?

---

### ğŸ§  Overview

If someone can run `terraform destroy` in production:

> This is not a human error â€” itâ€™s a **process and guardrail failure**.

Prevention must happen at **multiple layers**:

* Access control
* CI/CD enforcement
* Terraform configuration
* Account isolation
* Policy enforcement

Goal:

1. Make destructive actions impossible by default
2. Require explicit, auditable approval
3. Enforce separation of duties

---

# ğŸš¨ Phase 1: Immediate Containment

---

## 1ï¸âƒ£ Remove Direct CLI Access to Prod

No one should run Terraform directly in production.

Enforce:

* Prod changes only via CI/CD
* No local `terraform apply` allowed

Use IAM:

```json
{
  "Effect": "Deny",
  "Action": [
    "ec2:TerminateInstances",
    "rds:DeleteDBInstance",
    "s3:DeleteBucket"
  ],
  "Resource": "*"
}
```

Applied to developer role.

---

# ğŸ” Phase 2: Separate AWS Accounts

---

## ğŸ”¹ 1ï¸âƒ£ Dedicated Production Account

Never mix:

* Dev
* Staging
* Prod

Each in separate AWS account.

Use AWS Organizations + SCPs.

---

## ğŸ”¹ 2ï¸âƒ£ Restrict Prod Access

Only CI role can assume:

```
TerraformExecutionRole-Prod
```

Developers can:

* Run plan
* Not apply

---

# ğŸ”’ Phase 3: Enforce Remote State + Locking

---

## ğŸ”¹ 1ï¸âƒ£ Mandatory S3 Backend

```hcl
backend "s3" {
  bucket         = "prod-terraform-state"
  key            = "prod/terraform.tfstate"
  dynamodb_table = "terraform-lock"
}
```

Prevents:

* Local state destroy
* Parallel execution

---

## ğŸ”¹ 2ï¸âƒ£ Enable S3 Versioning

Allows state recovery.

---

# ğŸ›¡ Phase 4: Terraform-Level Safeguards

---

# ğŸ”¹ 1ï¸âƒ£ Use `prevent_destroy`

Critical resources:

```hcl
resource "aws_db_instance" "prod" {
  lifecycle {
    prevent_destroy = true
  }
}
```

Destroy will fail.

---

# ğŸ”¹ 2ï¸âƒ£ Add `-destroy` Detection in CI

Pipeline step:

```bash
terraform plan -detailed-exitcode
```

If plan contains:

```
Destroy
```

Pipeline fails.

---

# ğŸ”¹ 3ï¸âƒ£ Require Manual Approval for Prod Apply

Jenkins example:

```groovy
input "Approve production deployment?"
```

Require senior engineer approval.

---

# ğŸ”¥ Phase 5: Policy Enforcement

---

# ğŸ”¹ 1ï¸âƒ£ Use AWS SCP to Block Destructive Actions

Example:

```json
{
  "Effect": "Deny",
  "Action": [
    "ec2:TerminateInstances",
    "rds:DeleteDBInstance"
  ],
  "Resource": "*"
}
```

Except from CI role.

---

# ğŸ”¹ 2ï¸âƒ£ Use Terraform Cloud / Enterprise

Enable:

* Sentinel policies
* Mandatory approval
* Policy-as-code

---

# ğŸ” Phase 6: Environment Safety Checks

---

## ğŸ”¹ Add Explicit Confirmation for Destroy

Use:

```bash
terraform destroy -var-file=prod.tfvars
```

Require confirmation variable:

```hcl
variable "confirm_destroy" {
  type = bool
}
```

Fail if not explicitly true.

---

# ğŸ” Phase 7: RBAC & Least Privilege

---

| Role            | Permissions   |
| --------------- | ------------- |
| Developer       | Plan only     |
| DevOps Engineer | Apply in dev  |
| CI Role         | Apply in prod |
| Security        | Audit only    |

No shared admin credentials.

---

# ğŸ”„ Phase 8: Incident Simulation & Training

Run quarterly:

* â€œWhat if destroy happens?â€
* Recovery drill
* State restore test

---

# ğŸ“Š Prevention Layers

| Layer       | Control                  |
| ----------- | ------------------------ |
| IAM         | Deny destructive actions |
| AWS Account | Separate prod            |
| Terraform   | prevent_destroy          |
| CI/CD       | Manual approval          |
| State       | Versioned backend        |
| Policy      | SCP + Sentinel           |

Defense in depth.

---

# âš ï¸ What NOT to Do

* âŒ Blame junior developer
* âŒ Only rely on verbal process
* âŒ Allow direct prod CLI access
* âŒ Skip state locking

---

# ğŸ“‹ Final Safe Architecture

```
Developer â†’ PR â†’ CI Plan â†’
Manual Approval â†’
CI Apply (Prod Role Only)
```

No human runs prod Terraform locally.

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Remove direct prod access
* Separate AWS accounts
* Use prevent_destroy
* Enforce CI approval gates
* Use SCP to block destructive APIs

ğŸ‘‰ Accidental `terraform destroy` is a **governance failure**, solved by layered guardrails and strict environment isolation.

----
## Q39: Your on-call rotation is burning out the team because of too many overnight alerts. How do you improve this?

---

### ğŸ§  Overview

If on-call is burning out:

> The problem is not the team â€” itâ€™s the alerting system and operational maturity.

Symptoms:

* Too many low-severity alerts
* Alerts without action
* Flapping alerts
* No SLO-based thresholds
* Poor escalation policies

Goal:

1. Reduce overnight noise
2. Alert only on user-impacting issues
3. Improve signal quality
4. Protect team health

Healthy on-call =
Low alert volume + high signal accuracy.

---

# ğŸš¨ Phase 1: Audit Alerts

---

## 1ï¸âƒ£ Export Alert Data (Last 30 Days)

Categorize:

| Alert | Count | Action Taken? | Should Page? |
| ----- | ----- | ------------- | ------------ |

Identify:

* Alerts ignored
* Alerts auto-resolved
* Alerts not actionable

If 80% alerts require no action â†’ remove or downgrade.

---

# ğŸ”¥ Phase 2: Implement SLO-Based Paging

---

Instead of:

```promql
cpu_usage > 70%
```

Use:

```promql
rate(http_requests_total{status=~"5.."}[5m]) > 0.05
```

Only page when:

* Availability SLO breached
* Latency SLO breached
* Service down

Infrastructure noise â†’ Slack only.

---

# ğŸ” Phase 3: Add Alert Stabilization

---

Prevent flapping:

```yaml
for: 10m
```

Only alert if condition sustained.

Use:

```promql
avg_over_time(metric[10m])
```

Not instantaneous spikes.

---

# ğŸ” Phase 4: Group & Deduplicate Alerts

---

Alertmanager config:

```yaml
group_by: ['alertname']
group_wait: 30s
repeat_interval: 4h
```

Instead of 100 alerts â†’ 1 grouped alert.

---

# ğŸ”’ Phase 5: Introduce Severity Levels

---

| Severity | Action                        |
| -------- | ----------------------------- |
| Critical | Page immediately              |
| High     | Slack + during business hours |
| Warning  | Dashboard only                |
| Info     | Log only                      |

Overnight paging only for Critical.

---

# ğŸ”¥ Phase 6: Add Error Budget Alerts

Instead of raw thresholds:

Alert on burn rate:

```promql
(error_rate / allowed_error_budget) > threshold
```

Prevents over-alerting on small blips.

---

# ğŸ” Phase 7: Improve Runbooks

Each alert must include:

* Root cause checklist
* Immediate mitigation
* Escalation path

If alert has no clear action â†’ remove it.

---

# ğŸ”¥ Phase 8: Implement Follow-the-Sun Rotation

If global team:

* Asia handles Asia hours
* US handles US hours

Reduces 3 AM pages.

---

# ğŸ”¥ Phase 9: Add Automation

---

## Auto-remediation

Example:

If pod crashes:

```bash
kubectl rollout restart deployment/app
```

Auto-heal instead of paging.

---

## Self-Healing Systems

* HPA
* Cluster autoscaler
* Restart policies

---

# ğŸ“Š Alert Optimization Impact

| Before           | After              |
| ---------------- | ------------------ |
| 200 alerts/night | <5 alerts/night    |
| CPU alerts       | SLO alerts         |
| Flapping         | Stabilized         |
| Manual triage    | Automated grouping |

---

# ğŸ›¡ Cultural Improvements

---

## Weekly Alert Review

Ask:

* Was this alert actionable?
* Did it require human intervention?
* Should it page next time?

---

## Track Alert Metrics

Measure:

* Alerts per engineer per week
* False positive rate
* Mean time to acknowledge

---

# âš ï¸ What NOT to Do

* âŒ Disable all alerts
* âŒ Raise thresholds blindly
* âŒ Page for warnings
* âŒ Ignore burnout signals

---

# ğŸ“‹ 30-Day Burnout Fix Plan

| Week | Action                    |
| ---- | ------------------------- |
| 1    | Audit alerts              |
| 2    | Remove noise              |
| 3    | Implement SLO paging      |
| 4    | Add automation + grouping |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Alert only on SLO breaches
* Remove non-actionable alerts
* Add stabilization & grouping
* Separate severity levels
* Automate self-healing

ğŸ‘‰ On-call burnout is caused by **alert noise and poor signal quality** â€” fix monitoring, not people.

---
## Q40: A new regulation requires all infrastructure changes to be reviewed and approved within your company in 24 hours. How do you implement this?

---

### ğŸ§  Overview

New regulation =

> Every infrastructure change must be reviewed and approved within 24 hours.

This is a **governance + automation + auditability** problem.

The solution is **not** manual ticket approvals.
It must be:

* Automated
* Auditable
* Enforced by pipeline
* Traceable to a human approver

Goal:

1. No infra change without review
2. Approval within 24 hours
3. Full audit trail
4. No slowdown to delivery

---

# ğŸ— Target Architecture

```
Terraform PR â†’
CI: terraform plan â†’
Plan output stored â†’
Mandatory approval â†’
CI: terraform apply â†’
Audit logs stored
```

All infra changes go through Git.

---

# ğŸ”’ Phase 1: Enforce Infrastructure as Code Only

---

## 1ï¸âƒ£ Block Manual Console Changes

Use IAM policy:

```json
{
  "Effect": "Deny",
  "Action": [
    "ec2:*",
    "rds:*",
    "s3:*"
  ],
  "Condition": {
    "StringNotEquals": {
      "aws:PrincipalArn": "arn:aws:iam::<account>:role/CI-Role"
    }
  }
}
```

Only CI role can modify infra.

---

# ğŸ” Phase 2: Mandatory PR-Based Workflow

---

## ğŸ”¹ 1ï¸âƒ£ All Changes via Pull Request

No direct commits to `main`.

Enable:

* Branch protection
* Required reviewers
* Required status checks

---

## ğŸ”¹ 2ï¸âƒ£ Terraform Plan in CI

Example pipeline:

```bash
terraform init
terraform plan -out=tfplan
terraform show tfplan > plan.txt
```

Attach `plan.txt` to PR.

Reviewer sees exactly what will change.

---

# ğŸ”¥ Phase 3: Enforce 24-Hour Approval SLA

---

## ğŸ”¹ 1ï¸âƒ£ Require 2 Approvals

GitHub example:

* Require 2 reviewers
* Code owners enforced

---

## ğŸ”¹ 2ï¸âƒ£ Auto-Reminder for Pending PRs

GitHub Action:

* If PR > 12 hours â†’ notify Slack
* If PR > 24 hours â†’ escalate

---

## ğŸ”¹ 3ï¸âƒ£ Auto-Close Expired PRs (Optional)

If not approved within 24 hours â†’ close.

Prevents stale infra plans.

---

# ğŸ” Phase 4: Approval Gate Before Apply

---

## Example Jenkins:

```groovy
stage('Manual Approval') {
  input message: 'Approve Terraform Apply?'
}
```

Approval recorded in pipeline logs.

---

## Or GitHub Environments

Require:

* Environment approval before deploy

---

# ğŸ“œ Phase 5: Full Audit Trail

---

Ensure you log:

* PR author
* PR approver
* Plan output
* Apply logs
* Timestamp

Store:

* Plan artifact in S3
* Pipeline logs in centralized logging

Auditors must see:

| Change | Who Requested | Who Approved | When Applied |
| ------ | ------------- | ------------ | ------------ |

---

# ğŸ”¥ Phase 6: Policy-as-Code Enforcement

---

Use:

* Sentinel (Terraform Cloud)
* OPA (Open Policy Agent)
* Checkov in CI

Example rule:

* No public S3
* No open security groups

Fail pipeline before approval if policy violated.

---

# ğŸ”„ Phase 7: Emergency Changes

---

Define separate process:

```
Emergency â†’ Fast approval â†’
Post-review within 24h â†’
Document justification
```

Still auditable.

---

# ğŸ“Š Process Flow Summary

| Step         | Control          |
| ------------ | ---------------- |
| Dev change   | PR required      |
| Plan         | Auto-generated   |
| Review       | 2 approvals      |
| Approval SLA | 24-hour enforced |
| Apply        | CI only          |
| Logging      | Full audit trail |

---

# ğŸ›¡ Additional Safeguards

---

## âœ… Separate Prod Account

Prod infra changes require:

* Senior approval
* Restricted reviewers

---

## âœ… Change Classification

Label PR:

* Minor change
* Major change
* Destructive change

Major/destructive â†’ additional approval.

---

## âœ… Automated Diff Summary

Generate readable plan summary for reviewers.

---

# âš ï¸ What NOT to Do

* âŒ Manual email approvals
* âŒ Verbal approvals
* âŒ Direct console changes
* âŒ Shared admin credentials

---

# ğŸ“‹ 2-Week Implementation Plan

| Week | Action                         |
| ---- | ------------------------------ |
| 1    | Enforce PR + branch protection |
| 2    | Add CI plan + approval gate    |
| 3    | Add audit logging              |
| 4    | Add policy-as-code             |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* All infra via PR
* Terraform plan auto-generated
* Mandatory reviewer approval
* CI-only apply
* Full audit trail

ğŸ‘‰ Regulatory compliance requires **automated review workflows, enforced approvals, and auditable pipelines**, not manual change management.

----
## Q41: You need to design a CI/CD pipeline for a company that has 200 developers, 50 microservices, and 5 environments. How do you approach this?

---

### ğŸ§  Overview

Scale = **people + services + environments**.

200 developers
50 microservices
5 environments (Dev, QA, Staging, UAT, Prod)

This is not just a pipeline problem â€” itâ€™s:

* Standardization
* Isolation
* Automation
* Governance
* Observability

Principles:

* One pipeline template
* Service ownership isolation
* Environment promotion model
* Infrastructure as Code
* Strong guardrails

Goal:

* Fast feedback (<15 min CI)
* Safe deployments
* Independent service releases
* Clear environment promotion

---

# ğŸ— High-Level Architecture

```
Git (Per Service Repo)
        â†“
Standard CI Template
        â†“
Artifact Registry (ECR/Nexus)
        â†“
Environment Promotion Pipeline
        â†“
Kubernetes (5 Environments)
```

---

# ğŸ”’ Phase 1: Repository & Ownership Model

---

## ğŸ”¹ 1ï¸âƒ£ One Repo per Microservice

Avoid monolithic repo.

Structure:

```
service-a/
service-b/
...
```

Each team owns:

* Code
* CI config
* Deployment manifest

---

## ğŸ”¹ 2ï¸âƒ£ Centralized CI Template

Instead of 50 custom pipelines, use shared template.

Example (GitHub Actions):

```yaml
uses: org/ci-template/.github/workflows/build.yaml@v1
```

Standardizes:

* Build
* Test
* Scan
* Docker build
* Push to registry

---

# ğŸš€ Phase 2: CI Pipeline Design (Fast Feedback)

---

Pipeline Stages:

```text
1. Lint
2. Unit Test
3. Security Scan (SAST + SCA)
4. Build Docker Image
5. Push to Registry
6. Publish Artifact
```

Must complete in <15 minutes.

Fail fast.

---

# ğŸ” Phase 3: Environment Promotion Strategy

---

Use **Promotion-Based CD**, not rebuild per environment.

Flow:

```
Dev â†’ QA â†’ Staging â†’ UAT â†’ Prod
```

Artifact built once.

Promoted, not rebuilt.

---

## ğŸ”¹ Immutable Artifact Pattern

Tag image:

```
service-a:1.3.5
```

Promote same image across environments.

---

# ğŸ”¥ Phase 4: Deployment Strategy

---

# ğŸ”¹ Kubernetes per Environment

Namespaces:

```
dev/
qa/
staging/
uat/
prod/
```

Or separate clusters for prod.

---

# ğŸ”¹ GitOps for Deployment (Recommended)

Use:

* ArgoCD
* Flux

Flow:

```
Merge to env branch â†’
Argo sync â†’
Deploy automatically
```

Provides:

* Audit trail
* Rollback
* Drift detection

---

# ğŸ” Phase 5: Governance & Control

---

## ğŸ”¹ Branch Protection

* No direct push to main
* 2 reviewers required
* Passing CI required

---

## ğŸ”¹ Production Approval Gate

Before prod deploy:

* Manual approval
* Change log attached

---

## ğŸ”¹ Policy-as-Code

Check:

* No public S3
* No open security groups
* Resource limits enforced

---

# ğŸ”¥ Phase 6: Scaling CI Infrastructure

---

200 developers = high concurrency.

Use:

* Auto-scaling runners (Kubernetes-based runners)
* Spot instances for CI
* Ephemeral runners

Avoid static Jenkins masters overloaded.

---

# ğŸ“Š Phase 7: Deployment Strategies

---

## For Microservices:

* Rolling updates
* Canary releases
* Blue/Green (for critical services)

Example (K8s):

```yaml
strategy:
  rollingUpdate:
    maxUnavailable: 10%
    maxSurge: 20%
```

---

# ğŸ” Phase 8: Observability Integration

---

Pipeline must publish:

* Build duration
* Deployment success rate
* Rollback rate

Track DORA metrics:

* Deployment frequency
* Lead time
* MTTR
* Change failure rate

---

# ğŸ“¦ Phase 9: Secrets & Configuration

---

Use:

* AWS Secrets Manager
* SSM Parameter Store
* Vault

Never store secrets in pipeline.

---

# ğŸ“Š CI/CD Architecture Summary

| Layer      | Strategy                  |
| ---------- | ------------------------- |
| Code       | Repo per service          |
| CI         | Shared template           |
| Artifact   | Immutable versioned image |
| CD         | Promotion model           |
| Deploy     | GitOps                    |
| Governance | PR + approval gates       |
| Scaling    | Auto-scaling runners      |

---

# âš ï¸ What NOT to Do

* âŒ One giant shared pipeline
* âŒ Rebuild per environment
* âŒ Manual deployments
* âŒ Shared state files
* âŒ No approval for prod

---

# ğŸ“‹ Rollout Plan

| Month   | Focus                 |
| ------- | --------------------- |
| Month 1 | CI standardization    |
| Month 2 | Environment promotion |
| Month 3 | GitOps adoption       |
| Month 4 | Governance & metrics  |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* One pipeline template for all services
* Immutable artifact promotion
* GitOps-based deployments
* Separate environments
* Strong governance + auto-scaling CI

ğŸ‘‰ At scale (200 devs, 50 services), CI/CD must be **standardized, isolated, automated, and governed**, not custom per team.

----
## Q42: Your company wants to move from a monolithic deployment to blue-green deployments for zero downtime. How do you implement this?

---

### ğŸ§  Overview

Blue-Green Deployment =

> Two identical production environments.
> One live (Blue), one idle (Green).
> Switch traffic instantly after validation.

Benefits:

* Zero downtime
* Instant rollback
* Safer releases

Works well for:

* Monolith apps
* Stateful APIs (with DB backward compatibility)
* Kubernetes-based workloads

Goal:

* Deploy without impacting users
* Validate before switching
* Roll back instantly if needed

---

# ğŸ— High-Level Architecture

```
               Route53 / ALB
                    â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚
        Blue (v1)       Green (v2)
        Active          Standby
```

Traffic routed via:

* Load balancer target groups
* DNS switch
* Kubernetes Service selector

---

# ğŸš€ Phase 1: Prepare Infrastructure

---

## ğŸ”¹ 1ï¸âƒ£ Duplicate Environment

If on EC2:

* Two Auto Scaling Groups
* Two target groups

If on Kubernetes:

* Two Deployments
* Same Service

Example (K8s):

```yaml
app: my-app
version: blue
```

```yaml
app: my-app
version: green
```

---

# ğŸ”¹ 2ï¸âƒ£ Load Balancer Setup (AWS ALB)

Create:

* TargetGroup-Blue
* TargetGroup-Green

ALB routes to only one at a time.

Switch via:

```bash
aws elbv2 modify-listener \
  --listener-arn arn:xxx \
  --default-actions Type=forward,TargetGroupArn=green-target-group
```

Traffic switch is instant.

---

# ğŸ” Phase 2: Database Compatibility

---

Critical for monolith.

Must ensure:

* Backward-compatible schema
* No destructive migrations during switch

Use:

* Expand â†’ Migrate â†’ Contract pattern

Never deploy breaking DB changes in same release.

---

# ğŸ” Phase 3: CI/CD Pipeline Integration

---

Pipeline flow:

```text
1. Build
2. Test
3. Deploy to Green
4. Smoke Test Green
5. Switch traffic
6. Monitor
7. Keep Blue as rollback
```

Example pipeline snippet:

```groovy
stage('Deploy Green') {
  sh 'kubectl apply -f green.yaml'
}
```

---

# ğŸ” Phase 4: Health Validation

---

Before switching traffic:

* Health checks pass
* Smoke tests pass
* p95 latency acceptable
* No 5xx spike

Example smoke test:

```bash
curl https://green.example.com/health
```

---

# ğŸ”¥ Phase 5: Traffic Switch

---

## Kubernetes Method

Change service selector:

```bash
kubectl patch service my-app \
  -p '{"spec":{"selector":{"version":"green"}}}'
```

Instant switch.

---

## ALB Method

Switch target group.

---

# ğŸ”„ Phase 6: Monitoring After Switch

---

Monitor for:

* Error rate
* Latency
* CPU
* DB load

Example:

```promql
rate(http_requests_total{status=~"5.."}[5m])
```

If issue â†’ revert traffic to Blue immediately.

Rollback is instant.

---

# ğŸ”’ Phase 7: Cleanup

After confidence period:

* Terminate old Blue
* Rename Green â†’ Blue
* Prepare next release

---

# ğŸ“Š Blue-Green vs Rolling

| Feature    | Blue-Green | Rolling   |
| ---------- | ---------- | --------- |
| Downtime   | Zero       | Near-zero |
| Rollback   | Instant    | Slower    |
| Infra Cost | Higher     | Lower     |
| Risk       | Lower      | Medium    |

---

# ğŸ›¡ Advanced Option: Combine with Canary

Instead of full switch:

* Route 10% to Green
* Validate
* Then 100%

Using:

* ALB weighted routing
* Istio traffic splitting

---

# âš ï¸ What NOT to Do

* âŒ Switch without health validation
* âŒ Break DB compatibility
* âŒ Rebuild environment manually
* âŒ Delete old version immediately

---

# ğŸ“‹ Implementation Plan

| Week   | Focus                       |
| ------ | --------------------------- |
| Week 1 | Duplicate infra + LB config |
| Week 2 | Update pipeline             |
| Week 3 | Test in staging             |
| Week 4 | First production blue-green |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Maintain two identical environments
* Deploy to idle environment
* Validate health
* Switch traffic instantly
* Keep old version for rollback

ğŸ‘‰ Blue-Green enables **true zero-downtime deployments with instant rollback**, especially useful for monolithic applications transitioning to safer release models.

----
## Q43: Design a disaster recovery strategy for a system with an RTO of 15 minutes and RPO of 5 minutes.

---

### ğŸ§  Overview

Requirements:

* **RTO (Recovery Time Objective): 15 minutes** â†’ System must be fully operational within 15 minutes after failure.
* **RPO (Recovery Point Objective): 5 minutes** â†’ Maximum acceptable data loss is 5 minutes.

This eliminates:

* Backup & restore only strategies
* Manual rebuild
* Single-region setups

You need:

> Warm standby or active-active multi-region architecture with near real-time replication.

Goal:

* Automated failover
* Continuous data replication
* Minimal manual intervention
* Regular DR testing

---

# ğŸ— Recommended Architecture: Warm Standby Multi-Region

```
                 Route53 (Health Check + Failover)
                          â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                              â”‚
     Primary Region                  Secondary Region
     (Active)                        (Warm Standby)
          â”‚                              â”‚
       ALB + EKS                      ALB + EKS (scaled down)
          â”‚                              â”‚
     RDS Primary                   RDS Cross-Region Replica
          â”‚                              â”‚
     S3 + ECR (CRR)                S3 + ECR Replica
```

---

# ğŸ” Data Layer Strategy (Critical for RPO 5 min)

---

## ğŸ”¹ Database

### Option A: Aurora Global Database (Best)

* Cross-region replication < 1 second
* Fast failover (<1 minute)

### Option B: RDS Cross-Region Read Replica

```bash
aws rds create-db-instance-read-replica \
  --source-db-instance-identifier prod-db
```

Promote during failover:

```bash
aws rds promote-read-replica
```

Replication lag must stay < 5 minutes.

---

## ğŸ”¹ Object Storage

Enable S3 Cross-Region Replication:

```bash
aws s3api put-bucket-replication ...
```

---

## ğŸ”¹ Container Images

Enable ECR cross-region replication.

---

# ğŸ” Compute Layer Strategy

---

## ğŸ”¹ Kubernetes / EC2

Secondary region:

* Cluster pre-provisioned via Terraform
* Smaller instance size (warm standby)
* Scaled down replicas

Scale up during failover.

---

## ğŸ”¹ Infrastructure as Code

All infra defined in Terraform:

```hcl
provider "aws" {
  alias  = "secondary"
  region = "us-west-2"
}
```

Reproducible environments.

---

# ğŸ”¥ Failover Process (Must Meet 15 Min RTO)

---

## Step 1ï¸âƒ£ Detect Outage

* Route53 health check fails
* Monitoring alert triggers

---

## Step 2ï¸âƒ£ Promote Database

If using replica:

```bash
aws rds promote-read-replica
```

---

## Step 3ï¸âƒ£ Scale Secondary Cluster

```bash
kubectl scale deployment app --replicas=10
```

Or increase ASG capacity:

```bash
aws autoscaling update-auto-scaling-group ...
```

---

## Step 4ï¸âƒ£ DNS Failover

Route53 failover routing policy:

* Primary record
* Secondary record

Automatic switch within ~1â€“2 minutes.

---

## Step 5ï¸âƒ£ Validate System

* Smoke tests
* Health endpoints
* Error rate check

Total time target: < 15 minutes.

---

# ğŸ”’ Automation is Mandatory

Manual failover likely exceeds 15 minutes.

Use:

* Lambda for automated DB promotion
* Predefined runbook
* Infrastructure alarms trigger automation

---

# ğŸ“Š DR Strategy Comparison

| Strategy         | RTO        | RPO       | Meets Requirement? |
| ---------------- | ---------- | --------- | ------------------ |
| Backup & Restore | Hours      | Hours     | âŒ                  |
| Pilot Light      | 30â€“60 mins | Low       | âŒ                  |
| Warm Standby     | <15 mins   | <5 mins   | âœ…                  |
| Active-Active    | Near-zero  | Near-zero | âœ…                  |

Warm standby recommended for cost balance.

---

# ğŸ” Monitoring Requirements

Monitor:

* Replication lag:

```promql
rds_replica_lag_seconds
```

* Health checks
* Application SLO

Alert if replication lag > 5 min.

---

# ğŸ”„ Testing Strategy (Critical)

Quarterly DR test:

1. Simulate region failure
2. Promote secondary
3. Switch traffic
4. Validate system
5. Document results

Without testing â†’ RTO target unreliable.

---

# ğŸ›¡ Additional Hardening

---

## âœ… Use Multi-AZ in Primary

Protect against AZ-level failure.

---

## âœ… Store Terraform State Cross-Region

Avoid single-region state dependency.

---

## âœ… Centralized Logging Across Regions

Ensure logs accessible during outage.

---

# ğŸ“‹ Disaster Recovery Playbook

| Step | Action                  |
| ---- | ----------------------- |
| 1    | Detect outage           |
| 2    | Promote replica         |
| 3    | Scale secondary compute |
| 4    | Switch DNS              |
| 5    | Validate app            |
| 6    | Notify stakeholders     |

---

# âš ï¸ What NOT to Do

* âŒ Rely only on Multi-AZ
* âŒ Assume manual failover is fast
* âŒ Skip replication monitoring
* âŒ Avoid DR testing

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Use warm standby multi-region
* Cross-region DB replication (<5 min lag)
* Automated DNS failover
* Pre-provisioned secondary cluster
* Regular DR testing

ğŸ‘‰ RTO 15 min + RPO 5 min requires **multi-region replication + automated failover**, not backup-based recovery.

---
## Q44: Your startup is growing from 10 to 200 engineers in one year. How do you scale your DevOps practices and infrastructure?

---

### ğŸ§  Overview

Scaling from 10 â†’ 200 engineers is not just infra scaling. Itâ€™s:

* Process scaling
* Platform standardization
* Governance automation
* Self-service enablement

If you donâ€™t standardize early:

> Youâ€™ll get pipeline chaos, environment drift, security issues, and deployment conflicts.

Goal:

* Enable autonomy
* Maintain guardrails
* Standardize patterns
* Avoid bottlenecks

Principle:

> Build a **platform team** and treat infrastructure as a product.

---

# ğŸ— Phase 1: Standardize Foundations (Months 1â€“3)

---

# ğŸ”¹ 1ï¸âƒ£ Infrastructure as Code Mandatory

All infra via Terraform.

Structure:

```
modules/
environments/
platform/
```

Enforce:

* No manual console changes
* Remote state + locking

---

# ğŸ”¹ 2ï¸âƒ£ Standard CI/CD Template

Create reusable template:

```yaml
uses: org/ci-template/.github/workflows/build.yaml@v1
```

All 50+ services inherit same build pattern.

Prevents pipeline sprawl.

---

# ğŸ”¹ 3ï¸âƒ£ Define Golden Path

Document:

* How to create a new service
* How to deploy
* How to monitor
* Security requirements

Make onboarding simple.

---

# ğŸš€ Phase 2: Build a Platform Engineering Model

---

## Create a Platform Team

Responsibilities:

* CI/CD platform
* Kubernetes clusters
* Observability
* Security guardrails
* Cost optimization

Product mindset:

* Internal developer platform (IDP)

---

# ğŸ”¥ Phase 3: Self-Service Infrastructure

---

# ğŸ”¹ 1ï¸âƒ£ Service Templates

New microservice template:

* Dockerfile
* CI config
* Helm chart
* Monitoring config

Developers bootstrap service in 10 minutes.

---

# ğŸ”¹ 2ï¸âƒ£ Environment Provisioning Automation

Create dev environment via:

```bash
make create-service
```

Or internal portal.

Avoid manual infra requests.

---

# ğŸ” Phase 4: Governance at Scale

---

# ğŸ”¹ 1ï¸âƒ£ Multi-Account Strategy

Use AWS Organizations:

```
Dev
QA
Staging
Prod
Sandbox
```

Prevent cross-environment risk.

---

# ğŸ”¹ 2ï¸âƒ£ RBAC & Role Separation

Developers:

* Deploy to dev
* PR-based promotion to prod

Prod deploy requires approval.

---

# ğŸ”¹ 3ï¸âƒ£ Policy-as-Code

Enforce:

* No public S3
* No open security groups
* Resource limits mandatory

Fail CI if violated.

---

# ğŸ“¦ Phase 5: Kubernetes Scaling

---

# ğŸ”¹ 1ï¸âƒ£ Namespace Per Team

```
team-a/
team-b/
```

Isolation prevents cross-impact.

---

# ğŸ”¹ 2ï¸âƒ£ Resource Quotas

```yaml
limits:
  cpu: "20"
  memory: "40Gi"
```

Avoid noisy neighbors.

---

# ğŸ”¹ 3ï¸âƒ£ Cluster Autoscaler

Auto-scale nodes for growing workload.

---

# ğŸ” Phase 6: Observability Standardization

---

Every service must include:

* Structured logging
* Metrics endpoint
* Tracing instrumentation

Deploy:

* Prometheus
* Grafana
* OpenTelemetry

No service goes live without monitoring.

---

# ğŸ“Š Phase 7: Deployment & Release Maturity

---

Implement:

* Trunk-based development
* Feature flags
* Blue/Green or Canary
* Automated rollback

Track DORA metrics.

---

# ğŸ”¥ Phase 8: Cost & Efficiency Controls

---

## Implement:

* Rightsizing reviews
* Spot for non-prod
* Lifecycle policies
* Budget alerts

Growing team â†’ growing waste risk.

---

# ğŸ“‹ Growth Scaling Strategy

| Area         | Control                   |
| ------------ | ------------------------- |
| CI/CD        | Shared templates          |
| Infra        | IaC only                  |
| Environments | Multi-account             |
| Security     | Policy-as-code            |
| Deployments  | GitOps                    |
| Monitoring   | Mandatory instrumentation |
| Access       | RBAC                      |

---

# ğŸ”„ Cultural Scaling

---

## ğŸ”¹ Documentation

Everything documented:

* Deployment guide
* Incident runbook
* Architecture patterns

---

## ğŸ”¹ Developer Enablement

* Internal workshops
* Platform documentation
* Slack support channel

---

# âš ï¸ What NOT to Do

* âŒ Allow custom pipelines per team
* âŒ Allow console infra changes
* âŒ Delay governance until later
* âŒ Make DevOps a ticket-based bottleneck

---

# ğŸ“ˆ Target Model at 200 Engineers

```
Developers â†’ Self-Service Platform â†’
Standard CI/CD â†’
GitOps â†’
Multi-Account Infra â†’
Central Observability â†’
Automated Guardrails
```

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Standardize everything early
* Build platform team
* Enforce IaC + CI templates
* Multi-account isolation
* Self-service with guardrails

ğŸ‘‰ Scaling DevOps from 10 â†’ 200 engineers requires **platform engineering, automation, and governance at scale**, not just more servers.

----
## Q45: You need to implement GitOps for a team that has never used it before. How do you plan and execute the migration?

---

### ğŸ§  Overview

GitOps =

> Git is the single source of truth for infrastructure and application state.
> The cluster reconciles itself automatically to match Git.

Benefits:

* Auditability
* Versioned deployments
* Rollback via Git revert
* No manual kubectl in prod
* Reduced configuration drift

Goal:

1. Remove manual deployments
2. Make Git authoritative
3. Automate reconciliation
4. Train the team gradually

Principle:

> Start small â†’ Prove value â†’ Standardize â†’ Enforce.

---

# ğŸ— Target Architecture

```
Developer â†’ PR â†’ Merge â†’
Git Repo (Manifests/Helm) â†’
ArgoCD / Flux â†’
Kubernetes Cluster â†’
Reconciliation Loop
```

---

# ğŸš€ Phase 1: Preparation (Week 1)

---

## ğŸ”¹ 1ï¸âƒ£ Choose GitOps Tool

Recommended:

* ArgoCD (most common)
* Flux (lightweight)

Install ArgoCD:

```bash
kubectl create namespace argocd
helm install argocd argo/argo-cd -n argocd
```

---

## ğŸ”¹ 2ï¸âƒ£ Separate App Code from Deployment Config

Structure:

```
app-repo/
config-repo/
```

Config repo contains:

* Helm values
* K8s manifests
* Environment configs

This keeps GitOps clean.

---

# ğŸ” Phase 2: Pilot Migration (Week 2)

---

## ğŸ”¹ 1ï¸âƒ£ Select 1 Non-Critical Service

Do NOT migrate everything at once.

---

## ğŸ”¹ 2ï¸âƒ£ Move Deployment to GitOps

Example Argo Application:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
spec:
  source:
    repoURL: https://github.com/org/config-repo
    path: services/service-a
  destination:
    namespace: service-a
```

Apply once:

```bash
kubectl apply -f application.yaml
```

From now on:

* No manual kubectl
* No manual helm install

---

# ğŸ” Phase 3: CI/CD Integration

---

Update pipeline:

Instead of:

```bash
kubectl apply -f deployment.yaml
```

Do:

```bash
git commit -am "Update image tag"
git push
```

ArgoCD detects change â†’ deploys automatically.

---

# ğŸ”¥ Phase 4: Environment Structure

---

Use Git branches or directories:

```
config-repo/
 â”œâ”€â”€ dev/
 â”œâ”€â”€ qa/
 â”œâ”€â”€ staging/
 â””â”€â”€ prod/
```

Each environment controlled by separate Argo Application.

---

# ğŸ” Phase 5: Governance & Guardrails

---

## ğŸ”¹ 1ï¸âƒ£ Remove Manual Cluster Access

Developers:

* No direct prod kubectl
* Deploy via Git only

Enforce RBAC:

```bash
kubectl create role ...
```

---

## ğŸ”¹ 2ï¸âƒ£ Require PR Review for Prod Changes

Branch protection:

* 2 approvals required
* CI validation required

---

# ğŸ” Phase 6: Drift Detection

---

Argo automatically detects:

* Manual changes
* Configuration drift

Set:

```yaml
syncPolicy:
  automated:
    prune: true
    selfHeal: true
```

Cluster reverts manual changes.

---

# ğŸ”¥ Phase 7: Rollout Strategy

---

## Gradual Migration

| Week   | Action                               |
| ------ | ------------------------------------ |
| Week 1 | Install Argo + training              |
| Week 2 | Migrate 1 service                    |
| Week 3 | Migrate 5 services                   |
| Week 4 | Migrate all services                 |
| Week 5 | Remove manual deployment permissions |

---

# ğŸ”’ Phase 8: Observability Integration

---

Monitor:

* Sync status
* Deployment failures
* Drift events

Alert on:

```promql
argocd_app_health_status != "Healthy"
```

---

# ğŸ“Š GitOps vs Traditional CD

| Feature            | Traditional     | GitOps     |
| ------------------ | --------------- | ---------- |
| Deployment trigger | CI pipeline     | Git commit |
| Source of truth    | Pipeline config | Git repo   |
| Drift detection    | Manual          | Automatic  |
| Rollback           | Redeploy        | Git revert |

---

# âš ï¸ Common Migration Mistakes

* âŒ Migrate everything at once
* âŒ Keep manual kubectl access
* âŒ Mix app code and infra code randomly
* âŒ Skip RBAC enforcement
* âŒ Ignore team training

---

# ğŸ›¡ Training & Adoption

---

Provide:

* GitOps workflow documentation
* Example PR templates
* Rollback guide
* â€œHow to debug Argoâ€ guide

Run internal demo session.

---

# ğŸ“‹ Final GitOps Flow

```
Code change â†’
Image built â†’
Update config repo image tag â†’
PR approved â†’
Merge â†’
Argo sync â†’
Cluster updated
```

No manual apply.

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Install ArgoCD
* Separate config repo
* Deploy via Git commits
* Enforce RBAC
* Migrate gradually

ğŸ‘‰ GitOps migration succeeds when you **start small, enforce Git as the source of truth, remove manual deploys, and educate the team**, not when you flip everything at once.

---
## Q46: Amazon-style: Describe a time when you identified a significant infrastructure problem before it became an incident. What signals did you notice and what did you do?

---

### ğŸ§  Overview (STAR Approach)

This is a behavioral + technical question.

They want to assess:

* Proactiveness
* Observability maturity
* Ownership
* Risk mitigation
* Preventative mindset

Structure your answer using **STAR**:

* **S**ituation
* **T**ask
* **A**ction
* **R**esult

Below is a production-grade example answer suitable for senior DevOps roles.

---

# ğŸŸ¦ Example Answer (Production Scenario)

---

## ğŸŸ¦ Situation

We were running a high-traffic Kubernetes-based microservices platform on AWS EKS.
Traffic was steadily increasing due to new feature adoption.

Although there were no active incidents, I noticed subtle signals in our observability dashboards.

---

## ğŸŸ¦ Signals I Noticed

1ï¸âƒ£ Increasing p95 latency (but averages were normal)

```promql
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
```

2ï¸âƒ£ Gradual rise in DB connection usage (80% of max)

3ï¸âƒ£ Node memory usage trending upward over 2 weeks

4ï¸âƒ£ HPA frequently scaling up during peak hours

5ï¸âƒ£ Replica lag occasionally hitting 3â€“4 minutes

None triggered alerts yet â€” but trend analysis showed risk.

---

## ğŸŸ¦ Risk Identified

Projected traffic growth suggested:

* DB connection pool exhaustion
* Increased tail latency
* Potential cascading failures during peak traffic

Based on trend analysis, the system would likely fail within 2â€“3 weeks during high load.

---

## ğŸŸ¦ Actions Taken

---

### ğŸ”¹ 1ï¸âƒ£ Capacity Planning Analysis

Exported 30-day metrics and calculated growth trend.

Identified DB as bottleneck.

---

### ğŸ”¹ 2ï¸âƒ£ Introduced RDS Proxy

Implemented connection pooling layer:

```bash
aws rds create-db-proxy --db-proxy-name prod-proxy
```

Reduced DB connections by ~40%.

---

### ğŸ”¹ 3ï¸âƒ£ Added Read Replica

Shifted read-heavy queries to replica.

Reduced primary DB CPU from 75% â†’ 45%.

---

### ğŸ”¹ 4ï¸âƒ£ Tuned HPA to Scale on RPS Instead of CPU

Updated HPA:

```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: http_requests_per_second
```

Improved responsiveness during spikes.

---

### ğŸ”¹ 5ï¸âƒ£ Increased Node Group Buffer Capacity

Raised min nodes by 20% to maintain headroom.

---

### ğŸ”¹ 6ï¸âƒ£ Added Early-Warning Alerts

Created proactive alerts for:

* DB connections >70%
* Replica lag >2 min
* p95 latency trending upward

---

## ğŸŸ¦ Result

Two weeks later, traffic increased 2.5x due to a marketing campaign.

Outcome:

* No outage
* No latency degradation
* DB utilization stayed under 60%
* No on-call alerts

The preemptive changes prevented what would likely have been a production incident.

---

# ğŸ§© Why This Answer Works

It demonstrates:

* Proactive monitoring
* Trend analysis, not reactive firefighting
* Understanding of system bottlenecks
* Ownership without being asked
* Measurable impact

---

# ğŸ” Alternative Example Signals You Could Mention

* Increasing retry rates
* Slow memory leak patterns
* Certificate expiring in 30 days
* Increasing pod eviction events
* S3 lifecycle cost growth trend
* NAT gateway traffic spike

---

# ğŸ›¡ Key Leadership Principles Demonstrated

* Ownership
* Dive Deep
* Bias for Action
* Insist on High Standards
* Earn Trust

---

# ğŸ’¡ In Short (Interview Quick Recall)

* I noticed tail latency and DB connection trends rising
* Performed capacity analysis
* Added RDS Proxy + read replica
* Tuned autoscaling
* Prevented outage during traffic spike

ğŸ‘‰ Strong answer shows **proactive detection, data-driven action, and measurable impact before failure occurred.**

----
## Q47: Google-style: You have a service with five nines (99.999%) availability requirement. Design the deployment and operational strategy to achieve this.

---

### ğŸ§  Overview

99.999% availability = **5 minutes 15 seconds downtime per year**.

That eliminates:

* Single region
* Manual failover
* Monthly maintenance windows
* Risky deployments

To achieve 5 nines, you need:

> Redundancy + automation + fault isolation + SRE discipline.

This is not just infra design â€” itâ€™s **operational excellence**.

---

# ğŸ“Š What 99.999% Means

| Availability | Downtime per Year |
| ------------ | ----------------- |
| 99.9%        | 8h 45m            |
| 99.99%       | 52m               |
| **99.999%**  | **~5m**           |

Even small deployment mistakes break this target.

---

# ğŸ— Architecture Strategy (Multi-Region Active-Active)

```
                Global Load Balancer
                       â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                 â”‚
 Region A (Active)                Region B (Active)
      â”‚                                 â”‚
   EKS Cluster                        EKS Cluster
      â”‚                                 â”‚
   DB Cluster (Global Replication)
```

---

# ğŸ”’ 1ï¸âƒ£ Multi-Region Active-Active

* Traffic split across 2+ regions
* Automatic failover (Route53 / Global Accelerator)
* Health checks every 10â€“30 seconds

No warm standby. Both regions serve traffic.

---

# ğŸ—„ 2ï¸âƒ£ Data Layer Strategy

---

## ğŸ”¹ Use Globally Replicated Database

Options:

* Aurora Global Database
* Spanner-style distributed DB
* DynamoDB Global Tables

Must provide:

* <1 min replication lag
* Fast failover

Avoid:

* Manual replica promotion

---

# âš™ï¸ 3ï¸âƒ£ Deployment Strategy

---

## ğŸ”¹ Use Canary or Blue-Green (Never Rolling Directly)

Example:

```
Deploy to 1% traffic â†’
Monitor â†’
Increase to 10% â†’
Then 100%
```

Rollback automatically on:

```promql
rate(http_requests_total{status=~"5.."}[2m]) > threshold
```

---

## ğŸ”¹ One Region at a Time

Never deploy both regions simultaneously.

Deploy Region A â†’ validate â†’ then Region B.

Prevents global outage.

---

# ğŸ” 4ï¸âƒ£ SLO & Error Budget Management

---

Define SLO:

* 99.999% availability

Track error budget:

```
Allowed downtime = ~5 mins/year
```

If error budget burns fast:

* Freeze risky deployments
* Increase reliability focus

---

# ğŸ”¥ 5ï¸âƒ£ Failure Isolation

---

## ğŸ”¹ Cell-Based Architecture

Instead of one giant cluster:

```
Cell 1
Cell 2
Cell 3
```

Each cell handles subset of traffic.

Failure impacts only 5â€“10% users.

---

## ğŸ”¹ Bulkheads & Circuit Breakers

Prevent cascading failure.

---

# ğŸš€ 6ï¸âƒ£ Autoscaling & Capacity Headroom

---

Never run near 100%.

Maintain:

* 30% spare capacity
* Pre-warmed nodes

Scaling must happen before saturation.

---

# ğŸ›¡ 7ï¸âƒ£ Deployment Safety Controls

---

## Mandatory:

* Feature flags
* Progressive rollouts
* Automatic rollback
* Chaos testing before production

---

# ğŸ” 8ï¸âƒ£ Observability Requirements

---

Must monitor:

* p99 latency
* Error rate per region
* Replication lag
* Saturation
* Dependency health

Alert only on SLO breach.

---

# ğŸ”„ 9ï¸âƒ£ Disaster Recovery Strategy

---

RTO: < 1 minute
RPO: near zero

Automated region failover.

Regular DR tests (quarterly).

---

# ğŸ” 10ï¸âƒ£ Change Management Discipline

---

Rules:

* No Friday deploys
* No simultaneous infra + app change
* No manual console changes
* Everything via CI/CD

---

# ğŸ“Š Strategy Summary

| Area        | Requirement for 5 Nines    |
| ----------- | -------------------------- |
| Region      | Multi-region active-active |
| Database    | Global replication         |
| Deployments | Canary + region stagger    |
| Scaling     | Proactive headroom         |
| Isolation   | Cell-based                 |
| Monitoring  | SLO-based                  |
| Recovery    | Automated failover         |

---

# âš ï¸ What Breaks 5 Nines

* Single region
* Manual failover
* DB replica lag
* Long GC pauses
* Retry storms
* Simultaneous multi-region deploy
* Human-in-the-loop recovery

---

# ğŸ“‹ Operational Discipline Model

| Layer  | Protection         |
| ------ | ------------------ |
| Infra  | Multi-region       |
| App    | Circuit breakers   |
| Data   | Global replication |
| Deploy | Canary + rollback  |
| Ops    | SLO enforcement    |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Active-active multi-region
* Globally replicated DB
* Canary deployments (one region at a time)
* SLO-driven operations
* Automated failover

ğŸ‘‰ Five nines is achieved through **redundancy, automation, failure isolation, and strict deployment discipline**, not just more servers.

---
## Q48: Netflix-style: How would you implement chaos engineering in a production environment to improve system resilience without causing actual outages?

---

### ğŸ§  Overview

Chaos Engineering =

> Proactively injecting controlled failure into production to validate system resilience.

Key principle (Netflix-style):

* Small blast radius
* Controlled experiments
* Measurable hypothesis
* Abort if SLO impacted

Goal:

* Discover hidden weaknesses
* Validate auto-healing
* Strengthen fault tolerance
* Avoid real customer impact

---

# ğŸ— Foundational Requirements (Before Chaos)

You must already have:

* Multi-AZ or multi-region architecture
* Auto-scaling
* Circuit breakers
* Health checks
* SLO monitoring
* Automatic rollback

Without these â†’ chaos becomes outage.

---

# ğŸ”¬ Phase 1: Define Hypothesis (Scientific Approach)

Each experiment must answer:

> â€œIf X fails, system should still maintain SLO.â€

Example hypotheses:

* If one pod crashes, traffic reroutes within 10 seconds.
* If one node dies, HPA replaces pods automatically.
* If DB replica fails, read traffic shifts to primary.

Never run chaos without hypothesis.

---

# ğŸ”¥ Phase 2: Start Small (Low Blast Radius)

---

## ğŸ”¹ 1ï¸âƒ£ Inject Failure in One Pod Only

Example:

```bash
kubectl delete pod app-123
```

Expected:

* Pod recreated
* No SLO impact
* No alert triggered

---

## ğŸ”¹ 2ï¸âƒ£ Simulate CPU Spike

```bash
stress --cpu 4 --timeout 60
```

Check:

* HPA scaling response
* Latency impact

---

## ğŸ”¹ 3ï¸âƒ£ Simulate Network Latency

Using tc:

```bash
tc qdisc add dev eth0 root netem delay 200ms
```

Validate circuit breakers.

---

# ğŸ›  Tools for Chaos

---

| Tool        | Purpose                 |
| ----------- | ----------------------- |
| Chaos Mesh  | Kubernetes-native chaos |
| LitmusChaos | K8s experiments         |
| Gremlin     | Managed chaos platform  |
| AWS FIS     | AWS fault injection     |

Example AWS FIS experiment:

* Terminate 1 EC2 in ASG
* Validate replacement within SLA

---

# ğŸ” Phase 3: Use Guardrails

---

## ğŸ”¹ 1ï¸âƒ£ Limit Scope

Target:

* Single namespace
* Single pod
* Single AZ

Never entire cluster.

---

## ğŸ”¹ 2ï¸âƒ£ SLO-Based Abort Condition

Before experiment:

Define:

```promql
rate(http_requests_total{status=~"5.."}[2m])
```

If error rate > threshold â†’ auto-stop experiment.

---

## ğŸ”¹ 3ï¸âƒ£ Run During Low-Traffic Window

Prefer:

* Business hours with team online
* Not midnight

---

# ğŸ” Phase 4: Automate Experiments

---

## Scheduled Chaos

Weekly small experiments.

Example:

* Kill 1 pod every Monday 11 AM

Build muscle memory.

---

# ğŸ”¥ Phase 5: Advanced Chaos (Gradual Maturity)

---

## ğŸ”¹ AZ Failure Simulation

Disable nodes in one AZ.

Validate:

* Traffic reroutes
* Cluster autoscaler reacts

---

## ğŸ”¹ Dependency Failure

Simulate DB timeout.

Validate:

* Circuit breaker
* Retry limits
* Fallback response

---

## ğŸ”¹ Regional Failure (Game Day)

Simulate Route53 failover.

Validate:

* RTO achieved
* No manual steps needed

---

# ğŸ“Š Chaos Maturity Model

| Level   | Experiment         |
| ------- | ------------------ |
| Level 1 | Pod kill           |
| Level 2 | Node failure       |
| Level 3 | AZ outage          |
| Level 4 | Dependency failure |
| Level 5 | Region failover    |

Gradual progression.

---

# ğŸ”’ Cultural Implementation

---

## ğŸ”¹ Document Every Experiment

Include:

* Hypothesis
* Expected outcome
* Observed result
* Improvements needed

---

## ğŸ”¹ Run Post-Experiment Review

If weakness found:

* Add fix
* Add monitoring
* Add automation

Chaos is learning tool.

---

# âš ï¸ What NOT to Do

* âŒ Random failures without hypothesis
* âŒ Full-cluster kill
* âŒ Run without monitoring
* âŒ Hide experiments from stakeholders

---

# ğŸ“‹ Safe Chaos Framework

| Step | Action                           |
| ---- | -------------------------------- |
| 1    | Define hypothesis                |
| 2    | Limit blast radius               |
| 3    | Monitor SLO                      |
| 4    | Inject fault                     |
| 5    | Auto-abort if threshold breached |
| 6    | Document findings                |

---

# ğŸ›¡ Why Chaos Works

It validates:

* Autoscaling
* Failover automation
* Circuit breakers
* Recovery time
* Team readiness

Without chaos, resilience is assumed â€” not proven.

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Define hypothesis first
* Start small (pod-level)
* Use SLO-based aborts
* Automate experiments
* Gradually increase scope

ğŸ‘‰ Chaos engineering improves resilience when done **controlled, measurable, and incremental**, not random and destructive.

---
## Q49: Meta-style: Your team owns a service processing 1 million requests per second. A code change needs to go live in 2 hours. Walk through your complete deployment strategy including risk mitigation.

---

### ğŸ§  Overview

1 million RPS = hyperscale system.

At this scale:

* 0.1% error = 1,000 failures/sec
* Small regression â†’ massive impact
* Deployment mistakes can become global incidents

Principle:

> Progressive, isolated, observable rollout with automatic rollback.

Goals:

* Zero downtime
* Contained blast radius
* Fast detection
* Immediate rollback capability

---

# ğŸ— Baseline Assumptions

Before deploying at this scale, the system must have:

* Multi-region active-active
* Canary deployment capability
* Feature flags
* Real-time SLO monitoring
* Automated rollback
* Error budget policy

Without these â†’ high risk.

---

# ğŸš€ Phase 1: Pre-Deployment Validation (T - 2 Hours)

---

## ğŸ”¹ 1ï¸âƒ£ Full CI Validation

* Unit tests
* Integration tests
* Load test (simulate 1M RPS)
* Security scan
* Performance regression check

Critical check:

```promql
p99_latency_baseline
```

Compare with staging results.

---

## ğŸ”¹ 2ï¸âƒ£ Staging Load Test at Scale

Run production-like load:

```bash
k6 run --vus 10000 --duration 10m load-test.js
```

Validate:

* p99 latency
* Memory usage
* GC behavior
* Thread pool saturation

---

## ğŸ”¹ 3ï¸âƒ£ Verify Backward Compatibility

* DB migrations non-breaking
* API contract unchanged
* Feature flag off by default

---

# ğŸ”¥ Phase 2: Deployment Strategy (Progressive Rollout)

---

# ğŸ”¹ 1ï¸âƒ£ Deploy to Single Cell / Shard

If using cell-based architecture:

```
Cell 1 â†’ 1% traffic
```

Never global first.

---

# ğŸ”¹ 2ï¸âƒ£ Canary Release (1%)

Route 1% traffic to new version.

Using:

* Istio traffic split
* ALB weighted routing
* Envoy

Example:

```yaml
weight: 1
```

Monitor for 5â€“10 minutes.

---

# ğŸ” Phase 3: Real-Time Monitoring During Canary

---

Monitor:

* p99 latency
* Error rate
* CPU
* Memory
* Retry rate
* Dependency errors

Example:

```promql
rate(http_requests_total{status=~"5.."}[1m])
```

Abort threshold:

* Error rate increase > 0.05%
* p99 latency +10%

---

# ğŸ” Phase 4: Gradual Traffic Increase

---

If stable:

```
1% â†’ 5% â†’ 10% â†’ 25% â†’ 50% â†’ 100%
```

Each stage:

* 5â€“15 min observation
* Manual + automated review

Never jump directly to 100%.

---

# ğŸŒ Phase 5: Region-by-Region Rollout

---

Deploy in one region only.

If stable â†’ deploy next region.

Never deploy all regions simultaneously.

---

# ğŸ” Phase 6: Risk Mitigation Controls

---

## ğŸ”¹ Feature Flag Safety

Keep new logic behind flag.

If issue:

```
Toggle flag off instantly
```

No redeploy required.

---

## ğŸ”¹ Automated Rollback

If:

```promql
error_rate > threshold
```

Auto:

```bash
kubectl rollout undo deployment/service
```

Rollback must complete < 2 minutes.

---

## ğŸ”¹ Capacity Headroom

Ensure:

* 30% spare capacity
* No scaling events during deploy

Avoid autoscaler interference.

---

# ğŸ“Š Deployment Guardrails

| Guardrail       | Why                   |
| --------------- | --------------------- |
| Canary 1%       | Limit blast radius    |
| Cell isolation  | Contain failures      |
| Region stagger  | Prevent global outage |
| Feature flags   | Instant disable       |
| SLO-based abort | Fast detection        |
| Auto rollback   | Reduce MTTR           |

---

# ğŸ”¥ Phase 7: Post-Deployment Validation

---

After 100% rollout:

* Monitor for 30â€“60 minutes
* Check long-tail latency
* Check DB replication lag
* Check cache hit ratio

---

# âš ï¸ Failure Scenario Handling

If canary shows regression:

* Stop traffic increase
* Rollback immediately
* Root cause analysis
* Hotfix behind flag

No prolonged partial rollout.

---

# ğŸ›¡ Why This Strategy Works at 1M RPS

Because:

* Failure impact scales exponentially
* Even small regressions are amplified
* Progressive rollout limits damage

---

# ğŸ“‹ Deployment Timeline (2 Hours)

| Time   | Action                   |
| ------ | ------------------------ |
| T-120m | Final tests + validation |
| T-90m  | Deploy to 1 cell (1%)    |
| T-75m  | Increase to 5%           |
| T-60m  | Increase to 10%          |
| T-45m  | Increase to 25%          |
| T-30m  | Increase to 50%          |
| T-15m  | Full rollout region      |
| T-0    | Expand to other regions  |

Abort anytime if SLO breach.

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Validate thoroughly in staging
* Deploy to 1% canary
* Monitor p99 + error rate
* Increase traffic gradually
* Region-by-region rollout
* Automated rollback + feature flags

ğŸ‘‰ At 1M RPS, safe deployment requires **progressive rollout, strict SLO monitoring, isolation, and instant rollback**, not confidence in the code alone.

---
## Q50: Microsoft-style: A customer is experiencing an issue that you cannot reproduce in any non-production environment. How do you safely debug this in production while minimizing customer impact?

---

### ğŸ§  Overview

Production-only bug =

* Scale-related
* Data-specific
* Region-specific
* Timing/race condition
* Configuration drift
* External dependency behavior

Constraint:

> You must debug in production without impacting customers.

Principles:

* Observe first, change last
* Isolate customer context
* Use controlled instrumentation
* Avoid broad experiments

Goal:

* Identify root cause
* Minimize blast radius
* Avoid downtime

---

# ğŸ” Phase 1: Confirm and Scope the Problem

---

## ğŸ”¹ 1ï¸âƒ£ Identify Impact Scope

Questions:

* Single customer or many?
* Specific region?
* Specific API?
* Specific request pattern?

Use filtered logs:

```bash
grep customer_id=123 production.log
```

Or query:

```promql
rate(http_requests_total{customer="123",status=~"5.."}[5m])
```

Determine if isolated.

---

# ğŸ” Phase 2: Increase Observability (Without Changing Behavior)

---

## ğŸ”¹ 1ï¸âƒ£ Enable Targeted Debug Logging

Do NOT enable global debug logging.

Instead:

* Enable log level for specific customer ID
* Enable log level for specific instance

Example:

```
if (customerId == "123") enableDebug();
```

---

## ğŸ”¹ 2ï¸âƒ£ Add Temporary Metrics

Expose custom metric:

```bash
error_condition_counter{customer="123"}
```

Track behavior in real-time.

---

## ğŸ”¹ 3ï¸âƒ£ Use Distributed Tracing

Capture trace for specific request:

```
X-Debug-Trace: true
```

Inspect span latency.

---

# ğŸ” Phase 3: Safe Reproduction in Production

---

## ğŸ”¹ 1ï¸âƒ£ Replay Traffic in Isolated Instance

Spin up one debug replica:

```bash
kubectl scale deployment app --replicas=+1
```

Route only affected customer traffic using:

* Header-based routing
* Canary routing (1% traffic)

Using Istio:

```yaml
match:
  headers:
    customer:
      exact: "123"
```

---

## ğŸ”¹ 2ï¸âƒ£ Shadow Traffic (If Needed)

Duplicate production traffic to:

* Debug environment
* Isolated namespace

Without affecting live responses.

---

# ğŸ”¥ Phase 4: Hypothesis Testing (Controlled)

---

Instead of random changes:

Test hypothesis in isolated instance:

* Increase timeout
* Adjust config
* Disable feature flag

Only for targeted traffic.

---

# ğŸ›¡ Phase 5: Protect Customer Experience

---

## ğŸ”¹ Feature Flags

Wrap potential fix:

```
if (flag_enabled) new_logic();
```

Enable only for affected customer.

---

## ğŸ”¹ Circuit Breakers

If dependency suspected:

Fail fast instead of slow failure.

---

## ğŸ”¹ Rate Limiting

If issue tied to high request rate.

---

# ğŸ“Š Phase 6: Compare Production vs Non-Prod

Look for differences:

| Area         | Check                      |
| ------------ | -------------------------- |
| Data         | Large dataset? Edge cases? |
| Config       | Env vars different?        |
| Traffic      | Higher concurrency?        |
| Dependencies | Prod-only endpoints?       |
| Caching      | Cache invalidation issues? |

Often production has:

* Larger dataset
* Real concurrency
* Different timeout values

---

# ğŸ”„ Phase 7: Safe Fix Rollout

---

Once root cause identified:

* Deploy fix to 1% traffic
* Monitor SLO
* Expand gradually

Rollback plan ready.

---

# ğŸ” Guardrails During Debugging

---

| Control         | Purpose               |
| --------------- | --------------------- |
| Scoped logging  | Avoid overload        |
| Canary routing  | Isolate change        |
| Feature flag    | Instant rollback      |
| Monitoring      | Detect regression     |
| No global debug | Prevent system impact |

---

# âš ï¸ What NOT to Do

* âŒ Enable debug logs globally
* âŒ Restart entire cluster
* âŒ Apply fix directly to 100% traffic
* âŒ Modify database manually without validation

---

# ğŸ“‹ Production Debug Strategy Summary

| Step | Action                         |
| ---- | ------------------------------ |
| 1    | Scope issue                    |
| 2    | Add targeted logging           |
| 3    | Trace specific requests        |
| 4    | Route traffic to debug replica |
| 5    | Test hypothesis safely         |
| 6    | Roll out fix gradually         |

---

# ğŸ’¡ In Short (Interview Quick Recall)

* Scope impact first
* Add targeted observability
* Route affected traffic to isolated replica
* Test via feature flags
* Roll out fix gradually

ğŸ‘‰ Safe production debugging requires **isolation, controlled experimentation, and strict blast-radius management**, not risky live experimentation.
